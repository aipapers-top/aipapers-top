<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>ICCV 2023 - AI Papers</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../mytheme.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "ICCV 2023";
        var mkdocs_page_input_path = "ICCV/ICCV_2023.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> AI Papers
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">AI Papers</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">ACL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ACL/ACL_2023/">ACL 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">CVPR</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../CVPR/CVPR_2022/">CVPR 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../CVPR/CVPR_2023/">CVPR 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">EMNLP</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../EMNLP/EMNLP_2022/">EMNLP 2022</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICCV</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">ICCV 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICLR</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2023/">ICLR 2023</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2024_Reviewing/">ICLR 2024 Reviewing</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICML</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2022/">ICML 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2023/">ICML 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">NeurIPS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../NeurIPS/NeurIPS_2022/">NeurIPS 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../NeurIPS/NeurIPS_2023/">NeurIPS 2023</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">AI Papers</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">ICCV</li>
      <li class="breadcrumb-item active">ICCV 2023</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <p>Last updated: 2023-11-07 18:04:48. Maintained by <a href="https://wayson.tech/">Weisen Jiang</a>.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">citation</th>
<th style="text-align: center;">date</th>
<th style="text-align: center;">review</th>
<th style="text-align: left;">title (pdf)</th>
<th style="text-align: left;">authors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">768</td>
<td style="text-align: center;">2023-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf">Segment Anything</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">561</td>
<td style="text-align: center;">2023-02-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf">Adding Conditional Control to Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">124</td>
<td style="text-align: center;">2022-12-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.pdf">Scalable Diffusion Models with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">118</td>
<td style="text-align: center;">2022-06-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_PETRv2_A_Unified_Framework_for_3D_Perception_from_Multi-Camera_Images_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_PETRv2_A_Unified_Framework_for_3D_Perception_from_Multi-Camera_Images_ICCV_2023_paper.pdf">PETRv2: A Unified Framework for 3D Perception from Multi-Camera<br />Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">117</td>
<td style="text-align: center;">2022-12-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Tune-A-Video_One-Shot_Tuning_of_Image_Diffusion_Models_for_Text-to-Video_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Tune-A-Video_One-Shot_Tuning_of_Image_Diffusion_Models_for_Text-to-Video_Generation_ICCV_2023_paper.pdf">Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video<br />Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">105</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_DiffusionDet_Diffusion_Model_for_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_DiffusionDet_Diffusion_Model_for_Object_Detection_ICCV_2023_paper.pdf">DiffusionDet: Diffusion Model for Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">2023-02-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf">Structure and Content-Guided Video Synthesis with Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">91</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Suris_ViperGPT_Visual_Inference_via_Python_Execution_for_Reasoning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Suris_ViperGPT_Visual_Inference_via_Python_Execution_for_Reasoning_ICCV_2023_paper.pdf">ViperGPT: Visual Inference via Python Execution for Reasoning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">86</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Zero-1-to-3_Zero-shot_One_Image_to_3D_Object_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Zero-1-to-3_Zero-shot_One_Image_to_3D_Object_ICCV_2023_paper.pdf">Zero-1-to-3: Zero-shot One Image to 3D Object</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Khachatryan_Text2Video-Zero_Text-to-Image_Diffusion_Models_are_Zero-Shot_Video_Generators_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Khachatryan_Text2Video-Zero_Text-to-Image_Diffusion_Models_are_Zero-Shot_Video_Generators_ICCV_2023_paper.pdf">Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Fantasia3D_Disentangling_Geometry_and_Appearance_for_High-quality_Text-to-3D_Content_Creation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Fantasia3D_Disentangling_Geometry_and_Appearance_for_High-quality_Text-to-3D_Content_Creation_ICCV_2023_paper.pdf">Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content<br />Creation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">55</td>
<td style="text-align: center;">2022-05-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Prompt-aligned_Gradient_for_Prompt_Tuning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Prompt-aligned_Gradient_for_Prompt_Tuning_ICCV_2023_paper.pdf">Prompt-aligned Gradient for Prompt Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Haque_Instruct-NeRF2NeRF_Editing_3D_Scenes_with_Instructions_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Haque_Instruct-NeRF2NeRF_Editing_3D_Scenes_with_Instructions_ICCV_2023_paper.pdf">Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2022-09-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Pratt_What_Does_a_Platypus_Look_Like_Generating_Customized_Prompts_for_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Pratt_What_Does_a_Platypus_Look_Like_Generating_Customized_Prompts_for_ICCV_2023_paper.pdf">What does a platypus look like? Generating customized prompts<br />for zero-shot image classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Song_LLM-Planner_Few-Shot_Grounded_Planning_for_Embodied_Agents_with_Large_Language_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Song_LLM-Planner_Few-Shot_Grounded_Planning_for_Embodied_Agents_with_Large_Language_ICCV_2023_paper.pdf">LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large<br />Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2022-11-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Versatile_Diffusion_Text_Images_and_Variations_All_in_One_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Versatile_Diffusion_Text_Images_and_Variations_All_in_One_Diffusion_ICCV_2023_paper.pdf">Versatile Diffusion: Text, Images and Variations All in One<br />Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Make-It-3D_High-fidelity_3D_Creation_from_A_Single_Image_with_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_Make-It-3D_High-fidelity_3D_Creation_from_A_Single_Image_with_Diffusion_ICCV_2023_paper.pdf">Make-It-3D: High-Fidelity 3D Creation from A Single Image with<br />Diffusion Prior</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wei_ELITE_Encoding_Visual_Concepts_into_Textual_Embeddings_for_Customized_Text-to-Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_ELITE_Encoding_Visual_Concepts_into_Textual_Embeddings_for_Customized_Text-to-Image_ICCV_2023_paper.pdf">ELITE: Encoding Visual Concepts into Textual Embeddings for Customized<br />Text-to-Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gandikota_Erasing_Concepts_from_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gandikota_Erasing_Concepts_from_Diffusion_Models_ICCV_2023_paper.pdf">Erasing Concepts from Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Raj_DreamBooth3D_Subject-Driven_Text-to-3D_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Raj_DreamBooth3D_Subject-Driven_Text-to-3D_Generation_ICCV_2023_paper.pdf">DreamBooth3D: Subject-Driven Text-to-3D Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kerr_LERF_Language_Embedded_Radiance_Fields_ICCV_2023_paper.pdf">LERF: Language Embedded Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2022-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yuan_PhysDiff_Physics-Guided_Human_Motion_Diffusion_Model_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yuan_PhysDiff_Physics-Guided_Human_Motion_Diffusion_Model_ICCV_2023_paper.pdf">PhysDiff: Physics-Guided Human Motion Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2022-01-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yan_Implicit_Autoencoder_for_Point-Cloud_Self-Supervised_Representation_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_Implicit_Autoencoder_for_Point-Cloud_Self-Supervised_Representation_Learning_ICCV_2023_paper.pdf">Implicit Autoencoder for Point Cloud Self-supervised Representation Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ceylan_Pix2Video_Video_Editing_using_Image_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ceylan_Pix2Video_Video_Editing_using_Image_Diffusion_ICCV_2023_paper.pdf">Pix2Video: Video Editing using Image Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Han_SVDiff_Compact_Parameter_Space_for_Diffusion_Fine-Tuning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Han_SVDiff_Compact_Parameter_Space_for_Diffusion_Fine-Tuning_ICCV_2023_paper.pdf">SVDiff: Compact Parameter Space for Diffusion Fine-Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Your_Diffusion_Model_is_Secretly_a_Zero-Shot_Classifier_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Your_Diffusion_Model_is_Secretly_a_Zero-Shot_Classifier_ICCV_2023_paper.pdf">Your Diffusion Model is Secretly a Zero-Shot Classifier</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2021-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_One-Shot_Generative_Domain_Adaptation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_One-Shot_Generative_Domain_Adaptation_ICCV_2023_paper.pdf">One-Shot Generative Domain Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2023-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chan_Generative_Novel_View_Synthesis_with_3D-Aware_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chan_Generative_Novel_View_Synthesis_with_3D-Aware_Diffusion_Models_ICCV_2023_paper.pdf">Generative Novel View Synthesis with 3D-Aware Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.pdf">DETRs with Collaborative Hybrid Assignments Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_DiffuMask_Synthesizing_Images_with_Pixel-level_Annotations_for_Semantic_Segmentation_Using_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_DiffuMask_Synthesizing_Images_with_Pixel-level_Annotations_for_Semantic_Segmentation_Using_ICCV_2023_paper.pdf">DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation<br />Using Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-01-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_CLIP-Driven_Universal_Model_for_Organ_Segmentation_and_Tumor_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_CLIP-Driven_Universal_Model_for_Organ_Segmentation_and_Tumor_Detection_ICCV_2023_paper.pdf">CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_TIFA_Accurate_and_Interpretable_Text-to-Image_Faithfulness_Evaluation_with_Question_Answering_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_TIFA_Accurate_and_Interpretable_Text-to-Image_Faithfulness_Evaluation_with_Question_Answering_ICCV_2023_paper.pdf">TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question<br />Answering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cao_MasaCtrl_Tuning-Free_Mutual_Self-Attention_Control_for_Consistent_Image_Synthesis_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_MasaCtrl_Tuning-Free_Mutual_Self-Attention_Control_for_Consistent_Image_Synthesis_and_ICCV_2023_paper.pdf">MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis<br />and Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2022-10-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_SimpleClick_Interactive_Image_Segmentation_with_Simple_Vision_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_SimpleClick_Interactive_Image_Segmentation_with_Simple_Vision_Transformers_ICCV_2023_paper.pdf">SimpleClick: Interactive Image Segmentation with Simple Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2022-10-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_CLIP2Point_Transfer_CLIP_to_Point_Cloud_Classification_with_Image-Depth_Pre-Training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_CLIP2Point_Transfer_CLIP_to_Point_Cloud_Classification_with_Image-Depth_Pre-Training_ICCV_2023_paper.pdf">CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth<br />Pre-training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2023-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Unleashing_Text-to-Image_Diffusion_Models_for_Visual_Perception_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Unleashing_Text-to-Image_Diffusion_Models_for_Visual_Perception_ICCV_2023_paper.pdf">Unleashing Text-to-Image Diffusion Models for Visual Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2022-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fang_Unleashing_Vanilla_Vision_Transformer_with_Masked_Image_Modeling_for_Object_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_Unleashing_Vanilla_Vision_Transformer_with_Masked_Image_Modeling_for_Object_ICCV_2023_paper.pdf">Unleashing Vanilla Vision Transformer with Masked Image Modeling for<br />Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Text2Tex_Text-driven_Texture_Synthesis_via_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Text2Tex_Text-driven_Texture_Synthesis_via_Diffusion_Models_ICCV_2023_paper.pdf">Text2Tex: Text-driven Texture Synthesis via Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hang_Efficient_Diffusion_Training_via_Min-SNR_Weighting_Strategy_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hang_Efficient_Diffusion_Training_via_Min-SNR_Weighting_Strategy_ICCV_2023_paper.pdf">Efficient Diffusion Training via Min-SNR Weighting Strategy</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2022-08-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Hierarchically_Decomposed_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Hierarchically_Decomposed_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_ICCV_2023_paper.pdf">Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hollein_Text2Room_Extracting_Textured_3D_Meshes_from_2D_Text-to-Image_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hollein_Text2Room_Extracting_Textured_3D_Meshes_from_2D_Text-to-Image_Models_ICCV_2023_paper.pdf">Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2022-12-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Reed_Scale-MAE_A_Scale-Aware_Masked_Autoencoder_for_Multiscale_Geospatial_Representation_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Reed_Scale-MAE_A_Scale-Aware_Masked_Autoencoder_for_Multiscale_Geospatial_Representation_Learning_ICCV_2023_paper.pdf">Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation<br />Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2023-02-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ding_MOSE_A_New_Dataset_for_Video_Object_Segmentation_in_Complex_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_MOSE_A_New_Dataset_for_Video_Object_Segmentation_in_Complex_ICCV_2023_paper.pdf">MOSE: A New Dataset for Video Object Segmentation in<br />Complex Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2022-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Udandarao_SuS-X_Training-Free_Name-Only_Transfer_of_Vision-Language_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Udandarao_SuS-X_Training-Free_Name-Only_Transfer_of_Vision-Language_Models_ICCV_2023_paper.pdf">SuS-X: Training-Free Name-Only Transfer of Vision-Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fernandez_The_Stable_Signature_Rooting_Watermarks_in_Latent_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fernandez_The_Stable_Signature_Rooting_Watermarks_in_Latent_Diffusion_Models_ICCV_2023_paper.pdf">The Stable Signature: Rooting Watermarks in Latent Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Rethinking_Vision_Transformers_for_MobileNet_Size_and_Speed_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Rethinking_Vision_Transformers_for_MobileNet_Size_and_Speed_ICCV_2023_paper.pdf">Rethinking Vision Transformers for MobileNet Size and Speed</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Orgad_Editing_Implicit_Assumptions_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Orgad_Editing_Implicit_Assumptions_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf">Editing Implicit Assumptions in Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2022-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Bootstrap_Motion_Forecasting_With_Self-Consistent_Constraints_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_Bootstrap_Motion_Forecasting_With_Self-Consistent_Constraints_ICCV_2023_paper.pdf">Bootstrap Motion Forecasting With Self-Consistent Constraints</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Group_DETR_Fast_DETR_Training_with_Group-Wise_One-to-Many_Assignment_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Group_DETR_Fast_DETR_Training_with_Group-Wise_One-to-Many_Assignment_ICCV_2023_paper.pdf">Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-10-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hong_Improving_Sample_Quality_of_Diffusion_Models_Using_Self-Attention_Guidance_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Improving_Sample_Quality_of_Diffusion_Models_Using_Self-Attention_Guidance_ICCV_2023_paper.pdf">Improving Sample Quality of Diffusion Models Using Self-Attention Guidance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Masked_Diffusion_Transformer_is_a_Strong_Image_Synthesizer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Masked_Diffusion_Transformer_is_a_Strong_Image_Synthesizer_ICCV_2023_paper.pdf">Masked Diffusion Transformer is a Strong Image Synthesizer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ji_DDP_Diffusion_Model_for_Dense_Visual_Prediction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_DDP_Diffusion_Model_for_Dense_Visual_Prediction_ICCV_2023_paper.pdf">DDP: Diffusion Model for Dense Visual Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kong_Rethinking_Range_View_Representation_for_LiDAR_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kong_Rethinking_Range_View_Representation_for_LiDAR_Segmentation_ICCV_2023_paper.pdf">Rethinking Range View Representation for LiDAR Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Paiss_Teaching_CLIP_to_Count_to_Ten_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Paiss_Teaching_CLIP_to_Count_to_Ten_ICCV_2023_paper.pdf">Teaching CLIP to Count to Ten</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2020-01-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chu_MixPath_A_Unified_Approach_for_One-shot_Neural_Architecture_Search_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chu_MixPath_A_Unified_Approach_for_One-shot_Neural_Architecture_Search_ICCV_2023_paper.pdf">MixPath: A Unified Approach for One-shot Neural Architecture Search</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-04-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hertz_Delta_Denoising_Score_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hertz_Delta_Denoising_Score_ICCV_2023_paper.pdf">Delta Denoising Score</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2022-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_NeuS2_Fast_Learning_of_Neural_Implicit_Surfaces_for_Multi-view_Reconstruction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_NeuS2_Fast_Learning_of_Neural_Implicit_Surfaces_for_Multi-view_Reconstruction_ICCV_2023_paper.pdf">NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view<br />Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-03-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_OpenOccupancy_A_Large_Scale_Benchmark_for_Surrounding_Semantic_Occupancy_Perception_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_OpenOccupancy_A_Large_Scale_Benchmark_for_Surrounding_Semantic_Occupancy_Perception_ICCV_2023_paper.pdf">OpenOccupancy: A Large Scale Benchmark for Surrounding Semantic Occupancy<br />Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_A_Simple_Framework_for_Open-Vocabulary_Segmentation_and_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_A_Simple_Framework_for_Open-Vocabulary_Segmentation_and_Detection_ICCV_2023_paper.pdf">A Simple Framework for Open-Vocabulary Segmentation and Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2022-02-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cho_DALL-Eval_Probing_the_Reasoning_Skills_and_Social_Biases_of_Text-to-Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_DALL-Eval_Probing_the_Reasoning_Skills_and_Social_Biases_of_Text-to-Image_ICCV_2023_paper.pdf">DALL-Eval: Probing the Reasoning Skills and Social Biases of<br />Text-to-Image Generation Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2022-12-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ye_HiTeA_Hierarchical_Temporal-Aware_Video-Language_Pre-training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_HiTeA_Hierarchical_Temporal-Aware_Video-Language_Pre-training_ICCV_2023_paper.pdf">HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wei_SurroundOcc_Multi-camera_3D_Occupancy_Prediction_for_Autonomous_Driving_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_SurroundOcc_Multi-camera_3D_Occupancy_Prediction_for_Autonomous_Driving_ICCV_2023_paper.pdf">SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Patashnik_Localizing_Object-Level_Shape_Variations_with_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Patashnik_Localizing_Object-Level_Shape_Variations_with_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf">Localizing Object-level Shape Variations with Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-07-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_I-ViT_Integer-only_Quantization_for_Efficient_Vision_Transformer_Inference_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_I-ViT_Integer-only_Quantization_for_Efficient_Vision_Transformer_Inference_ICCV_2023_paper.pdf">I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bitton-Guetta_Breaking_Common_Sense_WHOOPS_A_Vision-and-Language_Benchmark_of_Synthetic_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bitton-Guetta_Breaking_Common_Sense_WHOOPS_A_Vision-and-Language_Benchmark_of_Synthetic_and_ICCV_2023_paper.pdf">Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic<br />and Compositional Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kumari_Ablating_Concepts_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kumari_Ablating_Concepts_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf">Ablating Concepts in Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-05-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.pdf">Preserve Your Own Correlation: A Noise Prior for Video<br />Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kong_Robo3D_Towards_Robust_and_Reliable_3D_Perception_against_Corruptions_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kong_Robo3D_Towards_Robust_and_Reliable_3D_Perception_against_Corruptions_ICCV_2023_paper.pdf">Robo3D: Towards Robust and Reliable 3D Perception against Corruptions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-04-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Feng_Score-Based_Diffusion_Models_as_Principled_Priors_for_Inverse_Imaging_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_Score-Based_Diffusion_Models_as_Principled_Priors_for_Inverse_Imaging_ICCV_2023_paper.pdf">Score-Based Diffusion Models as Principled Priors for Inverse Imaging</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-10-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ranasinghe_Perceptual_Grouping_in_Contrastive_Vision-Language_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ranasinghe_Perceptual_Grouping_in_Contrastive_Vision-Language_Models_ICCV_2023_paper.pdf">Perceptual Grouping in Contrastive Vision-Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-06-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Meister_Gender_Artifacts_in_Visual_Datasets_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Meister_Gender_Artifacts_in_Visual_Datasets_ICCV_2023_paper.pdf">Gender Artifacts in Visual Datasets</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhai_Sigmoid_Loss_for_Language_Image_Pre-Training_ICCV_2023_paper.pdf">Sigmoid Loss for Language Image Pre-Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2022-10-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ye_IntrinsicNeRF_Learning_Intrinsic_Neural_Radiance_Fields_for_Editable_Novel_View_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_IntrinsicNeRF_Learning_Intrinsic_Neural_Radiance_Fields_for_Editable_Novel_View_ICCV_2023_paper.pdf">IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel<br />View Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2022-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Holmquist_DiffPose_Multi-hypothesis_Human_Pose_Estimation_using_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Holmquist_DiffPose_Multi-hypothesis_Human_Pose_Estimation_using_Diffusion_Models_ICCV_2023_paper.pdf">DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2022-11-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chou_Diffusion-SDF_Conditional_Generative_Modeling_of_Signed_Distance_Functions_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chou_Diffusion-SDF_Conditional_Generative_Modeling_of_Signed_Distance_Functions_ICCV_2023_paper.pdf">DiffusionSDF: Conditional Generative Modeling of Signed Distance Functions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2022-09-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kil_PreSTU_Pre-Training_for_Scene-Text_Understanding_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kil_PreSTU_Pre-Training_for_Scene-Text_Understanding_ICCV_2023_paper.pdf">PreSTU: Pre-Training for Scene-Text Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-02-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Q-Diffusion_Quantizing_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Q-Diffusion_Quantizing_Diffusion_Models_ICCV_2023_paper.pdf">Q-Diffusion: Quantizing Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ning_All_in_Tokens_Unifying_Output_Space_of_Visual_Tasks_via_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ning_All_in_Tokens_Unifying_Output_Space_of_Visual_Tasks_via_ICCV_2023_paper.pdf">All in Tokens: Unifying Output Space of Visual Tasks<br />via Soft Token</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Single-Stage_Diffusion_NeRF_A_Unified_Approach_to_3D_Generation_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Single-Stage_Diffusion_NeRF_A_Unified_Approach_to_3D_Generation_and_ICCV_2023_paper.pdf">Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation<br />and Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Baldrati_Zero-Shot_Composed_Image_Retrieval_with_Textual_Inversion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Baldrati_Zero-Shot_Composed_Image_Retrieval_with_Textual_Inversion_ICCV_2023_paper.pdf">Zero-Shot Composed Image Retrieval with Textual Inversion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-03-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Mikaeili_SKED_Sketch-guided_Text-based_3D_Editing_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Mikaeili_SKED_Sketch-guided_Text-based_3D_Editing_ICCV_2023_paper.pdf">SKED: Sketch-guided Text-based 3D Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Delicate_Textured_Mesh_Recovery_from_NeRF_via_Adaptive_Surface_Refinement_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_Delicate_Textured_Mesh_Recovery_from_NeRF_via_Adaptive_Surface_Refinement_ICCV_2023_paper.pdf">Delicate Textured Mesh Recovery from NeRF via Adaptive Surface<br />Refinement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Vasu_FastViT_A_Fast_Hybrid_Vision_Transformer_Using_Structural_Reparameterization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Vasu_FastViT_A_Fast_Hybrid_Vision_Transformer_Using_Structural_Reparameterization_ICCV_2023_paper.pdf">FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_ReMoDiffuse_Retrieval-Augmented_Motion_Diffusion_Model_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_ReMoDiffuse_Retrieval-Augmented_Motion_Diffusion_Model_ICCV_2023_paper.pdf">ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2022-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dou_TORE_Token_Reduction_for_Efficient_Human_Mesh_Recovery_with_Transformer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dou_TORE_Token_Reduction_for_Efficient_Human_Mesh_Recovery_with_Transformer_ICCV_2023_paper.pdf">TORE: Token Reduction for Efficient Human Mesh Recovery with<br />Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xia_DiffIR_Efficient_Diffusion_Model_for_Image_Restoration_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_DiffIR_Efficient_Diffusion_Model_for_Image_Restoration_ICCV_2023_paper.pdf">DiffIR: Efficient Diffusion Model for Image Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shtedritski_What_does_CLIP_know_about_a_red_circle_Visual_prompt_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shtedritski_What_does_CLIP_know_about_a_red_circle_Visual_prompt_ICCV_2023_paper.pdf">What does CLIP know about a red circle? Visual<br />prompt engineering for VLMs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Exploring_Object-Centric_Temporal_Modeling_for_Efficient_Multi-View_3D_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Exploring_Object-Centric_Temporal_Modeling_for_Efficient_Multi-View_3D_Object_Detection_ICCV_2023_paper.pdf">Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/He_ICL-D3IE_In-Context_Learning_with_Diverse_Demonstrations_Updating_for_Document_Information_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/He_ICL-D3IE_In-Context_Learning_with_Diverse_Demonstrations_Updating_for_Document_Information_ICCV_2023_paper.pdf">ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document<br />Information Extraction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">12</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shi_VideoFlow_Exploiting_Temporal_Cues_for_Multi-frame_Optical_Flow_Estimation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_VideoFlow_Exploiting_Temporal_Cues_for_Multi-frame_Optical_Flow_Estimation_ICCV_2023_paper.pdf">VideoFlow: Exploiting Temporal Cues for Multi-frame Optical Flow Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-06-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Doersch_TAPIR_Tracking_Any_Point_with_Per-Frame_Initialization_and_Temporal_Refinement_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Doersch_TAPIR_Tracking_Any_Point_with_Per-Frame_Initialization_and_Temporal_Refinement_ICCV_2023_paper.pdf">TAPIR: Tracking Any Point with per-frame Initialization and temporal<br />Refinement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_AvatarCraft_Transforming_Text_into_Neural_Human_Avatars_with_Parameterized_Shape_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_AvatarCraft_Transforming_Text_into_Neural_Human_Avatars_with_Parameterized_Shape_ICCV_2023_paper.pdf">AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized<br />Shape and Pose Control</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_DDFM_Denoising_Diffusion_Model_for_Multi-Modality_Image_Fusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_DDFM_Denoising_Diffusion_Model_for_Multi-Modality_Image_Fusion_ICCV_2023_paper.pdf">DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Butoi_UniverSeg_Universal_Medical_Image_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Butoi_UniverSeg_Universal_Medical_Image_Segmentation_ICCV_2023_paper.pdf">UniverSeg: Universal Medical Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2022-05-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jiao_Semi-supervised_Semantics-guided_Adversarial_Training_for_Robust_Trajectory_Prediction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jiao_Semi-supervised_Semantics-guided_Adversarial_Training_for_Robust_Trajectory_Prediction_ICCV_2023_paper.pdf">Semi-supervised Semantics-guided Adversarial Training for Trajectory Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-02-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_DREAM_Efficient_Dataset_Distillation_by_Representative_Matching_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_DREAM_Efficient_Dataset_Distillation_by_Representative_Matching_ICCV_2023_paper.pdf">DREAM: Efficient Dataset Distillation by Representative Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-06-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lindenberger_LightGlue_Local_Feature_Matching_at_Light_Speed_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lindenberger_LightGlue_Local_Feature_Matching_at_Light_Speed_ICCV_2023_paper.pdf">LightGlue: Local Feature Matching at Light Speed</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2022-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_MonoDETR_Depth-guided_Transformer_for_Monocular_3D_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_MonoDETR_Depth-guided_Transformer_for_Monocular_3D_Object_Detection_ICCV_2023_paper.pdf">MonoDETR: Depth-guided Transformer for Monocular 3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-04-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_OccFormer_Dual-path_Transformer_for_Vision-based_3D_Semantic_Occupancy_Prediction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_OccFormer_Dual-path_Transformer_for_Vision-based_3D_Semantic_Occupancy_Prediction_ICCV_2023_paper.pdf">OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Unmasked_Teacher_Towards_Training-Efficient_Video_Foundation_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Unmasked_Teacher_Towards_Training-Efficient_Video_Foundation_Models_ICCV_2023_paper.pdf">Unmasked Teacher: Towards Training-Efficient Video Foundation Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gao_Adaptive_Testing_of_Computer_Vision_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Adaptive_Testing_of_Computer_Vision_Models_ICCV_2023_paper.pdf">Adaptive Testing of Computer Vision Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_SparseNeRF_Distilling_Depth_Ranking_for_Few-shot_Novel_View_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_SparseNeRF_Distilling_Depth_Ranking_for_Few-shot_Novel_View_Synthesis_ICCV_2023_paper.pdf">SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wallace_End-to-End_Diffusion_Latent_Optimization_Improves_Classifier_Guidance_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wallace_End-to-End_Diffusion_Latent_Optimization_Improves_Classifier_Guidance_ICCV_2023_paper.pdf">End-to-End Diffusion Latent Optimization Improves Classifier Guidance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-06-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Tracking_Everything_Everywhere_All_at_Once_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Tracking_Everything_Everywhere_All_at_Once_ICCV_2023_paper.pdf">Tracking Everything Everywhere All at Once</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">11</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shan_Diffusion-Based_3D_Human_Pose_Estimation_with_Multi-Hypothesis_Aggregation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shan_Diffusion-Based_3D_Human_Pose_Estimation_with_Multi-Hypothesis_Aggregation_ICCV_2023_paper.pdf">Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2023-01-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fang_UATVR_Uncertainty-Adaptive_Text-Video_Retrieval_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_UATVR_Uncertainty-Adaptive_Text-Video_Retrieval_ICCV_2023_paper.pdf">UATVR: Uncertainty-Adaptive Text-Video Retrieval</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Pu_Adaptive_Rotated_Convolution_for_Rotated_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Pu_Adaptive_Rotated_Convolution_for_Rotated_Object_Detection_ICCV_2023_paper.pdf">Adaptive Rotated Convolution for Rotated Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xie_DiffFit_Unlocking_Transferability_of_Large_Diffusion_Models_via_Simple_Parameter-efficient_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_DiffFit_Unlocking_Transferability_of_Large_Diffusion_Models_via_Simple_Parameter-efficient_ICCV_2023_paper.pdf">DiffFit: Unlocking Transferability of Large Diffusion Models via Simple<br />Parameter-Efficient Fine-Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2023-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_GameFormer_Game-theoretic_Modeling_and_Learning_of_Transformer-based_Interactive_Prediction_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_GameFormer_Game-theoretic_Modeling_and_Learning_of_Transformer-based_Interactive_Prediction_and_ICCV_2023_paper.pdf">GameFormer: Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction<br />and Planning for Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2023-02-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sargent_VQ3D_Learning_a_3D-Aware_Generative_Model_on_ImageNet_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sargent_VQ3D_Learning_a_3D-Aware_Generative_Model_on_ImageNet_ICCV_2023_paper.pdf">VQ3D: Learning a 3D-Aware Generative Model on ImageNet</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kim_CRN_Camera_Radar_Net_for_Accurate_Robust_Efficient_3D_Perception_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_CRN_Camera_Radar_Net_for_Accurate_Robust_Efficient_3D_Perception_ICCV_2023_paper.pdf">CRN: Camera Radar Net for Accurate, Robust, Efficient 3D<br />Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_DIRE_for_Diffusion-Generated_Image_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_DIRE_for_Diffusion-Generated_Image_Detection_ICCV_2023_paper.pdf">DIRE for Diffusion-Generated Image Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2023-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jin_DiffusionRet_Generative_Text-Video_Retrieval_with_Diffusion_Model_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jin_DiffusionRet_Generative_Text-Video_Retrieval_with_Diffusion_Model_ICCV_2023_paper.pdf">DiffusionRet: Generative Text-Video Retrieval with Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2023-04-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bakr_HRS-Bench_Holistic_Reliable_and_Scalable_Benchmark_for_Text-to-Image_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bakr_HRS-Bench_Holistic_Reliable_and_Scalable_Benchmark_for_Text-to-Image_Models_ICCV_2023_paper.pdf">HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2022-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Barquero_BeLFusion_Latent_Diffusion_for_Behavior-Driven_Human_Motion_Prediction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Barquero_BeLFusion_Latent_Diffusion_for_Behavior-Driven_Human_Motion_Prediction_ICCV_2023_paper.pdf">BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2022-08-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_SAFARI_Versatile_and_Efficient_Evaluations_for_Robustness_of_Interpretability_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_SAFARI_Versatile_and_Efficient_Evaluations_for_Robustness_of_Interpretability_ICCV_2023_paper.pdf">SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2023-03-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xiang_3D-aware_Image_Generation_using_2D_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xiang_3D-aware_Image_Generation_using_2D_Diffusion_Models_ICCV_2023_paper.pdf">3D-aware Image Generation using 2D Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2023-04-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_V3Det_Vast_Vocabulary_Visual_Detection_Dataset_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_V3Det_Vast_Vocabulary_Visual_Detection_Dataset_ICCV_2023_paper.pdf">V3Det: Vast Vocabulary Visual Detection Dataset</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Erkoc_HyperDiffusion_Generating_Implicit_Neural_Fields_with_Weight-Space_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Erkoc_HyperDiffusion_Generating_Implicit_Neural_Fields_with_Weight-Space_Diffusion_ICCV_2023_paper.pdf">HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2023-04-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Harnessing_the_Spatial-Temporal_Attention_of_Diffusion_Models_for_High-Fidelity_Text-to-Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Harnessing_the_Spatial-Temporal_Attention_of_Diffusion_Models_for_High-Fidelity_Text-to-Image_ICCV_2023_paper.pdf">Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity<br />Text-to-Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2023-04-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xiang_HM-ViT_Hetero-Modal_Vehicle-to-Vehicle_Cooperative_Perception_with_Vision_Transformer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xiang_HM-ViT_Hetero-Modal_Vehicle-to-Vehicle_Cooperative_Perception_with_Vision_Transformer_ICCV_2023_paper.pdf">HM-ViT: Hetero-modal Vehicle-to-Vehicle Cooperative perception with vision transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Large_Selective_Kernel_Network_for_Remote_Sensing_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Large_Selective_Kernel_Network_for_Remote_Sensing_Object_Detection_ICCV_2023_paper.pdf">Large Selective Kernel Network for Remote Sensing Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2022-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Vinker_CLIPascene_Scene_Sketching_with_Different_Types_and_Levels_of_Abstraction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Vinker_CLIPascene_Scene_Sketching_with_Different_Types_and_Levels_of_Abstraction_ICCV_2023_paper.pdf">CLIPascene: Scene Sketching with Different Types and Levels of<br />Abstraction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2023-01-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Betrayed_by_Captions_Joint_Caption_Grounding_and_Generation_for_Open_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Betrayed_by_Captions_Joint_Caption_Grounding_and_Generation_for_Open_ICCV_2023_paper.pdf">Betrayed by Captions: Joint Caption Grounding and Generation for<br />Open Vocabulary Instance Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2022-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ladune_COOL-CHIC_Coordinate-based_Low_Complexity_Hierarchical_Image_Codec_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ladune_COOL-CHIC_Coordinate-based_Low_Complexity_Hierarchical_Image_Codec_ICCV_2023_paper.pdf">COOL-CHIC: Coordinate-based Low Complexity Hierarchical Image Codec</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Zero-Shot_Contrastive_Loss_for_Text-Guided_Diffusion_Image_Style_Transfer_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Zero-Shot_Contrastive_Loss_for_Text-Guided_Diffusion_Image_Style_Transfer_ICCV_2023_paper.pdf">Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2023-04-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ju_HumanSD_A_Native_Skeleton-Guided_Diffusion_Model_for_Human_Image_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ju_HumanSD_A_Native_Skeleton-Guided_Diffusion_Model_for_Human_Image_Generation_ICCV_2023_paper.pdf">HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image<br />Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2022-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cao_SceneRF_Self-Supervised_Monocular_3D_Scene_Reconstruction_with_Radiance_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_SceneRF_Self-Supervised_Monocular_3D_Scene_Reconstruction_with_Radiance_Fields_ICCV_2023_paper.pdf">SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cascante-Bonilla_Going_Beyond_Nouns_With_Vision__Language_Models_Using_Synthetic_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cascante-Bonilla_Going_Beyond_Nouns_With_Vision__Language_Models_Using_Synthetic_ICCV_2023_paper.pdf">Going Beyond Nouns With Vision &amp; Language Models Using<br />Synthetic Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_DOLCE_A_Model-Based_Probabilistic_Diffusion_Framework_for_Limited-Angle_CT_Reconstruction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_DOLCE_A_Model-Based_Probabilistic_Diffusion_Framework_for_Limited-Angle_CT_Reconstruction_ICCV_2023_paper.pdf">DOLCE: A Model-Based Probabilistic Diffusion Framework for Limited-Angle CT<br />Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2022-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_One-shot_Implicit_Animatable_Avatars_with_Model-based_Priors_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_One-shot_Implicit_Animatable_Avatars_with_Model-based_Priors_ICCV_2023_paper.pdf">One-shot Implicit Animatable Avatars with Model-based Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2023-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yu_FreeDoM_Training-Free_Energy-Guided_Conditional_Diffusion_Model_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_FreeDoM_Training-Free_Energy-Guided_Conditional_Diffusion_Model_ICCV_2023_paper.pdf">FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sella_Vox-E_Text-Guided_Voxel_Editing_of_3D_Objects_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sella_Vox-E_Text-Guided_Voxel_Editing_of_3D_Objects_ICCV_2023_paper.pdf">Vox-E: Text-guided Voxel Editing of 3D Objects</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Peng_EmoTalk_Speech-Driven_Emotional_Disentanglement_for_3D_Face_Animation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Peng_EmoTalk_Speech-Driven_Emotional_Disentanglement_for_3D_Face_Animation_ICCV_2023_paper.pdf">EmoTalk: Speech-driven emotional disentanglement for 3D face animation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2023-06-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Han_Dynamic_Perceiver_for_Efficient_Visual_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Han_Dynamic_Perceiver_for_Efficient_Visual_Recognition_ICCV_2023_paper.pdf">Dynamic Perceiver for Efficient Visual Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_UniT3D_A_Unified_Transformer_for_3D_Dense_Captioning_and_Visual_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_UniT3D_A_Unified_Transformer_for_3D_Dense_Captioning_and_Visual_ICCV_2023_paper.pdf">UniT3D: A Unified Transformer for 3D Dense Captioning and<br />Visual Grounding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2023-06-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tong_Scene_as_Occupancy_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tong_Scene_as_Occupancy_ICCV_2023_paper.pdf">Scene as Occupancy</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2022-12-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Thambiraja_Imitator_Personalized_Speech-driven_3D_Facial_Animation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Thambiraja_Imitator_Personalized_Speech-driven_3D_Facial_Animation_ICCV_2023_paper.pdf">Imitator: Personalized Speech-driven 3D Facial Animation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2022-05-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Integrally_Migrating_Pre-trained_Transformer_Encoder-decoders_for_Visual_Object_Detection_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Integrally_Migrating_Pre-trained_Transformer_Encoder-decoders_for_Visual_Object_Detection_ICCV_2023_paper.pdf">Integral Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_AIDE_A_Vision-Driven_Multi-View_Multi-Modal_Multi-Tasking_Dataset_for_Assistive_Driving_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_AIDE_A_Vision-Driven_Multi-View_Multi-Modal_Multi-Tasking_Dataset_for_Assistive_Driving_ICCV_2023_paper.pdf">AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive<br />Driving Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-02-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Open-domain_Visual_Entity_Recognition_Towards_Recognizing_Millions_of_Wikipedia_Entities_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Open-domain_Visual_Entity_Recognition_Towards_Recognizing_Millions_of_Wikipedia_Entities_ICCV_2023_paper.pdf">Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia<br />Entities</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2022-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/WU_Source-free_Depth_for_Object_Pop-out_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/WU_Source-free_Depth_for_Object_Pop-out_ICCV_2023_paper.pdf">Source-free Depth for Object Pop-out</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Implicit_Neural_Representation_for_Cooperative_Low-light_Image_Enhancement_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Implicit_Neural_Representation_for_Cooperative_Low-light_Image_Enhancement_ICCV_2023_paper.pdf">Implicit Neural Representation for Cooperative Low-light Image Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Han_Global_Knowledge_Calibration_for_Fast_Open-Vocabulary_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Han_Global_Knowledge_Calibration_for_Fast_Open-Vocabulary_Segmentation_ICCV_2023_paper.pdf">Global Knowledge Calibration for Fast Open-Vocabulary Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-04-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cui_SportsMOT_A_Large_Multi-Object_Tracking_Dataset_in_Multiple_Sports_Scenes_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cui_SportsMOT_A_Large_Multi-Object_Tracking_Dataset_in_Multiple_Sports_Scenes_ICCV_2023_paper.pdf">SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports<br />Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2022-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_MatrixVT_Efficient_Multi-Camera_to_BEV_Transformation_for_3D_Perception_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_MatrixVT_Efficient_Multi-Camera_to_BEV_Transformation_for_3D_Perception_ICCV_2023_paper.pdf">MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chai_Global_Adaptation_Meets_Local_Generalization_Unsupervised_Domain_Adaptation_for_3D_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chai_Global_Adaptation_Meets_Local_Generalization_Unsupervised_Domain_Adaptation_for_3D_ICCV_2023_paper.pdf">Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for<br />3D Human Pose Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ma_X-Mesh_Towards_Fast_and_Accurate_Text-driven_3D_Stylization_via_Dynamic_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_X-Mesh_Towards_Fast_and_Accurate_Text-driven_3D_Stylization_via_Dynamic_ICCV_2023_paper.pdf">X-Mesh: Towards Fast and Accurate Text-driven 3D Stylization via<br />Dynamic Textual Guidance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-02-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_HumanMAC_Masked_Motion_Completion_for_Human_Motion_Prediction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_HumanMAC_Masked_Motion_Completion_for_Human_Motion_Prediction_ICCV_2023_paper.pdf">HumanMAC: Masked Motion Completion for Human Motion Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_CiT_Curation_in_Training_for_Effective_Vision-Language_Data_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_CiT_Curation_in_Training_for_Effective_Vision-Language_Data_ICCV_2023_paper.pdf">CiT: Curation in Training for Effective Vision-Language Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Text2Performer_Text-Driven_Human_Video_Generation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Text2Performer_Text-Driven_Human_Video_Generation_ICCV_2023_paper.pdf">Text2Performer: Text-Driven Human Video Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-04-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dukler_SAFE_Machine_Unlearning_With_Shard_Graphs_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dukler_SAFE_Machine_Unlearning_With_Shard_Graphs_ICCV_2023_paper.pdf">SAFE: Machine Unlearning With Shard Graphs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-03-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Diffusion_Action_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Diffusion_Action_Segmentation_ICCV_2023_paper.pdf">Diffusion Action Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2022-10-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bulat_FS-DETR_Few-Shot_DEtection_TRansformer_with_Prompting_and_without_Re-Training_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bulat_FS-DETR_Few-Shot_DEtection_TRansformer_with_Prompting_and_without_Re-Training_ICCV_2023_paper.pdf">FS-DETR: Few-Shot DEtection TRansformer with prompting and without re-training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-04-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tu_Implicit_Temporal_Modeling_with_Learnable_Alignment_for_Video_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tu_Implicit_Temporal_Modeling_with_Learnable_Alignment_for_Video_Recognition_ICCV_2023_paper.pdf">Implicit Temporal Modeling with Learnable Alignment for Video Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-05-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Synthesizing_Diverse_Human_Motions_in_3D_Indoor_Scenes_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Synthesizing_Diverse_Human_Motions_in_3D_Indoor_Scenes_ICCV_2023_paper.pdf">Synthesizing Diverse Human Motions in 3D Indoor Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2022-11-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Exploring_Video_Quality_Assessment_on_User_Generated_Contents_from_Aesthetic_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Exploring_Video_Quality_Assessment_on_User_Generated_Contents_from_Aesthetic_ICCV_2023_paper.pdf">Exploring Video Quality Assessment on User Generated Contents from<br />Aesthetic and Technical Perspectives</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_VAD_Vectorized_Scene_Representation_for_Efficient_Autonomous_Driving_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_VAD_Vectorized_Scene_Representation_for_Efficient_Autonomous_Driving_ICCV_2023_paper.pdf">VAD: Vectorized Scene Representation for Efficient Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-01-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lin_InfiniCity_Infinite-Scale_City_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_InfiniCity_Infinite-Scale_City_Synthesis_ICCV_2023_paper.pdf">InfiniCity: Infinite-Scale City Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gururani_SPACE_Speech-driven_Portrait_Animation_with_Controllable_Expression_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gururani_SPACE_Speech-driven_Portrait_Animation_with_Controllable_Expression_ICCV_2023_paper.pdf">SPACEx: Speech-driven Portrait Animation with Controllable Expression</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Unify_Align_and_Refine_Multi-Level_Semantic_Alignment_for_Radiology_Report_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Unify_Align_and_Refine_Multi-Level_Semantic_Alignment_for_Radiology_Report_ICCV_2023_paper.pdf">Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology<br />Report Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_MixSpeech_Cross-Modality_Self-Learning_with_Audio-Visual_Stream_Mixup_for_Visual_Speech_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_MixSpeech_Cross-Modality_Self-Learning_with_Audio-Visual_Stream_Mixup_for_Visual_Speech_ICCV_2023_paper.pdf">MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual<br />Speech Translation and Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-04-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wan_UniDexGrasp_Improving_Dexterous_Grasping_Policy_Learning_via_Geometry-Aware_Curriculum_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wan_UniDexGrasp_Improving_Dexterous_Grasping_Policy_Learning_via_Geometry-Aware_Curriculum_and_ICCV_2023_paper.pdf">UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum<br />and Iterative Generalist-Specialist Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Georgescu_Audiovisual_Masked_Autoencoders_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Georgescu_Audiovisual_Masked_Autoencoders_ICCV_2023_paper.pdf">Audiovisual Masked Autoencoders</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-07-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Generative_Prompt_Model_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Generative_Prompt_Model_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.pdf">Generative Prompt Model for Weakly Supervised Object Localization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2022-05-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Roessle_End2End_Multi-View_Feature_Matching_with_Differentiable_Pose_Optimization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Roessle_End2End_Multi-View_Feature_Matching_with_Differentiable_Pose_Optimization_ICCV_2023_paper.pdf">End2End Multi-View Feature Matching with Differentiable Pose Optimization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2022-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sanchez_Domain_Generalization_of_3D_Semantic_Segmentation_in_Autonomous_Driving_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sanchez_Domain_Generalization_of_3D_Semantic_Segmentation_in_Autonomous_Driving_ICCV_2023_paper.pdf">Domain generalization of 3D semantic segmentation in autonomous driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-09-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gan_Towards_Robust_Model_Watermark_via_Reducing_Parametric_Vulnerability_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gan_Towards_Robust_Model_Watermark_via_Reducing_Parametric_Vulnerability_ICCV_2023_paper.pdf">Towards Robust Model Watermark via Reducing Parametric Vulnerability</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-06-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ma_DetZero_Rethinking_Offboard_3D_Object_Detection_with_Long-term_Sequential_Point_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_DetZero_Rethinking_Offboard_3D_Object_Detection_with_Long-term_Sequential_Point_ICCV_2023_paper.pdf">DetZero: Rethinking Offboard 3D Object Detection with Long-term Sequential<br />Point Clouds</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_PointCLIP_V2_Prompting_CLIP_and_GPT_for_Powerful_3D_Open-world_ICCV_2023_paper.pdf">PointCLIP V2: Prompting CLIP and GPT for Powerful 3D<br />Open-world Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2022-04-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Simulating_Fluids_in_Real-World_Still_Images_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_Simulating_Fluids_in_Real-World_Still_Images_ICCV_2023_paper.pdf">Simulating Fluids in Real-World Still Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-01-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lu_A_Large-Scale_Outdoor_Multi-Modal_Dataset_and_Benchmark_for_Novel_View_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_A_Large-Scale_Outdoor_Multi-Modal_Dataset_and_Benchmark_for_Novel_View_ICCV_2023_paper.pdf">A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel<br />View Synthesis and Implicit Scene Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2022-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yu_Talking_Head_Generation_with_Probabilistic_Audio-to-Visual_Diffusion_Priors_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Talking_Head_Generation_with_Probabilistic_Audio-to-Visual_Diffusion_Priors_ICCV_2023_paper.pdf">Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-05-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Going_Denser_with_Open-Vocabulary_Part_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Going_Denser_with_Open-Vocabulary_Part_Segmentation_ICCV_2023_paper.pdf">Going Denser with Open-Vocabulary Part Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-05-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Goel_Humans_in_4D_Reconstructing_and_Tracking_Humans_with_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Goel_Humans_in_4D_Reconstructing_and_Tracking_Humans_with_Transformers_ICCV_2023_paper.pdf">Humans in 4D: Reconstructing and Tracking Humans with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ge_Expressive_Text-to-Image_Generation_with_Rich_Text_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Expressive_Text-to-Image_Generation_with_Rich_Text_ICCV_2023_paper.pdf">Expressive Text-to-Image Generation with Rich Text</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-01-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ding_Unsupervised_Manifold_Linearizing_and_Clustering_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_Unsupervised_Manifold_Linearizing_and_Clustering_ICCV_2023_paper.pdf">Unsupervised Manifold Linearizing and Clustering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Improving_3D_Imaging_with_Pre-Trained_Perpendicular_2D_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Improving_3D_Imaging_with_Pre-Trained_Perpendicular_2D_Diffusion_Models_ICCV_2023_paper.pdf">Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-05-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Petrovich_TMR_Text-to-Motion_Retrieval_Using_Contrastive_3D_Human_Motion_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Petrovich_TMR_Text-to-Motion_Retrieval_Using_Contrastive_3D_Human_Motion_Synthesis_ICCV_2023_paper.pdf">TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-01-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Locomotion-Action-Manipulation_Synthesizing_Human-Scene_Interactions_in_Complex_3D_Environments_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Locomotion-Action-Manipulation_Synthesizing_Human-Scene_Interactions_in_Complex_3D_Environments_ICCV_2023_paper.pdf">Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Preventing_Zero-Shot_Transfer_Degradation_in_Continual_Learning_of_Vision-Language_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Preventing_Zero-Shot_Transfer_Degradation_in_Continual_Learning_of_Vision-Language_Models_ICCV_2023_paper.pdf">Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2022-12-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Do_DALL-E_and_Flamingo_Understand_Each_Other_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Do_DALL-E_and_Flamingo_Understand_Each_Other_ICCV_2023_paper.pdf">Do DALL-E and Flamingo Understand Each Other?</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-01-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_LinkGAN_Linking_GAN_Latents_to_Pixels_for_Controllable_Image_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_LinkGAN_Linking_GAN_Latents_to_Pixels_for_Controllable_Image_Synthesis_ICCV_2023_paper.pdf">LinkGAN: Linking GAN Latents to Pixels for Controllable Image<br />Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Diffusion_Models_as_Masked_Autoencoders_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_Diffusion_Models_as_Masked_Autoencoders_ICCV_2023_paper.pdf">Diffusion Models as Masked Autoencoders</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2022-10-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Marza_Multi-Object_Navigation_with_Dynamically_Learned_Neural_Implicit_Representations_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Marza_Multi-Object_Navigation_with_Dynamically_Learned_Neural_Implicit_Representations_ICCV_2023_paper.pdf">Multi-Object Navigation with dynamically learned neural implicit representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2023-04-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Detection_Transformer_with_Stable_Matching_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Detection_Transformer_with_Stable_Matching_ICCV_2023_paper.pdf">Detection Transformer with Stable Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2022-11-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hataya_Will_Large-scale_Generative_Models_Corrupt_Future_Datasets_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hataya_Will_Large-scale_Generative_Models_Corrupt_Future_Datasets_ICCV_2023_paper.pdf">Will Large-scale Generative Models Corrupt Future Datasets?</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_EfficientTrain_Exploring_Generalized_Curriculum_Learning_for_Training_Visual_Backbones_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_EfficientTrain_Exploring_Generalized_Curriculum_Learning_for_Training_Visual_Backbones_ICCV_2023_paper.pdf">EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xie_BoxDiff_Text-to-Image_Synthesis_with_Training-Free_Box-Constrained_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_BoxDiff_Text-to-Image_Synthesis_with_Training-Free_Box-Constrained_Diffusion_ICCV_2023_paper.pdf">BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sun_Spatially-Adaptive_Feature_Modulation_for_Efficient_Image_Super-Resolution_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Spatially-Adaptive_Feature_Modulation_for_Efficient_Image_Super-Resolution_ICCV_2023_paper.pdf">Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-06-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_DVIS_Decoupled_Video_Instance_Segmentation_Framework_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_DVIS_Decoupled_Video_Instance_Segmentation_Framework_ICCV_2023_paper.pdf">DVIS: Decoupled Video Instance Segmentation Framework</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-04-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Fan_Once_Detected_Never_Lost_Surpassing_Human_Performance_in_Offline_LiDAR_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_Once_Detected_Never_Lost_Surpassing_Human_Performance_in_Offline_LiDAR_ICCV_2023_paper.pdf">Once Detected, Never Lost: Surpassing Human Performance in Offline<br />LiDAR based 3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhou_SRFormer_Permuted_Self-Attention_for_Single_Image_Super-Resolution_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_SRFormer_Permuted_Self-Attention_for_Single_Image_Super-Resolution_ICCV_2023_paper.pdf">SRFormer: Permuted Self-Attention for Single Image Super-Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_SHERF_Generalizable_Human_NeRF_from_a_Single_Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_SHERF_Generalizable_Human_NeRF_from_a_Single_Image_ICCV_2023_paper.pdf">SHERF: Generalizable Human NeRF from a Single Image</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-06-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Roth_Waffling_Around_for_Performance_Visual_Classification_with_Random_Words_and_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Roth_Waffling_Around_for_Performance_Visual_Classification_with_Random_Words_and_ICCV_2023_paper.pdf">Waffling around for Performance: Visual Classification with Random Words<br />and Broad Concepts</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-03-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Grounding_3D_Object_Affordance_from_2D_Interactions_in_Images_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Grounding_3D_Object_Affordance_from_2D_Interactions_in_Images_ICCV_2023_paper.pdf">Grounding 3D Object Affordance from 2D Interactions in Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shaker_SwiftFormer_Efficient_Additive_Attention_for_Transformer-based_Real-time_Mobile_Vision_Applications_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shaker_SwiftFormer_Efficient_Additive_Attention_for_Transformer-based_Real-time_Mobile_Vision_Applications_ICCV_2023_paper.pdf">SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision<br />Applications</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Traj-MAE_Masked_Autoencoders_for_Trajectory_Prediction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Traj-MAE_Masked_Autoencoders_for_Trajectory_Prediction_ICCV_2023_paper.pdf">Traj-MAE: Masked Autoencoders for Trajectory Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-08-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Multi-interactive_Feature_Learning_and_a_Full-time_Multi-modality_Benchmark_for_Image_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Multi-interactive_Feature_Learning_and_a_Full-time_Multi-modality_Benchmark_for_Image_ICCV_2023_paper.pdf">Multi-interactive Feature Learning and a Full-time Multi-modality Benchmark for<br />Image Fusion and Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Panos_First_Session_Adaptation_A_Strong_Replay-Free_Baseline_for_Class-Incremental_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Panos_First_Session_Adaptation_A_Strong_Replay-Free_Baseline_for_Class-Incremental_Learning_ICCV_2023_paper.pdf">First Session Adaptation: A Strong Replay-Free Baseline for Class-Incremental<br />Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ge_Ref-NeuS_Ambiguity-Reduced_Neural_Implicit_Surface_Learning_for_Multi-View_Reconstruction_with_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Ref-NeuS_Ambiguity-Reduced_Neural_Implicit_Surface_Learning_for_Multi-View_Reconstruction_with_ICCV_2023_paper.pdf">Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction<br />with Reflection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-07-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Hallucination_Improves_the_Performance_of_Unsupervised_Visual_Representation_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Hallucination_Improves_the_Performance_of_Unsupervised_Visual_Representation_Learning_ICCV_2023_paper.pdf">Hallucination Improves the Performance of Unsupervised Visual Representation Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2022-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Nayal_RbA_Segmenting_Unknown_Regions_Rejected_by_All_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Nayal_RbA_Segmenting_Unknown_Regions_Rejected_by_All_ICCV_2023_paper.pdf">RbA: Segmenting Unknown Regions Rejected by All</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2022-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Huang_PADDLES_Phase-Amplitude_Spectrum_Disentangled_Early_Stopping_for_Learning_with_Noisy_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_PADDLES_Phase-Amplitude_Spectrum_Disentangled_Early_Stopping_for_Learning_with_Noisy_ICCV_2023_paper.pdf">PADDLES: Phase-Amplitude Spectrum Disentangled Early Stopping for Learning with<br />Noisy Labels</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lee_Robust_Evaluation_of_Diffusion-Based_Adversarial_Purification_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Robust_Evaluation_of_Diffusion-Based_Adversarial_Purification_ICCV_2023_paper.pdf">Robust Evaluation of Diffusion-Based Adversarial Purification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ponghiran_Event-based_Temporally_Dense_Optical_Flow_Estimation_with_Sequential_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ponghiran_Event-based_Temporally_Dense_Optical_Flow_Estimation_with_Sequential_Learning_ICCV_2023_paper.pdf">Event-based Temporally Dense Optical Flow Estimation with Sequential Neural<br />Networks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-06-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Szymanowicz_Viewset_Diffusion_0-Image-Conditioned_3D_Generative_Models_from_2D_Data_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Szymanowicz_Viewset_Diffusion_0-Image-Conditioned_3D_Generative_Models_from_2D_Data_ICCV_2023_paper.pdf">Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Among_Us_Adversarially_Robust_Collaborative_Perception_by_Consensus_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Among_Us_Adversarially_Robust_Collaborative_Perception_by_Consensus_ICCV_2023_paper.pdf">Among Us: Adversarially Robust Collaborative Perception by Consensus</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Momeni_Verbs_in_Action_Improving_Verb_Understanding_in_Video-Language_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Momeni_Verbs_in_Action_Improving_Verb_Understanding_in_Video-Language_Models_ICCV_2023_paper.pdf">Verbs in Action: Improving verb understanding in video-language models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Xu_Human-centric_Scene_Understanding_for_3D_Large-scale_Scenarios_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Human-centric_Scene_Understanding_for_3D_Large-scale_Scenarios_ICCV_2023_paper.pdf">Human-centric Scene Understanding for 3D Large-scale Scenarios</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-01-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Aberdam_CLIPTER_Looking_at_the_Bigger_Picture_in_Scene_Text_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Aberdam_CLIPTER_Looking_at_the_Bigger_Picture_in_Scene_Text_Recognition_ICCV_2023_paper.pdf">CLIPTER: Looking at the Bigger Picture in Scene Text<br />Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lin_MAtch_eXpand_and_Improve_Unsupervised_Finetuning_for_Zero-Shot_Action_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_MAtch_eXpand_and_Improve_Unsupervised_Finetuning_for_Zero-Shot_Action_Recognition_ICCV_2023_paper.pdf">MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action<br />Recognition with Language Knowledge</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-05-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dong_Prompt_Tuning_Inversion_for_Text-driven_Image_Editing_Using_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Prompt_Tuning_Inversion_for_Text-driven_Image_Editing_Using_Diffusion_Models_ICCV_2023_paper.pdf">Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2022-08-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dang_Search_for_or_Navigate_to_Dual_Adaptive_Thinking_for_Object_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dang_Search_for_or_Navigate_to_Dual_Adaptive_Thinking_for_Object_ICCV_2023_paper.pdf">Search for or Navigate to? Dual Adaptive Thinking for<br />Object Navigation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Not_All_Features_Matter_Enhancing_Few-shot_CLIP_with_Adaptive_Prior_ICCV_2023_paper.pdf">Not All Features Matter: Enhancing Few-shot CLIP with Adaptive<br />Prior Refinement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-04-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Enhancing_Fine-Tuning_Based_Backdoor_Defense_with_Sharpness-Aware_Minimization_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Enhancing_Fine-Tuning_Based_Backdoor_Defense_with_Sharpness-Aware_Minimization_ICCV_2023_paper.pdf">Enhancing Fine-Tuning Based Backdoor Defense with Sharpness-Aware Minimization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Neuhaus_Spurious_Features_Everywhere_-_Large-Scale_Detection_of_Harmful_Spurious_Features_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Neuhaus_Spurious_Features_Everywhere_-_Large-Scale_Detection_of_Harmful_Spurious_Features_ICCV_2023_paper.pdf">Spurious Features Everywhere - Large-Scale Detection of Harmful Spurious<br />Features in ImageNet</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2022-10-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Sevastopolskiy_How_to_Boost_Face_Recognition_with_StyleGAN_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Sevastopolskiy_How_to_Boost_Face_Recognition_with_StyleGAN_ICCV_2023_paper.pdf">How to Boost Face Recognition with StyleGAN?</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_DyGait_Exploiting_Dynamic_Representations_for_High-performance_Gait_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_DyGait_Exploiting_Dynamic_Representations_for_High-performance_Gait_Recognition_ICCV_2023_paper.pdf">DyGait: Exploiting Dynamic Representations for High-performance Gait Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lu_See_More_and_Know_More_Zero-shot_Point_Cloud_Segmentation_via_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_See_More_and_Know_More_Zero-shot_Point_Cloud_Segmentation_via_ICCV_2023_paper.pdf">See More and Know More: Zero-shot Point Cloud Segmentation<br />via Multi-modal Visual Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ouali_Black_Box_Few-Shot_Adaptation_for_Vision-Language_Models_ICCV_2023_paper.pdf">Black Box Few-Shot Adaptation for Vision-Language models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Zolly_Zoom_Focal_Length_Correctly_for_Perspective-Distorted_Human_Mesh_Reconstruction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Zolly_Zoom_Focal_Length_Correctly_for_Perspective-Distorted_Human_Mesh_Reconstruction_ICCV_2023_paper.pdf">Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh<br />Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-07-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_DNA-Rendering_A_Diverse_Neural_Actor_Repository_for_High-Fidelity_Human-Centric_Rendering_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_DNA-Rendering_A_Diverse_Neural_Actor_Repository_for_High-Fidelity_Human-Centric_Rendering_ICCV_2023_paper.pdf">DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric<br />Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2023-08-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chai_StableVideo_Text-driven_Consistency-aware_Diffusion_Video_Editing_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chai_StableVideo_Text-driven_Consistency-aware_Diffusion_Video_Editing_ICCV_2023_paper.pdf">StableVideo: Text-driven Consistency-aware Diffusion Video Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-08-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Membrane_Potential_Batch_Normalization_for_Spiking_Neural_Networks_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Membrane_Potential_Batch_Normalization_for_Spiking_Neural_Networks_ICCV_2023_paper.pdf">Membrane Potential Batch Normalization for Spiking Neural Networks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-08-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_SparseBEV_High-Performance_Sparse_3D_Object_Detection_from_Multi-Camera_Videos_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_SparseBEV_High-Performance_Sparse_3D_Object_Detection_from_Multi-Camera_Videos_ICCV_2023_paper.pdf">SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-04-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Mirzaei_Reference-guided_Controllable_Inpainting_of_Neural_Radiance_Fields_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Mirzaei_Reference-guided_Controllable_Inpainting_of_Neural_Radiance_Fields_ICCV_2023_paper.pdf">Reference-guided Controllable Inpainting of Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Tyszkiewicz_GECCO_Geometrically-Conditioned_Point_Diffusion_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Tyszkiewicz_GECCO_Geometrically-Conditioned_Point_Diffusion_Models_ICCV_2023_paper.pdf">GECCO: Geometrically-Conditioned Point Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-07-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Revisiting_Scene_Text_Recognition_A_Data_Perspective_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Revisiting_Scene_Text_Recognition_A_Data_Perspective_ICCV_2023_paper.pdf">Revisiting Scene Text Recognition: A Data Perspective</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-08-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_Forecast-MAE_Self-supervised_Pre-training_for_Motion_Forecasting_with_Masked_Autoencoders_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Forecast-MAE_Self-supervised_Pre-training_for_Motion_Forecasting_with_Masked_Autoencoders_ICCV_2023_paper.pdf">Forecast-MAE: Self-supervised Pre-training for Motion Forecasting with Masked Autoencoders</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_CuNeRF_Cube-Based_Neural_Radiance_Field_for_Zero-Shot_Medical_Image_Arbitrary-Scale_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CuNeRF_Cube-Based_Neural_Radiance_Field_for_Zero-Shot_Medical_Image_Arbitrary-Scale_ICCV_2023_paper.pdf">CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image<br />Arbitrary-Scale Super Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-08-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jia_DriveAdapter_Breaking_the_Coupling_Barrier_of_Perception_and_Planning_in_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jia_DriveAdapter_Breaking_the_Coupling_Barrier_of_Perception_and_Planning_in_ICCV_2023_paper.pdf">DriveAdapter: Breaking the Coupling Barrier of Perception and Planning<br />in End-to-End Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Overwriting_Pretrained_Bias_with_Finetuning_Data_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Overwriting_Pretrained_Bias_with_Finetuning_Data_ICCV_2023_paper.pdf">Overwriting Pretrained Bias with Finetuning Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2022-11-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shejwalkar_The_Perils_of_Learning_From_Unlabeled_Data_Backdoor_Attacks_on_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Shejwalkar_The_Perils_of_Learning_From_Unlabeled_Data_Backdoor_Attacks_on_ICCV_2023_paper.pdf">The Perils of Learning From Unlabeled Data: Backdoor Attacks<br />on Semi-supervised Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-04-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Instance_Neural_Radiance_Field_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Instance_Neural_Radiance_Field_ICCV_2023_paper.pdf">Instance Neural Radiance Field</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2022-11-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yoshimura_DynamicISP_Dynamically_Controlled_Image_Signal_Processor_for_Image_Recognition_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yoshimura_DynamicISP_Dynamically_Controlled_Image_Signal_Processor_for_Image_Recognition_ICCV_2023_paper.pdf">DynamicISP: Dynamically Controlled Image Signal Processor for Image Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2022-12-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_SMMix_Self-Motivated_Image_Mixing_for_Vision_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_SMMix_Self-Motivated_Image_Mixing_for_Vision_Transformers_ICCV_2023_paper.pdf">SMMix: Self-Motivated Image Mixing for Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Patel_Pretrained_Language_Models_as_Visual_Planners_for_Human_Assistance_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Patel_Pretrained_Language_Models_as_Visual_Planners_for_Human_Assistance_ICCV_2023_paper.pdf">Pretrained Language Models as Visual Planners for Human Assistance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-06-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lorraine_ATT3D_Amortized_Text-to-3D_Object_Synthesis_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lorraine_ATT3D_Amortized_Text-to-3D_Object_Synthesis_ICCV_2023_paper.pdf">ATT3D: Amortized Text-to-3D Object Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Miao_DDS2M_Self-Supervised_Denoising_Diffusion_Spatio-Spectral_Model_for_Hyperspectral_Image_Restoration_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Miao_DDS2M_Self-Supervised_Denoising_Diffusion_Spatio-Spectral_Model_for_Hyperspectral_Image_Restoration_ICCV_2023_paper.pdf">DDS2M: Self-Supervised Denoising Diffusion Spatio-Spectral Model for Hyperspectral Image<br />Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-01-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Puy_Using_a_Waffle_Iron_for_Automotive_Point_Cloud_Semantic_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Puy_Using_a_Waffle_Iron_for_Automotive_Point_Cloud_Semantic_Segmentation_ICCV_2023_paper.pdf">Using a Waffle Iron for Automotive Point Cloud Semantic<br />Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-07-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Nandan_Unmasking_Anomalies_in_Road-Scene_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Nandan_Unmasking_Anomalies_in_Road-Scene_Segmentation_ICCV_2023_paper.pdf">Unmasking Anomalies in Road-Scene Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2022-11-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/He_BiViT_Extremely_Compressed_Binary_Vision_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/He_BiViT_Extremely_Compressed_Binary_Vision_Transformers_ICCV_2023_paper.pdf">BiViT: Extremely Compressed Binary Vision Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-07-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_CORE_Cooperative_Reconstruction_for_Multi-Agent_Perception_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_CORE_Cooperative_Reconstruction_for_Multi-Agent_Perception_ICCV_2023_paper.pdf">CORE: Cooperative Reconstruction for Multi-Agent Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Open-vocabulary_Panoptic_Segmentation_with_Embedding_Modulation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Open-vocabulary_Panoptic_Segmentation_with_Embedding_Modulation_ICCV_2023_paper.pdf">Open-vocabulary Panoptic Segmentation with Embedding Modulation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-09-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Structure_Invariant_Transformation_for_better_Adversarial_Transferability_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Structure_Invariant_Transformation_for_better_Adversarial_Transferability_ICCV_2023_paper.pdf">Structure Invariant Transformation for better Adversarial Transferability</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_No_Fear_of_Classifier_Biases_Neural_Collapse_Inspired_Federated_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_No_Fear_of_Classifier_Biases_Neural_Collapse_Inspired_Federated_Learning_ICCV_2023_paper.pdf">No Fear of Classifier Biases: Neural Collapse Inspired Federated<br />Learning with Synthetic and Fixed Classifier</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-04-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ge_MetaBEV_Solving_Sensor_Failures_for_3D_Detection_and_Map_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_MetaBEV_Solving_Sensor_Failures_for_3D_Detection_and_Map_Segmentation_ICCV_2023_paper.pdf">MetaBEV: Solving Sensor Failures for BEV Detection and Map<br />Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_Spatio-Temporal_Domain_Awareness_for_Multi-Agent_Collaborative_Perception_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Spatio-Temporal_Domain_Awareness_for_Multi-Agent_Collaborative_Perception_ICCV_2023_paper.pdf">Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wen_Parametric_Classification_for_Generalized_Category_Discovery_A_Baseline_Study_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wen_Parametric_Classification_for_Generalized_Category_Discovery_A_Baseline_Study_ICCV_2023_paper.pdf">Parametric Classification for Generalized Category Discovery: A Baseline Study</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_NeMF_Inverse_Volume_Rendering_with_Neural_Microflake_Field_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_NeMF_Inverse_Volume_Rendering_with_Neural_Microflake_Field_ICCV_2023_paper.pdf">NeMF: Inverse Volume Rendering with Neural Microflake Field</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Spherical_Space_Feature_Decomposition_for_Guided_Depth_Map_Super-Resolution_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Spherical_Space_Feature_Decomposition_for_Guided_Depth_Map_Super-Resolution_ICCV_2023_paper.pdf">Spherical Space Feature Decomposition for Guided Depth Map Super-Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bahmani_CC3D_Layout-Conditioned_Generation_of_Compositional_3D_Scenes_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bahmani_CC3D_Layout-Conditioned_Generation_of_Compositional_3D_Scenes_ICCV_2023_paper.pdf">CC3D: Layout-Conditioned Generation of Compositional 3D Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_Exploring_the_Benefits_of_Visual_Prompting_in_Differential_Privacy_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Exploring_the_Benefits_of_Visual_Prompting_in_Differential_Privacy_ICCV_2023_paper.pdf">Exploring the Benefits of Visual Prompting in Differential Privacy</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2022-06-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Landscape_Learning_for_Neural_Network_Inversion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Landscape_Learning_for_Neural_Network_Inversion_ICCV_2023_paper.pdf">Landscape Learning for Neural Network Inversion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-06-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Unsupervised_Compositional_Concepts_Discovery_with_Text-to-Image_Generative_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Unsupervised_Compositional_Concepts_Discovery_with_Text-to-Image_Generative_Models_ICCV_2023_paper.pdf">Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Cao_Multi-Modal_Continual_Test-Time_Adaptation_for_3D_Semantic_Segmentation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_Multi-Modal_Continual_Test-Time_Adaptation_for_3D_Semantic_Segmentation_ICCV_2023_paper.pdf">Multi-Modal Continual Test-Time Adaptation for 3D Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-04-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Incremental_Generalized_Category_Discovery_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Incremental_Generalized_Category_Discovery_ICCV_2023_paper.pdf">Incremental Generalized Category Discovery</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-09-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_UniSeg_A_Unified_Multi-Modal_LiDAR_Segmentation_Network_and_the_OpenPCSeg_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_UniSeg_A_Unified_Multi-Modal_LiDAR_Segmentation_Network_and_the_OpenPCSeg_ICCV_2023_paper.pdf">UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the<br />OpenPCSeg Codebase</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-05-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Al_Kader_Hammoud_Rapid_Adaptation_in_Online_Continual_Learning_Are_We_Evaluating_It_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Al_Kader_Hammoud_Rapid_Adaptation_in_Online_Continual_Learning_Are_We_Evaluating_It_ICCV_2023_paper.pdf">Rapid Adaptation in Online Continual Learning: Are We Evaluating<br />It Right?</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Guo_Robustifying_Token_Attention_for_Vision_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Robustifying_Token_Attention_for_Vision_Transformers_ICCV_2023_paper.pdf">Robustifying Token Attention for Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liang_Iterative_Prompt_Learning_for_Unsupervised_Backlit_Image_Enhancement_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Iterative_Prompt_Learning_for_Unsupervised_Backlit_Image_Enhancement_ICCV_2023_paper.pdf">Iterative Prompt Learning for Unsupervised Backlit Image Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gupta_ASIC_Aligning_Sparse_in-the-wild_Image_Collections_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gupta_ASIC_Aligning_Sparse_in-the-wild_Image_Collections_ICCV_2023_paper.pdf">ASIC: Aligning Sparse in-the-wild Image Collections</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-04-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhu_Universal_Domain_Adaptation_via_Compressive_Attention_Matching_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Universal_Domain_Adaptation_via_Compressive_Attention_Matching_ICCV_2023_paper.pdf">Universal Domain Adaptation via Compressive Attention Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Yang_BoxSnake_Polygonal_Instance_Segmentation_with_Box_Supervision_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_BoxSnake_Polygonal_Instance_Segmentation_with_Box_Supervision_ICCV_2023_paper.pdf">BoxSnake: Polygonal Instance Segmentation with Box Supervision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Bansal_CleanCLIP_Mitigating_Data_Poisoning_Attacks_in_Multimodal_Contrastive_Learning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bansal_CleanCLIP_Mitigating_Data_Poisoning_Attacks_in_Multimodal_Contrastive_Learning_ICCV_2023_paper.pdf">CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-08-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jiang_Coordinate_Quantized_Neural_Implicit_Representations_for_Multi-view_Reconstruction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Coordinate_Quantized_Neural_Implicit_Representations_for_Multi-view_Reconstruction_ICCV_2023_paper.pdf">Coordinate Quantized Neural Implicit Representations for Multi-view Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-06-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jaeger_Hidden_Biases_of_End-to-End_Driving_Models_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Jaeger_Hidden_Biases_of_End-to-End_Driving_Models_ICCV_2023_paper.pdf">Hidden Biases of End-to-End Driving Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-09-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_GO-SLAM_Global_Optimization_for_Consistent_3D_Instant_Reconstruction_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_GO-SLAM_Global_Optimization_for_Consistent_3D_Instant_Reconstruction_ICCV_2023_paper.pdf">GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-07-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Distilling_Coarse-to-Fine_Semantic_Matching_Knowledge_for_Weakly_Supervised_3D_Visual_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Distilling_Coarse-to-Fine_Semantic_Matching_Knowledge_for_Weakly_Supervised_3D_Visual_ICCV_2023_paper.pdf">Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D<br />Visual Grounding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2022-11-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Dukic_A_Low-Shot_Object_Counting_Network_With_Iterative_Prototype_Adaptation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Dukic_A_Low-Shot_Object_Counting_Network_With_Iterative_Prototype_Adaptation_ICCV_2023_paper.pdf">A Low-Shot Object Counting Network With Iterative Prototype Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-07-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Scaling_Data_Generation_in_Vision-and-Language_Navigation_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Scaling_Data_Generation_in_Vision-and-Language_Navigation_ICCV_2023_paper.pdf">Scaling Data Generation in Vision-and-Language Navigation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-08-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Improving_Generalization_in_Visual_Reinforcement_Learning_via_Conflict-aware_Gradient_Agreement_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Improving_Generalization_in_Visual_Reinforcement_Learning_via_Conflict-aware_Gradient_Agreement_ICCV_2023_paper.pdf">Improving Generalization in Visual Reinforcement Learning via Conflict-aware Gradient<br />Agreement Augmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zong_Temporal_Enhanced_Training_of_Multi-view_3D_Object_Detector_via_Historical_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zong_Temporal_Enhanced_Training_of_Multi-view_3D_Object_Detector_via_Historical_ICCV_2023_paper.pdf">Temporal Enhanced Training of Multi-view 3D Object Detector via<br />Historical Object Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-07-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Tangent_Model_Composition_for_Ensembling_and_Continual_Fine-tuning_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Tangent_Model_Composition_for_Ensembling_and_Continual_Fine-tuning_ICCV_2023_paper.pdf">Tangent Model Composition for Ensembling and Continual Fine-tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Nag_DiffTAD_Temporal_Action_Detection_with_Proposal_Denoising_Diffusion_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Nag_DiffTAD_Temporal_Action_Detection_with_Proposal_Denoising_Diffusion_ICCV_2023_paper.pdf">DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-02-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Chen_Towards_Unifying_Medical_Vision-and-Language_Pre-Training_via_Soft_Prompts_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Towards_Unifying_Medical_Vision-and-Language_Pre-Training_via_Soft_Prompts_ICCV_2023_paper.pdf">Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ni_Deep_Incubation_Training_Large_Models_by_Divide-and-Conquering_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ni_Deep_Incubation_Training_Large_Models_by_Divide-and-Conquering_ICCV_2023_paper.pdf">Deep Incubation: Training Large Models by Divide-and-Conquering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Graph_Matching_with_Bi-level_Noisy_Correspondence_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Graph_Matching_with_Bi-level_Noisy_Correspondence_ICCV_2023_paper.pdf">Graph Matching with Bi-level Noisy Correspondence</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2022-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_RepQ-ViT_Scale_Reparameterization_for_Post-Training_Quantization_of_Vision_Transformers_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Li_RepQ-ViT_Scale_Reparameterization_for_Post-Training_Quantization_of_Vision_Transformers_ICCV_2023_paper.pdf">RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-08-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Ding_MeViS_A_Large-scale_Benchmark_for_Video_Segmentation_with_Motion_Expressions_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_MeViS_A_Large-scale_Benchmark_for_Video_Segmentation_with_Motion_Expressions_ICCV_2023_paper.pdf">MeViS: A Large-scale Benchmark for Video Segmentation with Motion<br />Expressions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_UMC_A_Unified_Bandwidth-efficient_and_Multi-resolution_based_Collaborative_Perception_Framework_ICCV_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_UMC_A_Unified_Bandwidth-efficient_and_Multi-resolution_based_Collaborative_Perception_Framework_ICCV_2023_paper.pdf">UMC: A Unified Bandwidth-efficient and Multi-resolution based Collaborative Perception<br />Framework</a></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../EMNLP/EMNLP_2022/" class="btn btn-neutral float-left" title="EMNLP 2022"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../ICLR/ICLR_2023/" class="btn btn-neutral float-right" title="ICLR 2023">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../EMNLP/EMNLP_2022/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../ICLR/ICLR_2023/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
