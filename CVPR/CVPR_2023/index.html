<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>CVPR 2023 - AI Papers (top)</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../mytheme.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "CVPR 2023";
        var mkdocs_page_input_path = "CVPR/CVPR_2023.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> AI Papers (top)
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">AI Papers (Top)</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">ACL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ACL/ACL_2022/">ACL 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ACL/ACL_2023/">ACL 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">CVPR</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../CVPR_2022/">CVPR 2022</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">CVPR 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">EMNLP</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../EMNLP/EMNLP_2022/">EMNLP 2022</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICCV</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICCV/ICCV_2023/">ICCV 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICLR</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2022/">ICLR 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2023/">ICLR 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICML</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2022/">ICML 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2023/">ICML 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">NeurIPS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../NeurIPS/NeurIPS_2022/">NeurIPS 2022</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">AI Papers (top)</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">CVPR</li>
      <li class="breadcrumb-item active">CVPR 2023</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <p>Last updated: 2023-10-11 15:18:57. Maintained by <a href="https://wayson.tech/">Weisen Jiang</a>.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">citation</th>
<th style="text-align: center;">date</th>
<th style="text-align: center;">review</th>
<th style="text-align: left;">title (pdf)</th>
<th style="text-align: left;">authors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1253</td>
<td style="text-align: center;">2022-07-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_YOLOv7_Trainable_Bag-of-Freebies_Sets_New_State-of-the-Art_for_Real-Time_Object_Detectors_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_YOLOv7_Trainable_Bag-of-Freebies_Sets_New_State-of-the-Art_for_Real-Time_Object_Detectors_CVPR_2023_paper.pdf">YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object<br />Detectors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">556</td>
<td style="text-align: center;">2022-08-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf">DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">277</td>
<td style="text-align: center;">2022-10-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf">Imagic: Text-Based Real Image Editing with Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">251</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf">InstructPix2Pix: Learning to Follow Image Editing Instructions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">199</td>
<td style="text-align: center;">2022-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Magic3D_High-Resolution_Text-to-3D_Content_Creation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Magic3D_High-Resolution_Text-to-3D_Content_Creation_CVPR_2023_paper.pdf">Magic3D: High-Resolution Text-to-3D Content Creation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">157</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mokady_NULL-Text_Inversion_for_Editing_Real_Images_Using_Guided_Diffusion_Models_CVPR_2023_paper.pdf">Null-text Inversion for Editing Real Images using Guided Diffusion<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">136</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf">Multi-Concept Customization of Text-to-Image Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">2022-11-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.pdf">EVA: Exploring the Limits of Masked Visual Representation Learning<br />at Scale</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">117</td>
<td style="text-align: center;">2022-06-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Mask_DINO_Towards_a_Unified_Transformer-Based_Framework_for_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Mask_DINO_Towards_a_Unified_Transformer-Based_Framework_for_Object_Detection_CVPR_2023_paper.pdf">Mask DINO: Towards A Unified Transformer-based Framework for Object<br />Detection and Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">111</td>
<td style="text-align: center;">2022-11-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_InternImage_Exploring_Large-Scale_Vision_Foundation_Models_With_Deformable_Convolutions_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_InternImage_Exploring_Large-Scale_Vision_Foundation_Models_With_Deformable_Convolutions_CVPR_2023_paper.pdf">InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">109</td>
<td style="text-align: center;">2022-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cao_Observation-Centric_SORT_Rethinking_SORT_for_Robust_Multi-Object_Tracking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Observation-Centric_SORT_Rethinking_SORT_for_Robust_Multi-Object_Tracking_CVPR_2023_paper.pdf">Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">109</td>
<td style="text-align: center;">2023-05-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf">ImageBind One Embedding Space to Bind Them All</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">2022-05-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Activating_More_Pixels_in_Image_Super-Resolution_Transformer_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Activating_More_Pixels_in_Image_Super-Resolution_Transformer_CVPR_2023_paper.pdf">Activating More Pixels in Image Super-Resolution Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">93</td>
<td style="text-align: center;">2022-10-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2023_paper.pdf">On Distillation of Guided Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">90</td>
<td style="text-align: center;">2022-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper.pdf">Reproducible Scaling Laws for Contrastive Language-Image Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">90</td>
<td style="text-align: center;">2022-11-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Metzer_Latent-NeRF_for_Shape-Guided_Generation_of_3D_Shapes_and_Textures_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Metzer_Latent-NeRF_for_Shape-Guided_Generation_of_3D_Shapes_and_Textures_CVPR_2023_paper.pdf">Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">90</td>
<td style="text-align: center;">2023-01-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.pdf">GLIGEN: Open-Set Grounded Text-to-Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">87</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Score_Jacobian_Chaining_Lifting_Pretrained_2D_Diffusion_Models_for_3D_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Score_Jacobian_Chaining_Lifting_Pretrained_2D_Diffusion_Models_for_3D_CVPR_2023_paper.pdf">Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for<br />3D Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">85</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tumanyan_Plug-and-Play_Diffusion_Features_for_Text-Driven_Image-to-Image_Translation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tumanyan_Plug-and-Play_Diffusion_Features_for_Text-Driven_Image-to-Image_Translation_CVPR_2023_paper.pdf">Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2023-04-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Blattmann_Align_Your_Latents_High-Resolution_Video_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Blattmann_Align_Your_Latents_High-Resolution_Video_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.pdf">Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">76</td>
<td style="text-align: center;">2022-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.pdf">Diffusion Art or Digital Forgery? Investigating Data Replication in<br />Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">74</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf">Scaling up GANs for Text-to-Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2022-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.pdf">All in One: Exploring Unified Video-Language Pre-Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.pdf">Scaling Language-Image Pre-Training via Masking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Deitke_Objaverse_A_Universe_of_Annotated_3D_Objects_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Deitke_Objaverse_A_Universe_of_Annotated_3D_Objects_CVPR_2023_paper.pdf">Objaverse: A Universe of Annotated 3D Objects</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2022-07-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_MobileNeRF_Exploiting_the_Polygon_Rasterization_Pipeline_for_Efficient_Neural_Field_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_MobileNeRF_Exploiting_the_Polygon_Rasterization_Pipeline_for_Efficient_Neural_Field_CVPR_2023_paper.pdf">MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural<br />Field Rendering on Mobile Architectures</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2023-01-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.pdf">ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-10-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liang_Open-Vocabulary_Semantic_Segmentation_With_Mask-Adapted_CLIP_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Open-Vocabulary_Semantic_Segmentation_With_Mask-Adapted_CLIP_CVPR_2023_paper.pdf">Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-10-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.pdf">MaPLe: Multi-modal Prompt Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2022-04-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hassani_Neighborhood_Attention_Transformer_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hassani_Neighborhood_Attention_Transformer_CVPR_2023_paper.pdf">Neighborhood Attention Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2022-11-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jain_OneFormer_One_Transformer_To_Rule_Universal_Image_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_OneFormer_One_Transformer_To_Rule_Universal_Image_Segmentation_CVPR_2023_paper.pdf">OneFormer: One Transformer to Rule Universal Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">58</td>
<td style="text-align: center;">2023-03-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Open-Vocabulary_Panoptic_Segmentation_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Open-Vocabulary_Panoptic_Segmentation_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf">Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">55</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf">Paint by Example: Exemplar-based Image Editing with Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">54</td>
<td style="text-align: center;">2022-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jia_DETRs_With_Hybrid_Matching_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jia_DETRs_With_Hybrid_Matching_CVPR_2023_paper.pdf">DETRs with Hybrid Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-03-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Takagi_High-Resolution_Image_Reconstruction_With_Latent_Diffusion_Models_From_Human_Brain_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Takagi_High-Resolution_Image_Reconstruction_With_Latent_Diffusion_Models_From_Human_Brain_CVPR_2023_paper.pdf">High-resolution image reconstruction with latent diffusion models from human<br />brain activity</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2022-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.pdf">Planning-oriented Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">52</td>
<td style="text-align: center;">2022-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf">Visual Programming: Compositional visual reasoning without training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2022-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_RODIN_A_Generative_Model_for_Sculpting_3D_Digital_Avatars_Using_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_RODIN_A_Generative_Model_for_Sculpting_3D_Digital_Avatars_Using_CVPR_2023_paper.pdf">RODIN: A Generative Model for Sculpting 3D Digital Avatars<br />Using Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-01-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fridovich-Keil_K-Planes_Explicit_Radiance_Fields_in_Space_Time_and_Appearance_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fridovich-Keil_K-Planes_Explicit_Radiance_Fields_in_Space_Time_and_Appearance_CVPR_2023_paper.pdf">K-Planes: Explicit Radiance Fields in Space, Time, and Appearance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2022-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.pdf">Generalized Decoding for Pixel, Image, and Language</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2022-05-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_Revealing_the_Dark_Secrets_of_Masked_Image_Modeling_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Revealing_the_Dark_Secrets_of_Masked_Image_Modeling_CVPR_2023_paper.pdf">Revealing the Dark Secrets of Masked Image Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2022-08-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dong_MaskCLIP_Masked_Self-Distillation_Advances_Contrastive_Language-Image_Pretraining_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_MaskCLIP_Masked_Self-Distillation_Advances_Contrastive_Language-Image_Pretraining_CVPR_2023_paper.pdf">MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2022-12-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Dream3D_Zero-Shot_Text-to-3D_Synthesis_Using_3D_Shape_Prior_and_Text-to-Image_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Dream3D_Zero-Shot_Text-to-3D_Synthesis_Using_3D_Shape_Prior_and_Text-to-Image_CVPR_2023_paper.pdf">Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and<br />Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2022-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Peng_OpenScene_3D_Scene_Understanding_With_Open_Vocabularies_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_OpenScene_3D_Scene_Understanding_With_Open_Vocabularies_CVPR_2023_paper.pdf">OpenScene: 3D Scene Understanding with Open Vocabularies</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2022-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.pdf">SpaText: Spatio-Textual Representation for Controllable Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2022-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tseng_EDGE_Editable_Dance_Generation_From_Music_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tseng_EDGE_Editable_Dance_Generation_From_Music_CVPR_2023_paper.pdf">EDGE: Editable Dance Generation From Music</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-02-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Melas-Kyriazi_RealFusion_360deg_Reconstruction_of_Any_Object_From_a_Single_Image_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Melas-Kyriazi_RealFusion_360deg_Reconstruction_of_Any_Object_From_a_Single_Image_CVPR_2023_paper.pdf">RealFusion 360° Reconstruction of Any Object from a Single<br />Image</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2022-10-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_ERNIE-ViLG_2.0_Improving_Text-to-Image_Diffusion_Model_With_Knowledge-Enhanced_Mixture-of-Denoising-Experts_CVPR_2023_paper.pdf">ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2022-06-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tao_Siamese_Image_Modeling_for_Self-Supervised_Vision_Representation_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_Siamese_Image_Modeling_for_Self-Supervised_Vision_Representation_Learning_CVPR_2023_paper.pdf">Siamese Image Modeling for Self-Supervised Vision Representation Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2022-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Muller_DiffRF_Rendering-Guided_3D_Radiance_Field_Diffusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Muller_DiffRF_Rendering-Guided_3D_Radiance_Field_Diffusion_CVPR_2023_paper.pdf">DiffRF: Rendering-Guided 3D Radiance Field Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2022-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Girdhar_OmniMAE_Single_Model_Masked_Pretraining_on_Images_and_Videos_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_OmniMAE_Single_Model_Masked_Pretraining_on_Images_and_Videos_CVPR_2023_paper.pdf">OmniMAE: Single Model Masked Pretraining on Images and Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2022-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_NeuralLift-360_Lifting_an_In-the-Wild_2D_Photo_to_a_3D_Object_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_NeuralLift-360_Lifting_an_In-the-Wild_2D_Photo_to_a_3D_Object_CVPR_2023_paper.pdf">NeuralLift-360: Lifting an in-the-Wild 2D Photo to A 3D<br />Object with 360° Views</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2022-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shue_3D_Neural_Field_Generation_Using_Triplane_Diffusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shue_3D_Neural_Field_Generation_Using_Triplane_Diffusion_CVPR_2023_paper.pdf">3D Neural Field Generation Using Triplane Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_SparseFusion_Distilling_View-Conditioned_Diffusion_for_3D_Reconstruction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_SparseFusion_Distilling_View-Conditioned_Diffusion_for_3D_Reconstruction_CVPR_2023_paper.pdf">SparseFusion: Distilling View-Conditioned Diffusion for 3D Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2021-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mukhoti_Deep_Deterministic_Uncertainty_A_New_Simple_Baseline_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mukhoti_Deep_Deterministic_Uncertainty_A_New_Simple_Baseline_CVPR_2023_paper.pdf">Deep Deterministic Uncertainty: A New Simple Baseline</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Luo_VideoFusion_Decomposed_Diffusion_Models_for_High-Quality_Video_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_VideoFusion_Decomposed_Diffusion_Models_for_High-Quality_Video_Generation_CVPR_2023_paper.pdf">VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cheng_SDFusion_Multimodal_3D_Shape_Completion_Reconstruction_and_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_SDFusion_Multimodal_3D_Shape_Completion_Reconstruction_and_Generation_CVPR_2023_paper.pdf">SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Executing_Your_Commands_via_Motion_Diffusion_in_Latent_Space_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Executing_Your_Commands_via_Motion_Diffusion_in_Latent_Space_CVPR_2023_paper.pdf">Executing your Commands via Motion Diffusion in Latent Space</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf">SINE: SINgle Image Editing with Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2022-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.pdf">ULIP: Learning a Unified Representation of Language, Images, and<br />Point Clouds for 3D Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2022-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_NICO_Towards_Better_Benchmarking_for_Domain_Generalization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_NICO_Towards_Better_Benchmarking_for_Domain_Generalization_CVPR_2023_paper.pdf">NICO++: Towards Better Benchmarking for Domain Generalization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2022-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVPR_2023_paper.pdf">Images Speak in Images: A Generalist Painter for In-Context<br />Visual Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2022-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.pdf">BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition<br />via Perspective Supervision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-03-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Run_Dont_Walk_Chasing_Higher_FLOPS_for_Faster_Neural_Networks_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Run_Dont_Walk_Chasing_Higher_FLOPS_for_Faster_Neural_Networks_CVPR_2023_paper.pdf">Run, Don't Walk: Chasing Higher FLOPS for Faster Neural<br />Networks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2022-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Learning_3D_Representations_From_2D_Pre-Trained_Models_via_Image-to-Point_Masked_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_3D_Representations_From_2D_Pre-Trained_Models_via_Image-to-Point_Masked_CVPR_2023_paper.pdf">Learning 3D Representations from 2D Pre-Trained Models via Image-to-Point<br />Masked Autoencoders</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_VideoMAE_V2_Scaling_Video_Masked_Autoencoders_With_Dual_Masking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_VideoMAE_V2_Scaling_Video_Masked_Autoencoders_With_Dual_Masking_CVPR_2023_paper.pdf">VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2023-01-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf">DepGraph: Towards Any Structural Pruning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2023-01-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cao_HexPlane_A_Fast_Representation_for_Dynamic_Scenes_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_HexPlane_A_Fast_Representation_for_Dynamic_Scenes_CVPR_2023_paper.pdf">HexPlane: A Fast Representation for Dynamic Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2023-02-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.pdf">Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2022-11-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Schramowski_Safe_Latent_Diffusion_Mitigating_Inappropriate_Degeneration_in_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Schramowski_Safe_Latent_Diffusion_Mitigating_Inappropriate_Degeneration_in_Diffusion_Models_CVPR_2023_paper.pdf">Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2023-01-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Diffusion-Based_Generation_Optimization_and_Planning_in_3D_Scenes_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Diffusion-Based_Generation_Optimization_and_Planning_in_3D_Scenes_CVPR_2023_paper.pdf">Diffusion-based Generation, Optimization, and Planning in 3D Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">31</td>
<td style="text-align: center;">2022-06-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_LAVENDER_Unifying_Video-Language_Understanding_As_Masked_Language_Modeling_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_LAVENDER_Unifying_Video-Language_Understanding_As_Masked_Language_Modeling_CVPR_2023_paper.pdf">LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2023-01-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Generating_Human_Motion_From_Textual_Descriptions_With_Discrete_Representations_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Generating_Human_Motion_From_Textual_Descriptions_With_Discrete_Representations_CVPR_2023_paper.pdf">Generating Human Motion from Textual Descriptions with Discrete Representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2022-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Imagen_Editor_and_EditBench_Advancing_and_Evaluating_Text-Guided_Image_Inpainting_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Imagen_Editor_and_EditBench_Advancing_and_Evaluating_Text-Guided_Image_Inpainting_CVPR_2023_paper.pdf">Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image<br />Inpainting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2022-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf">ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">29</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_SmartBrush_Text_and_Shape_Guided_Object_Inpainting_With_Diffusion_Model_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_SmartBrush_Text_and_Shape_Guided_Object_Inpainting_With_Diffusion_Model_CVPR_2023_paper.pdf">SmartBrush: Text and Shape Guided Object Inpainting with Diffusion<br />Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Anciukevicius_RenderDiffusion_Image_Diffusion_for_3D_Reconstruction_Inpainting_and_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Anciukevicius_RenderDiffusion_Image_Diffusion_for_3D_Reconstruction_Inpainting_and_Generation_CVPR_2023_paper.pdf">RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.pdf">Prompt, Generate, Then Cache: Cascade of Foundation Models Makes<br />Strong Few-Shot Learners</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2023-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_VoxFormer_Sparse_Voxel_Transformer_for_Camera-Based_3D_Semantic_Scene_Completion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_VoxFormer_Sparse_Voxel_Transformer_for_Camera-Based_3D_Semantic_Scene_Completion_CVPR_2023_paper.pdf">VoxFormer: Sparse Voxel Transformer for Camera-Based 3D Semantic Scene<br />Completion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wallace_EDICT_Exact_Diffusion_Inversion_via_Coupled_Transformations_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wallace_EDICT_Exact_Diffusion_Inversion_via_Coupled_Transformations_CVPR_2023_paper.pdf">EDICT: Exact Diffusion Inversion via Coupled Transformations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">28</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Deng_NeRDi_Single-View_NeRF_Synthesis_With_Language-Guided_Diffusion_As_General_Image_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_NeRDi_Single-View_NeRF_Synthesis_With_Language-Guided_Diffusion_As_General_Image_CVPR_2023_paper.pdf">NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General<br />Image Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2021-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Boutros_CR-FIQA_Face_Image_Quality_Assessment_by_Learning_Sample_Relative_Classifiability_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Boutros_CR-FIQA_Face_Image_Quality_Assessment_by_Learning_Sample_Relative_Classifiability_CVPR_2023_paper.pdf">CR-FIQA: Face Image Quality Assessment by Learning Sample Relative<br />Classifiability</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2022-11-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Seeing_Beyond_the_Brain_Conditional_Diffusion_Model_With_Sparse_Masked_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Seeing_Beyond_the_Brain_Conditional_Diffusion_Model_With_Sparse_Masked_CVPR_2023_paper.pdf">Seeing Beyond the Brain: Conditional Diffusion Model with Sparse<br />Masked Modeling for Vision Decoding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Side_Adapter_Network_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Side_Adapter_Network_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2023_paper.pdf">Side Adapter Network for Open-Vocabulary Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">27</td>
<td style="text-align: center;">2023-01-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Cut_and_Learn_for_Unsupervised_Object_Detection_and_Instance_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Cut_and_Learn_for_Unsupervised_Object_Detection_and_Instance_Segmentation_CVPR_2023_paper.pdf">Cut and Learn for Unsupervised Object Detection and Instance<br />Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">26</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dabral_Mofusion_A_Framework_for_Denoising-Diffusion-Based_Motion_Synthesis_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dabral_Mofusion_A_Framework_for_Denoising-Diffusion-Based_Motion_Synthesis_CVPR_2023_paper.pdf">MoFusion: A Framework for Denoising-Diffusion-Based Motion Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Goyal_Finetune_Like_You_Pretrain_Improved_Finetuning_of_Zero-Shot_Vision_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Goyal_Finetune_Like_You_Pretrain_Improved_Finetuning_of_Zero-Shot_Vision_Models_CVPR_2023_paper.pdf">Finetune like you pretrain: Improved finetuning of zero-shot vision<br />models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2022-12-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ruan_MM-Diffusion_Learning_Multi-Modal_Diffusion_Models_for_Joint_Audio_and_Video_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ruan_MM-Diffusion_Learning_Multi-Modal_Diffusion_Models_for_Joint_Audio_and_Video_CVPR_2023_paper.pdf">MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and<br />Video Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2022-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hoyer_MIC_Masked_Image_Consistency_for_Context-Enhanced_Domain_Adaptation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hoyer_MIC_Masked_Image_Consistency_for_Context-Enhanced_Domain_Adaptation_CVPR_2023_paper.pdf">MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2023-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yan_Universal_Instance_Perception_As_Object_Discovery_and_Retrieval_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Universal_Instance_Perception_As_Object_Discovery_and_Retrieval_CVPR_2023_paper.pdf">Universal Instance Perception as Object Discovery and Retrieval</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2022-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ding_PLA_Language-Driven_Open-Vocabulary_3D_Scene_Understanding_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_PLA_Language-Driven_Open-Vocabulary_3D_Scene_Understanding_CVPR_2023_paper.pdf">PLA: Language-Driven Open-Vocabulary 3D Scene Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2023-01-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Assran_Self-Supervised_Learning_From_Images_With_a_Joint-Embedding_Predictive_Architecture_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Assran_Self-Supervised_Learning_From_Images_With_a_Joint-Embedding_Predictive_Architecture_CVPR_2023_paper.pdf">Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">23</td>
<td style="text-align: center;">2022-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.pdf">NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Next3D_Generative_Neural_Texture_Rasterization_for_3D-Aware_Head_Avatars_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Next3D_Generative_Neural_Texture_Rasterization_for_3D-Aware_Head_Avatars_CVPR_2023_paper.pdf">Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2022-12-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Siddiqui_Panoptic_Lifting_for_3D_Scene_Understanding_With_Neural_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Siddiqui_Panoptic_Lifting_for_3D_Scene_Understanding_With_Neural_Fields_CVPR_2023_paper.pdf">Panoptic Lifting for 3D Scene Understanding with Neural Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2022-10-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yadav_Habitat-Matterport_3D_Semantics_Dataset_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yadav_Habitat-Matterport_3D_Semantics_Dataset_CVPR_2023_paper.pdf">Habitat-Matterport 3D Semantics Dataset</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.pdf">ReCo: Region-Controlled Text-to-Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yi_Generating_Holistic_3D_Human_Motion_From_Speech_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_Generating_Holistic_3D_Human_Motion_From_Speech_CVPR_2023_paper.pdf">Generating Holistic 3D Human Motion from Speech</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2022-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sariyildiz_Fake_It_Till_You_Make_It_Learning_Transferable_Representations_From_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sariyildiz_Fake_It_Till_You_Make_It_Learning_Transferable_Representations_From_CVPR_2023_paper.pdf">Fake it Till You Make it: Learning Transferable Representations<br />from Synthetic ImageNet Clones</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">22</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Smith_CODA-Prompt_COntinual_Decomposed_Attention-Based_Prompting_for_Rehearsal-Free_Continual_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_CODA-Prompt_COntinual_Decomposed_Attention-Based_Prompting_for_Rehearsal-Free_Continual_Learning_CVPR_2023_paper.pdf">CODA-Prompt: COntinual Decomposed Attention-Based Prompting for Rehearsal-Free Continual Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_VoxelNeXt_Fully_Sparse_VoxelNet_for_3D_Object_Detection_and_Tracking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_VoxelNeXt_Fully_Sparse_VoxelNet_for_3D_Object_Detection_and_Tracking_CVPR_2023_paper.pdf">VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and<br />Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2022-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.pdf">CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2022-12-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_DisCoScene_Spatially_Disentangled_Generative_Radiance_Fields_for_Controllable_3D-Aware_Scene_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_DisCoScene_Spatially_Disentangled_Generative_Radiance_Fields_for_Controllable_3D-Aware_Scene_CVPR_2023_paper.pdf">DisCoScene: Spatially Disentangled Generative Radiance Fields for Controllable 3D-aware<br />Scene Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2022-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.pdf">MAGVIT: Masked Generative Video Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Learning_Video_Representations_From_Large_Language_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Learning_Video_Representations_From_Large_Language_Models_CVPR_2023_paper.pdf">Learning Video Representations from Large Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2022-08-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Revisiting_Weak-to-Strong_Consistency_in_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Revisiting_Weak-to-Strong_Consistency_in_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2022-11-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_MAGE_MAsked_Generative_Encoder_To_Unify_Representation_Learning_and_Image_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MAGE_MAsked_Generative_Encoder_To_Unify_Representation_Learning_and_Image_CVPR_2023_paper.pdf">MAGE: MAsked Generative Encoder to Unify Representation Learning and<br />Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2022-06-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_On_Data_Scaling_in_Masked_Image_Modeling_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_On_Data_Scaling_in_Masked_Image_Modeling_CVPR_2023_paper.pdf">On Data Scaling in Masked Image Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2022-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gong_DiffPose_Toward_More_Reliable_3D_Pose_Estimation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gong_DiffPose_Toward_More_Reliable_3D_Pose_Estimation_CVPR_2023_paper.pdf">DiffPose: Toward More Reliable 3D Pose Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2022-08-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kong_Understanding_Masked_Image_Modeling_via_Learning_Occlusion_Invariant_Feature_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_Understanding_Masked_Image_Modeling_via_Learning_Occlusion_Invariant_Feature_CVPR_2023_paper.pdf">Understanding Masked Image Modeling via Learning Occlusion Invariant Feature</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_V2V4Real_A_Real-World_Large-Scale_Dataset_for_Vehicle-to-Vehicle_Cooperative_Perception_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_V2V4Real_A_Real-World_Large-Scale_Dataset_for_Vehicle-to-Vehicle_Cooperative_Perception_CVPR_2023_paper.pdf">V2V4Real: A Real-World Large-Scale Dataset for Vehicle-to-Vehicle Cooperative Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Radenovic_Filtering_Distillation_and_Hard_Negatives_for_Vision-Language_Pre-Training_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Radenovic_Filtering_Distillation_and_Hard_Negatives_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf">Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-01-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.pdf">CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2022-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tang_Unifying_Vision_Text_and_Layout_for_Universal_Document_Processing_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Unifying_Vision_Text_and_Layout_for_Universal_Document_Processing_CVPR_2023_paper.pdf">Unifying Vision, Text, and Layout for Universal Document Processing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">2023-01-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf">GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yin_3D_GAN_Inversion_With_Facial_Symmetry_Prior_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_3D_GAN_Inversion_With_Facial_Symmetry_Prior_CVPR_2023_paper.pdf">3D GAN Inversion with Facial Symmetry Prior</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chung_Parallel_Diffusion_Models_of_Operator_and_Image_for_Blind_Inverse_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chung_Parallel_Diffusion_Models_of_Operator_and_Image_for_Blind_Inverse_CVPR_2023_paper.pdf">Parallel Diffusion Models of Operator and Image for Blind<br />Inverse Problems</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Beyer_FlexiViT_One_Model_for_All_Patch_Sizes_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Beyer_FlexiViT_One_Model_for_All_Patch_Sizes_CVPR_2023_paper.pdf">FlexiViT: One Model for All Patch Sizes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_BiFormer_Vision_Transformer_With_Bi-Level_Routing_Attention_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_BiFormer_Vision_Transformer_With_Bi-Level_Routing_Attention_CVPR_2023_paper.pdf">BiFormer: Vision Transformer with Bi-Level Routing Attention</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chung_Solving_3D_Inverse_Problems_Using_Pre-Trained_2D_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chung_Solving_3D_Inverse_Problems_Using_Pre-Trained_2D_Diffusion_Models_CVPR_2023_paper.pdf">Solving 3D Inverse Problems Using Pre-Trained 2D Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf">Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-11-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fan_OpenGait_Revisiting_Gait_Recognition_Towards_Better_Practicality_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_OpenGait_Revisiting_Gait_Recognition_Towards_Better_Practicality_CVPR_2023_paper.pdf">OpenGait: Revisiting Gait Recognition Toward Better Practicality</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Truong_SPARF_Neural_Radiance_Fields_From_Sparse_and_Noisy_Poses_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Truong_SPARF_Neural_Radiance_Fields_From_Sparse_and_Noisy_Poses_CVPR_2023_paper.pdf">SPARF: Neural Radiance Fields from Sparse and Noisy Poses</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-10-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf">Visual Prompt Tuning for Generative Transfer Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2023-02-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.pdf">Video Probabilistic Diffusion Models in Projected Latent Space</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ma_CREPE_Can_Vision-Language_Foundation_Models_Reason_Compositionally_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_CREPE_Can_Vision-Language_Foundation_Models_Reason_Compositionally_CVPR_2023_paper.pdf">@ CREPE: Can Vision-Language Foundation Models Reason Compositionally?</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Language_in_a_Bottle_Language_Model_Guided_Concept_Bottlenecks_for_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Language_in_a_Bottle_Language_Model_Guided_Concept_Bottlenecks_for_CVPR_2023_paper.pdf">Language in a Bottle: Language Model Guided Concept Bottlenecks<br />for Interpretable Image Classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gehrig_Recurrent_Vision_Transformers_for_Object_Detection_With_Event_Cameras_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gehrig_Recurrent_Vision_Transformers_for_Object_Detection_With_Event_Cameras_CVPR_2023_paper.pdf">Recurrent Vision Transformers for Object Detection with Event Cameras</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2022-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xiong_FedDM_Iterative_Distribution_Matching_for_Communication-Efficient_Federated_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_FedDM_Iterative_Distribution_Matching_for_Communication-Efficient_Federated_Learning_CVPR_2023_paper.pdf">FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2023-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Vid2Seq_Large-Scale_Pretraining_of_a_Visual_Language_Model_for_Dense_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Vid2Seq_Large-Scale_Pretraining_of_a_Visual_Language_Model_for_Dense_CVPR_2023_paper.pdf">Vid2Seq: Large-Scale Pretraining of a Visual Language Model for<br />Dense Video Captioning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.pdf">Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and<br />Vision-Language Tasks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2022-09-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fu_An_Empirical_Study_of_End-to-End_Video-Language_Transformers_With_Masked_Visual_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_An_Empirical_Study_of_End-to-End_Video-Language_Transformers_With_Masked_Visual_CVPR_2023_paper.pdf">An Empirical Study of End-to-End Video-Language Transformers with Masked<br />Visual Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2022-04-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.pdf">ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2022-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_High-Fidelity_3D_GAN_Inversion_by_Pseudo-Multi-View_Optimization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_High-Fidelity_3D_GAN_Inversion_by_Pseudo-Multi-View_Optimization_CVPR_2023_paper.pdf">High-fidelity 3D GAN Inversion by Pseudo-multi-view Optimization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jain_VectorFusion_Text-to-SVG_by_Abstracting_Pixel-Based_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_VectorFusion_Text-to-SVG_by_Abstracting_Pixel-Based_Diffusion_Models_CVPR_2023_paper.pdf">VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.pdf">Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2022-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Diffusion_Probabilistic_Model_Made_Slim_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Diffusion_Probabilistic_Model_Made_Slim_CVPR_2023_paper.pdf">Diffusion Probabilistic Model Made Slim</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_A_Whac-a-Mole_Dilemma_Shortcuts_Come_in_Multiples_Where_Mitigating_One_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_A_Whac-a-Mole_Dilemma_Shortcuts_Come_in_Multiples_Where_Mitigating_One_CVPR_2023_paper.pdf">A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating<br />One Amplifies Others</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/He_Camouflaged_Object_Detection_With_Feature_Decomposition_and_Edge_Reconstruction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Camouflaged_Object_Detection_With_Feature_Decomposition_and_Edge_Reconstruction_CVPR_2023_paper.pdf">Camouflaged Object Detection with Feature Decomposition and Edge Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Taming_Diffusion_Models_for_Audio-Driven_Co-Speech_Gesture_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Taming_Diffusion_Models_for_Audio-Driven_Co-Speech_Gesture_Generation_CVPR_2023_paper.pdf">Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tschannen_CLIPPO_Image-and-Language_Understanding_From_Pixels_Only_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tschannen_CLIPPO_Image-and-Language_Understanding_From_Pixels_Only_CVPR_2023_paper.pdf">CLIPPO: Image-and-Language Understanding from Pixels Only</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-01-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xing_CodeTalker_Speech-Driven_3D_Facial_Animation_With_Discrete_Motion_Prior_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xing_CodeTalker_Speech-Driven_3D_Facial_Animation_With_Discrete_Motion_Prior_CVPR_2023_paper.pdf">CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_FreeNeRF_Improving_Few-Shot_Neural_Rendering_With_Free_Frequency_Regularization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_FreeNeRF_Improving_Few-Shot_Neural_Rendering_With_Free_Frequency_Regularization_CVPR_2023_paper.pdf">FreeNeRF: Improving Few-Shot Neural Rendering with Free Frequency Regularization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">17</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shao_Tensor4D_Efficient_Neural_4D_Decomposition_for_High-Fidelity_Dynamic_Reconstruction_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shao_Tensor4D_Efficient_Neural_4D_Decomposition_for_High-Fidelity_Dynamic_Reconstruction_and_CVPR_2023_paper.pdf">Tensor4D: Efficient Neural 4D Decomposition for High-Fidelity Dynamic Reconstruction<br />and Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/VS_Instance_Relation_Graph_Guided_Source-Free_Domain_Adaptive_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/VS_Instance_Relation_Graph_Guided_Source-Free_Domain_Adaptive_Object_Detection_CVPR_2023_paper.pdf">Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.pdf">SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-08-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gu_ViP3D_End-to-End_Visual_Trajectory_Prediction_via_3D_Agent_Queries_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_ViP3D_End-to-End_Visual_Trajectory_Prediction_via_3D_Agent_Queries_CVPR_2023_paper.pdf">ViP3D: End-to-End Visual Trajectory Prediction via 3D Agent Queries</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Diffusion-SDF_Text-To-Shape_via_Voxelized_Diffusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Diffusion-SDF_Text-To-Shape_via_Voxelized_Diffusion_CVPR_2023_paper.pdf">Diffusion-SDF: Text-to-Shape via Voxelized Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-01-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Multiview_Compressive_Coding_for_3D_Reconstruction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Multiview_Compressive_Coding_for_3D_Reconstruction_CVPR_2023_paper.pdf">Multiview Compressive Coding for 3D Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gadre_CoWs_on_Pasture_Baselines_and_Benchmarks_for_Language-Driven_Zero-Shot_Object_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gadre_CoWs_on_Pasture_Baselines_and_Benchmarks_for_Language-Driven_Zero-Shot_Object_CVPR_2023_paper.pdf">CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot<br />Object Navigation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Rasheed_Fine-Tuned_CLIP_Models_Are_Efficient_Video_Learners_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rasheed_Fine-Tuned_CLIP_Models_Are_Efficient_Video_Learners_CVPR_2023_paper.pdf">Fine-tuned CLIP Models are Efficient Video Learners</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Visual_Prompt_Multi-Modal_Tracking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Visual_Prompt_Multi-Modal_Tracking_CVPR_2023_paper.pdf">Visual Prompt Multi-Modal Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-05-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_PointDistiller_Structured_Knowledge_Distillation_Towards_Efficient_and_Compact_3D_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PointDistiller_Structured_Knowledge_Distillation_Towards_Efficient_and_Compact_3D_Detection_CVPR_2023_paper.pdf">PointDistiller: Structured Knowledge Distillation Towards Efficient and Compact 3D<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-01-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Multimodality_Helps_Unimodality_Cross-Modal_Few-Shot_Learning_With_Multimodal_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Multimodality_Helps_Unimodality_Cross-Modal_Few-Shot_Learning_With_Multimodal_Models_CVPR_2023_paper.pdf">Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_PointAvatar_Deformable_Point-Based_Head_Avatars_From_Videos_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_PointAvatar_Deformable_Point-Based_Head_Avatars_From_Videos_CVPR_2023_paper.pdf">PointAvatar: Deformable Point-Based Head Avatars from Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mirzaei_SPIn-NeRF_Multiview_Segmentation_and_Perceptual_Inpainting_With_Neural_Radiance_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mirzaei_SPIn-NeRF_Multiview_Segmentation_and_Perceptual_Inpainting_With_Neural_Radiance_Fields_CVPR_2023_paper.pdf">SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural Radiance<br />Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-01-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_OmniObject3D_Large-Vocabulary_3D_Object_Dataset_for_Realistic_Perception_Reconstruction_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_OmniObject3D_Large-Vocabulary_3D_Object_Dataset_for_Realistic_Perception_Reconstruction_and_CVPR_2023_paper.pdf">OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction<br />and Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.pdf">Image as a Foreign Language: BEIT Pretraining for Vision<br />and Vision-Language Tasks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2022-02-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fel_Dont_Lie_to_Me_Robust_and_Efficient_Explainability_With_Verified_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fel_Dont_Lie_to_Me_Robust_and_Efficient_Explainability_With_Verified_CVPR_2023_paper.pdf">Don't Lie to Me! Robust and Efficient Explainability with<br />Verified Perturbation Analysis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">16</td>
<td style="text-align: center;">2023-01-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Abdal_3DAvatarGAN_Bridging_Domains_for_Personalized_Editable_Avatars_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Abdal_3DAvatarGAN_Bridging_Domains_for_Personalized_Editable_Avatars_CVPR_2023_paper.pdf">3DAvatarGAN: Bridging Domains for Personalized Editable Avatars</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_PiMAE_Point_Cloud_and_Image_Interactive_Masked_Autoencoders_for_3D_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_PiMAE_Point_Cloud_and_Image_Interactive_Masked_Autoencoders_for_3D_CVPR_2023_paper.pdf">PiMAE: Point Cloud and Image Interactive Masked Autoencoders for<br />3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_MOTRv2_Bootstrapping_End-to-End_Multi-Object_Tracking_by_Pretrained_Object_Detectors_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MOTRv2_Bootstrapping_End-to-End_Multi-Object_Tracking_by_Pretrained_Object_Detectors_CVPR_2023_paper.pdf">MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cheng_VindLU_A_Recipe_for_Effective_Video-and-Language_Pretraining_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_VindLU_A_Recipe_for_Effective_Video-and-Language_Pretraining_CVPR_2023_paper.pdf">VindLU: A Recipe for Effective Video-and-Language Pretraining</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Inversion-Based_Style_Transfer_With_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Inversion-Based_Style_Transfer_With_Diffusion_Models_CVPR_2023_paper.pdf">Inversion-based Style Transfer with Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Robust_Dynamic_Radiance_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Robust_Dynamic_Radiance_Fields_CVPR_2023_paper.pdf">Robust Dynamic Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chou_How_to_Backdoor_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chou_How_to_Backdoor_Diffusion_Models_CVPR_2023_paper.pdf">How to Backdoor Diffusion Models?</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yi_MIME_Human-Aware_3D_Scene_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_MIME_Human-Aware_3D_Scene_Generation_CVPR_2023_paper.pdf">MIME: Human-Aware 3D Scene Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-03-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jeong_WinCLIP_Zero-Few-Shot_Anomaly_Classification_and_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jeong_WinCLIP_Zero-Few-Shot_Anomaly_Classification_and_Segmentation_CVPR_2023_paper.pdf">WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shao_Prompting_Large_Language_Models_With_Answer_Heuristics_for_Knowledge-Based_Visual_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shao_Prompting_Large_Language_Models_With_Answer_Heuristics_for_Knowledge-Based_Visual_CVPR_2023_paper.pdf">Prompting Large Language Models with Answer Heuristics for Knowledge-Based<br />Visual Question Answering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-09-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bao_All_Are_Worth_Words_A_ViT_Backbone_for_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_All_Are_Worth_Words_A_ViT_Backbone_for_Diffusion_Models_CVPR_2023_paper.pdf">All are Worth Words: A ViT Backbone for Diffusion<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Saragadam_WIRE_Wavelet_Implicit_Neural_Representations_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Saragadam_WIRE_Wavelet_Implicit_Neural_Representations_CVPR_2023_paper.pdf">WIRE: Wavelet Implicit Neural Representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_SadTalker_Learning_Realistic_3D_Motion_Coefficients_for_Stylized_Audio-Driven_Single_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SadTalker_Learning_Realistic_3D_Motion_Coefficients_for_Stylized_Audio-Driven_Single_CVPR_2023_paper.pdf">SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven<br />Single Image Talking Face Animation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf">HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-07-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Clover_Towards_a_Unified_Video-Language_Alignment_and_Fusion_Model_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Clover_Towards_a_Unified_Video-Language_Alignment_and_Fusion_Model_CVPR_2023_paper.pdf">Clover: Towards A Unified Video-Language Alignment and Fusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2022-11-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_DynIBaR_Neural_Dynamic_Image-Based_Rendering_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DynIBaR_Neural_Dynamic_Image-Based_Rendering_CVPR_2023_paper.pdf">DynIBaR: Neural Dynamic Image-Based Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">2023-01-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wei_LEGO-Net_Learning_Regular_Rearrangements_of_Objects_in_Rooms_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_LEGO-Net_Learning_Regular_Rearrangements_of_Objects_in_Rooms_CVPR_2023_paper.pdf">LEGO-Net: Learning Regular Rearrangements of Objects in Rooms</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-03-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Efficient_and_Explicit_Modelling_of_Image_Hierarchies_for_Image_Restoration_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Efficient_and_Explicit_Modelling_of_Image_Hierarchies_for_Image_Restoration_CVPR_2023_paper.pdf">Efficient and Explicit Modelling of Image Hierarchies for Image<br />Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-02-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ye_Decoupling_Human_and_Camera_Motion_From_Videos_in_the_Wild_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Decoupling_Human_and_Camera_Motion_From_Videos_in_the_Wild_CVPR_2023_paper.pdf">Decoupling Human and Camera Motion from Videos in the<br />Wild</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2023-02-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Saito_Pic2Word_Mapping_Pictures_to_Words_for_Zero-Shot_Composed_Image_Retrieval_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Pic2Word_Mapping_Pictures_to_Words_for_Zero-Shot_Composed_Image_Retrieval_CVPR_2023_paper.pdf">Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image<br />Retrieval</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2022-01-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sha_Cant_Steal_Cont-Steal_Contrastive_Stealing_Attacks_Against_Image_Encoders_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sha_Cant_Steal_Cont-Steal_Contrastive_Stealing_Attacks_Against_Image_Encoders_CVPR_2023_paper.pdf">Can't Steal? Cont-Steal! Contrastive Stealing Attacks Against Image Encoders</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2022-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_CLIP_Is_Also_an_Efficient_Segmenter_A_Text-Driven_Approach_for_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_CLIP_Is_Also_an_Efficient_Segmenter_A_Text-Driven_Approach_for_CVPR_2023_paper.pdf">CLIP is Also an Efficient Segmenter: A Text-Driven Approach<br />for Weakly Supervised Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">14</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Guo_ShadowDiffusion_When_Degradation_Prior_Meets_Diffusion_Model_for_Shadow_Removal_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_ShadowDiffusion_When_Degradation_Prior_Meets_Diffusion_Model_for_Shadow_Removal_CVPR_2023_paper.pdf">ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow<br />Removal</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_SimpleNet_A_Simple_Network_for_Image_Anomaly_Detection_and_Localization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SimpleNet_A_Simple_Network_for_Image_Anomaly_Detection_and_Localization_CVPR_2023_paper.pdf">SimpleNet: A Simple Network for Image Anomaly Detection and<br />Localization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.pdf">UniSim: A Neural Closed-Loop Sensor Simulator</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-01-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Learning_Open-Vocabulary_Semantic_Segmentation_Models_From_Natural_Language_Supervision_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Learning_Open-Vocabulary_Semantic_Segmentation_Models_From_Natural_Language_Supervision_CVPR_2023_paper.pdf">Learning Open-Vocabulary Semantic Segmentation Models From Natural Language Supervision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Explicit_Visual_Prompting_for_Low-Level_Structure_Segmentations_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Explicit_Visual_Prompting_for_Low-Level_Structure_Segmentations_CVPR_2023_paper.pdf">Explicit Visual Prompting for Low-Level Structure Segmentations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2022-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Long_NeuralUDF_Learning_Unsigned_Distance_Fields_for_Multi-View_Reconstruction_of_Surfaces_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Long_NeuralUDF_Learning_Unsigned_Distance_Fields_for_Multi-View_Reconstruction_of_Surfaces_CVPR_2023_paper.pdf">NeuralUDF: Learning Unsigned Distance Fields for Multi-View Reconstruction of<br />Surfaces with Arbitrary Topologies</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ni_Conditional_Image-to-Video_Generation_With_Latent_Flow_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_Conditional_Image-to-Video_Generation_With_Latent_Flow_Diffusion_Models_CVPR_2023_paper.pdf">Conditional Image-to-Video Generation with Latent Flow Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-05-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf">Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2022-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_HumanGen_Generating_Human_Radiance_Fields_With_Explicit_Priors_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_HumanGen_Generating_Human_Radiance_Fields_With_Explicit_Priors_CVPR_2023_paper.pdf">HumanGen: Generating Human Radiance Fields with Explicit Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xing_SVFormer_Semi-Supervised_Video_Transformer_for_Action_Recognition_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xing_SVFormer_Semi-Supervised_Video_Transformer_for_Action_Recognition_CVPR_2023_paper.pdf">SVFormer: Semi-supervised Video Transformer for Action Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zielonka_Instant_Volumetric_Head_Avatars_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zielonka_Instant_Volumetric_Head_Avatars_CVPR_2023_paper.pdf">Instant Volumetric Head Avatars</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-01-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ramanathan_PACO_Parts_and_Attributes_of_Common_Objects_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ramanathan_PACO_Parts_and_Attributes_of_Common_Objects_CVPR_2023_paper.pdf">PACO: Parts and Attributes of Common Objects</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2022-06-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kong_LaserMix_for_Semi-Supervised_LiDAR_Semantic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_LaserMix_for_Semi-Supervised_LiDAR_Semantic_Segmentation_CVPR_2023_paper.pdf">LaserMix for Semi-Supervised LiDAR Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jin_Video-Text_As_Game_Players_Hierarchical_Banzhaf_Interaction_for_Cross-Modal_Representation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Video-Text_As_Game_Players_Hierarchical_Banzhaf_Interaction_for_Cross-Modal_Representation_CVPR_2023_paper.pdf">Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal<br />Representation Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Leclerc_FFCV_Accelerating_Training_by_Removing_Data_Bottlenecks_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Leclerc_FFCV_Accelerating_Training_by_Removing_Data_Bottlenecks_CVPR_2023_paper.pdf">FFCV: Accelerating Training by Removing Data Bottlenecks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2023-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Song_EcoTTA_Memory-Efficient_Continual_Test-Time_Adaptation_via_Self-Distilled_Regularization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_EcoTTA_Memory-Efficient_Continual_Test-Time_Adaptation_via_Self-Distilled_Regularization_CVPR_2023_paper.pdf">EcoTTA: Memory-Efficient Continual Test-Time Adaptation via Self-Distilled Regularization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2022-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shang_Post-Training_Quantization_on_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Post-Training_Quantization_on_Diffusion_Models_CVPR_2023_paper.pdf">Post-Training Quantization on Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2022-06-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tan_Temporal_Attention_Unit_Towards_Efficient_Spatiotemporal_Predictive_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Temporal_Attention_Unit_Towards_Efficient_Spatiotemporal_Predictive_Learning_CVPR_2023_paper.pdf">Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2022-11-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_T-SEA_Transfer-Based_Self-Ensemble_Attack_on_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_T-SEA_Transfer-Based_Self-Ensemble_Attack_on_Object_Detection_CVPR_2023_paper.pdf">T-SEA: Transfer-Based Self-Ensemble Attack on Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../CVPR_2022/" class="btn btn-neutral float-left" title="CVPR 2022"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../../EMNLP/EMNLP_2022/" class="btn btn-neutral float-right" title="EMNLP 2022">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../CVPR_2022/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../../EMNLP/EMNLP_2022/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
