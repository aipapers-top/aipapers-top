<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>CVPR 2023 - AI Papers</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../mytheme.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "CVPR 2023";
        var mkdocs_page_input_path = "CVPR/CVPR_2023.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> AI Papers
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">AI Papers</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">ACL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ACL/ACL_2023/">ACL 2023</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ACL/ACL_2024/">ACL 2024</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">COLM</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../COLM/COLM_2024/">COLM 2024</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">CVPR</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">CVPR 2023</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../CVPR_2024/">CVPR 2024</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">EMNLP</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../EMNLP/EMNLP_2023/">EMNLP 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICCV</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICCV/ICCV_2023/">ICCV 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICLR</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2024/">ICLR 2024</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2025/">ICLR 2025</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICML</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2023/">ICML 2023</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2024/">ICML 2024</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">NeurIPS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../NeurIPS/NeurIPS_2023/">NeurIPS 2023</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../NeurIPS/NeurIPS_2024/">NeurIPS 2024</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">AI Papers</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">CVPR</li>
      <li class="breadcrumb-item active">CVPR 2023</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <p>Last updated: 2024-12-09 08:45:54. Maintained by <a href="https://wayson-ust.github.io/">Weisen Jiang</a>.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">citation</th>
<th style="text-align: center;">date</th>
<th style="text-align: center;">review</th>
<th style="text-align: left;">title (pdf)</th>
<th style="text-align: left;">authors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">4650</td>
<td style="text-align: center;">2022-07-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_YOLOv7_Trainable_Bag-of-Freebies_Sets_New_State-of-the-Art_for_Real-Time_Object_Detectors_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_YOLOv7_Trainable_Bag-of-Freebies_Sets_New_State-of-the-Art_for_Real-Time_Object_Detectors_CVPR_2023_paper.pdf">YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object<br />Detectors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">2084</td>
<td style="text-align: center;">2022-08-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ruiz_DreamBooth_Fine_Tuning_Text-to-Image_Diffusion_Models_for_Subject-Driven_Generation_CVPR_2023_paper.pdf">DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">1247</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.pdf">InstructPix2Pix: Learning to Follow Image Editing Instructions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">916</td>
<td style="text-align: center;">2022-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Magic3D_High-Resolution_Text-to-3D_Content_Creation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Magic3D_High-Resolution_Text-to-3D_Content_Creation_CVPR_2023_paper.pdf">Magic3D: High-Resolution Text-to-3D Content Creation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">862</td>
<td style="text-align: center;">2022-10-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kawar_Imagic_Text-Based_Real_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf">Imagic: Text-Based Real Image Editing with Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">738</td>
<td style="text-align: center;">2023-04-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Blattmann_Align_Your_Latents_High-Resolution_Video_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Blattmann_Align_Your_Latents_High-Resolution_Video_Synthesis_With_Latent_Diffusion_Models_CVPR_2023_paper.pdf">Align Your Latents: High-Resolution Video Synthesis with Latent Diffusion<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">631</td>
<td style="text-align: center;">2023-05-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_ImageBind_One_Embedding_Space_To_Bind_Them_All_CVPR_2023_paper.pdf">ImageBind One Embedding Space to Bind Them All</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">627</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kumari_Multi-Concept_Customization_of_Text-to-Image_Diffusion_CVPR_2023_paper.pdf">Multi-Concept Customization of Text-to-Image Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">614</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Deitke_Objaverse_A_Universe_of_Annotated_3D_Objects_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Deitke_Objaverse_A_Universe_of_Annotated_3D_Objects_CVPR_2023_paper.pdf">Objaverse: A Universe of Annotated 3D Objects</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">539</td>
<td style="text-align: center;">2022-11-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.pdf">EVA: Exploring the Limits of Masked Visual Representation Learning<br />at Scale</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">533</td>
<td style="text-align: center;">2022-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper.pdf">Reproducible Scaling Laws for Contrastive Language-Image Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">489</td>
<td style="text-align: center;">2022-11-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_InternImage_Exploring_Large-Scale_Vision_Foundation_Models_With_Deformable_Convolutions_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_InternImage_Exploring_Large-Scale_Vision_Foundation_Models_With_Deformable_Convolutions_CVPR_2023_paper.pdf">InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">489</td>
<td style="text-align: center;">2023-01-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.pdf">ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">467</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tumanyan_Plug-and-Play_Diffusion_Features_for_Text-Driven_Image-to-Image_Translation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tumanyan_Plug-and-Play_Diffusion_Features_for_Text-Driven_Image-to-Image_Translation_CVPR_2023_paper.pdf">Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">445</td>
<td style="text-align: center;">2023-01-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.pdf">GLIGEN: Open-Set Grounded Text-to-Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">442</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Score_Jacobian_Chaining_Lifting_Pretrained_2D_Diffusion_Models_for_3D_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Score_Jacobian_Chaining_Lifting_Pretrained_2D_Diffusion_Models_for_3D_CVPR_2023_paper.pdf">Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for<br />3D Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">438</td>
<td style="text-align: center;">2023-03-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Run_Dont_Walk_Chasing_Higher_FLOPS_for_Faster_Neural_Networks_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Run_Dont_Walk_Chasing_Higher_FLOPS_for_Faster_Neural_Networks_CVPR_2023_paper.pdf">Run, Don't Walk: Chasing Higher FLOPS for Faster Neural<br />Networks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">433</td>
<td style="text-align: center;">2022-05-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Activating_More_Pixels_in_Image_Super-Resolution_Transformer_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Activating_More_Pixels_in_Image_Super-Resolution_Transformer_CVPR_2023_paper.pdf">Activating More Pixels in Image Super-Resolution Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">406</td>
<td style="text-align: center;">2022-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.pdf">Planning-oriented Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">388</td>
<td style="text-align: center;">2023-01-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fridovich-Keil_K-Planes_Explicit_Radiance_Fields_in_Space_Time_and_Appearance_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fridovich-Keil_K-Planes_Explicit_Radiance_Fields_in_Space_Time_and_Appearance_CVPR_2023_paper.pdf">K-Planes: Explicit Radiance Fields in Space, Time, and Appearance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">376</td>
<td style="text-align: center;">2022-10-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_On_Distillation_of_Guided_Diffusion_Models_CVPR_2023_paper.pdf">On Distillation of Guided Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">375</td>
<td style="text-align: center;">2022-10-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Khattak_MaPLe_Multi-Modal_Prompt_Learning_CVPR_2023_paper.pdf">MaPLe: Multi-modal Prompt Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">373</td>
<td style="text-align: center;">2022-11-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Metzer_Latent-NeRF_for_Shape-Guided_Generation_of_3D_Shapes_and_Textures_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Metzer_Latent-NeRF_for_Shape-Guided_Generation_of_3D_Shapes_and_Textures_CVPR_2023_paper.pdf">Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">366</td>
<td style="text-align: center;">2022-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cao_Observation-Centric_SORT_Rethinking_SORT_for_Robust_Multi-Object_Tracking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Observation-Centric_SORT_Rethinking_SORT_for_Robust_Multi-Object_Tracking_CVPR_2023_paper.pdf">Observation-Centric SORT: Rethinking SORT for Robust Multi-Object Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">354</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Scaling_Up_GANs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf">Scaling up GANs for Text-to-Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">336</td>
<td style="text-align: center;">2022-10-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liang_Open-Vocabulary_Semantic_Segmentation_With_Mask-Adapted_CLIP_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_Open-Vocabulary_Semantic_Segmentation_With_Mask-Adapted_CLIP_CVPR_2023_paper.pdf">Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">320</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_BiFormer_Vision_Transformer_With_Bi-Level_Routing_Attention_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_BiFormer_Vision_Transformer_With_Bi-Level_Routing_Attention_CVPR_2023_paper.pdf">BiFormer: Vision Transformer with Bi-Level Routing Attention</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">312</td>
<td style="text-align: center;">2022-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf">Visual Programming: Compositional visual reasoning without training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">311</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Paint_by_Example_Exemplar-Based_Image_Editing_With_Diffusion_Models_CVPR_2023_paper.pdf">Paint by Example: Exemplar-based Image Editing with Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">311</td>
<td style="text-align: center;">2023-01-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cao_HexPlane_A_Fast_Representation_for_Dynamic_Scenes_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_HexPlane_A_Fast_Representation_for_Dynamic_Scenes_CVPR_2023_paper.pdf">HexPlane: A Fast Representation for Dynamic Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">300</td>
<td style="text-align: center;">2022-06-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Mask_DINO_Towards_a_Unified_Transformer-Based_Framework_for_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Mask_DINO_Towards_a_Unified_Transformer-Based_Framework_for_Object_Detection_CVPR_2023_paper.pdf">Mask DINO: Towards A Unified Transformer-based Framework for Object<br />Detection and Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">280</td>
<td style="text-align: center;">2023-03-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Open-Vocabulary_Panoptic_Segmentation_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Open-Vocabulary_Panoptic_Segmentation_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf">Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">272</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Neuralangelo_High-Fidelity_Neural_Surface_Reconstruction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Neuralangelo_High-Fidelity_Neural_Surface_Reconstruction_CVPR_2023_paper.pdf">Neuralangelo: High-Fidelity Neural Surface Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">266</td>
<td style="text-align: center;">2022-07-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_MobileNeRF_Exploiting_the_Polygon_Rasterization_Pipeline_for_Efficient_Neural_Field_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_MobileNeRF_Exploiting_the_Polygon_Rasterization_Pipeline_for_Efficient_Neural_Field_CVPR_2023_paper.pdf">MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural<br />Field Rendering on Mobile Architectures</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">261</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.pdf">Scaling Language-Image Pre-Training via Masking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">251</td>
<td style="text-align: center;">2022-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Peng_OpenScene_3D_Scene_Understanding_With_Open_Vocabularies_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_OpenScene_3D_Scene_Understanding_With_Open_Vocabularies_CVPR_2023_paper.pdf">OpenScene: 3D Scene Understanding with Open Vocabularies</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">250</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_VideoMAE_V2_Scaling_Video_Masked_Autoencoders_With_Dual_Masking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_VideoMAE_V2_Scaling_Video_Masked_Autoencoders_With_Dual_Masking_CVPR_2023_paper.pdf">VideoMAE V2: Scaling Video Masked Autoencoders with Dual Masking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">243</td>
<td style="text-align: center;">2022-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Somepalli_Diffusion_Art_or_Digital_Forgery_Investigating_Data_Replication_in_Diffusion_CVPR_2023_paper.pdf">Diffusion Art or Digital Forgery? Investigating Data Replication in<br />Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">243</td>
<td style="text-align: center;">2022-11-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jain_OneFormer_One_Transformer_To_Rule_Universal_Image_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_OneFormer_One_Transformer_To_Rule_Universal_Image_Segmentation_CVPR_2023_paper.pdf">OneFormer: One Transformer to Rule Universal Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">236</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Executing_Your_Commands_via_Motion_Diffusion_in_Latent_Space_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Executing_Your_Commands_via_Motion_Diffusion_in_Latent_Space_CVPR_2023_paper.pdf">Executing your Commands via Motion Diffusion in Latent Space</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">231</td>
<td style="text-align: center;">2022-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_RODIN_A_Generative_Model_for_Sculpting_3D_Digital_Avatars_Using_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_RODIN_A_Generative_Model_for_Sculpting_3D_Digital_Avatars_Using_CVPR_2023_paper.pdf">RODIN: A Generative Model for Sculpting 3D Digital Avatars<br />Using Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">222</td>
<td style="text-align: center;">2023-01-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Generating_Human_Motion_From_Textual_Descriptions_With_Discrete_Representations_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Generating_Human_Motion_From_Textual_Descriptions_With_Discrete_Representations_CVPR_2023_paper.pdf">Generating Human Motion from Textual Descriptions with Discrete Representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">221</td>
<td style="text-align: center;">2023-01-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Assran_Self-Supervised_Learning_From_Images_With_a_Joint-Embedding_Predictive_Architecture_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Assran_Self-Supervised_Learning_From_Images_With_a_Joint-Embedding_Predictive_Architecture_CVPR_2023_paper.pdf">Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">215</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.pdf">Image as a Foreign Language: BEIT Pretraining for Vision<br />and Vision-Language Tasks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">206</td>
<td style="text-align: center;">2022-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.pdf">BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition<br />via Perspective Supervision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">201</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_FreeNeRF_Improving_Few-Shot_Neural_Rendering_With_Free_Frequency_Regularization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_FreeNeRF_Improving_Few-Shot_Neural_Rendering_With_Free_Frequency_Regularization_CVPR_2023_paper.pdf">FreeNeRF: Improving Few-Shot Neural Rendering with Free Frequency Regularization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">199</td>
<td style="text-align: center;">2023-02-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Tri-Perspective_View_for_Vision-Based_3D_Semantic_Occupancy_Prediction_CVPR_2023_paper.pdf">Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">199</td>
<td style="text-align: center;">2022-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shue_3D_Neural_Field_Generation_Using_Triplane_Diffusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shue_3D_Neural_Field_Generation_Using_Triplane_Diffusion_CVPR_2023_paper.pdf">3D Neural Field Generation Using Triplane Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">198</td>
<td style="text-align: center;">2022-09-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bao_All_Are_Worth_Words_A_ViT_Backbone_for_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_All_Are_Worth_Words_A_ViT_Backbone_for_Diffusion_Models_CVPR_2023_paper.pdf">All are Worth Words: A ViT Backbone for Diffusion<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">198</td>
<td style="text-align: center;">2022-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVPR_2023_paper.pdf">Images Speak in Images: A Generalist Painter for In-Context<br />Visual Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">196</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cheng_SDFusion_Multimodal_3D_Shape_Completion_Reconstruction_and_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_SDFusion_Multimodal_3D_Shape_Completion_Reconstruction_and_Generation_CVPR_2023_paper.pdf">SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">194</td>
<td style="text-align: center;">2022-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.pdf">Generalized Decoding for Pixel, Image, and Language</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">193</td>
<td style="text-align: center;">2022-11-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Schramowski_Safe_Latent_Diffusion_Mitigating_Inappropriate_Degeneration_in_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Schramowski_Safe_Latent_Diffusion_Mitigating_Inappropriate_Degeneration_in_Diffusion_Models_CVPR_2023_paper.pdf">Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">2022-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.pdf">CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">190</td>
<td style="text-align: center;">2022-04-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hassani_Neighborhood_Attention_Transformer_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hassani_Neighborhood_Attention_Transformer_CVPR_2023_paper.pdf">Neighborhood Attention Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">188</td>
<td style="text-align: center;">2023-03-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Takagi_High-Resolution_Image_Reconstruction_With_Latent_Diffusion_Models_From_Human_Brain_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Takagi_High-Resolution_Image_Reconstruction_With_Latent_Diffusion_Models_From_Human_Brain_CVPR_2023_paper.pdf">High-resolution image reconstruction with latent diffusion models from human<br />brain activity</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">183</td>
<td style="text-align: center;">2023-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Side_Adapter_Network_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Side_Adapter_Network_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2023_paper.pdf">Side Adapter Network for Open-Vocabulary Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">181</td>
<td style="text-align: center;">2022-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bian_NoPe-NeRF_Optimising_Neural_Radiance_Field_With_No_Pose_Prior_CVPR_2023_paper.pdf">NoPe-NeRF: Optimising Neural Radiance Field with No Pose Prior</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">179</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Smith_CODA-Prompt_COntinual_Decomposed_Attention-Based_Prompting_for_Rehearsal-Free_Continual_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Smith_CODA-Prompt_COntinual_Decomposed_Attention-Based_Prompting_for_Rehearsal-Free_Continual_Learning_CVPR_2023_paper.pdf">CODA-Prompt: COntinual Decomposed Attention-Based Prompting for Rehearsal-Free Continual Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">178</td>
<td style="text-align: center;">2023-01-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.pdf">DepGraph: Towards Any Structural Pruning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">176</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Inversion-Based_Style_Transfer_With_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Inversion-Based_Style_Transfer_With_Diffusion_Models_CVPR_2023_paper.pdf">Inversion-based Style Transfer with Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">175</td>
<td style="text-align: center;">2022-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.pdf">All in One: Exploring Unified Video-Language Pre-Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">173</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_SparseFusion_Distilling_View-Conditioned_Diffusion_for_3D_Reconstruction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_SparseFusion_Distilling_View-Conditioned_Diffusion_for_3D_Reconstruction_CVPR_2023_paper.pdf">SparseFusion: Distilling View-Conditioned Diffusion for 3D Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">169</td>
<td style="text-align: center;">2022-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hoyer_MIC_Masked_Image_Consistency_for_Context-Enhanced_Domain_Adaptation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hoyer_MIC_Masked_Image_Consistency_for_Context-Enhanced_Domain_Adaptation_CVPR_2023_paper.pdf">MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">167</td>
<td style="text-align: center;">2022-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Avrahami_SpaText_Spatio-Textual_Representation_for_Controllable_Image_Generation_CVPR_2023_paper.pdf">SpaText: Spatio-Textual Representation for Controllable Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">167</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_SmartBrush_Text_and_Shape_Guided_Object_Inpainting_With_Diffusion_Model_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_SmartBrush_Text_and_Shape_Guided_Object_Inpainting_With_Diffusion_Model_CVPR_2023_paper.pdf">SmartBrush: Text and Shape Guided Object Inpainting with Diffusion<br />Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">166</td>
<td style="text-align: center;">2023-05-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_EfficientViT_Memory_Efficient_Vision_Transformer_With_Cascaded_Group_Attention_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_EfficientViT_Memory_Efficient_Vision_Transformer_With_Cascaded_Group_Attention_CVPR_2023_paper.pdf">EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">165</td>
<td style="text-align: center;">2022-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tseng_EDGE_Editable_Dance_Generation_From_Music_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tseng_EDGE_Editable_Dance_Generation_From_Music_CVPR_2023_paper.pdf">EDGE: Editable Dance Generation From Music</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">160</td>
<td style="text-align: center;">2022-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Muller_DiffRF_Rendering-Guided_3D_Radiance_Field_Diffusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Muller_DiffRF_Rendering-Guided_3D_Radiance_Field_Diffusion_CVPR_2023_paper.pdf">DiffRF: Rendering-Guided 3D Radiance Field Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">159</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_VoxelNeXt_Fully_Sparse_VoxelNet_for_3D_Object_Detection_and_Tracking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_VoxelNeXt_Fully_Sparse_VoxelNet_for_3D_Object_Detection_and_Tracking_CVPR_2023_paper.pdf">VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and<br />Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">157</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gao_Implicit_Diffusion_Models_for_Continuous_Super-Resolution_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Implicit_Diffusion_Models_for_Continuous_Super-Resolution_CVPR_2023_paper.pdf">Implicit Diffusion Models for Continuous Super-Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">155</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_SadTalker_Learning_Realistic_3D_Motion_Coefficients_for_Stylized_Audio-Driven_Single_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SadTalker_Learning_Realistic_3D_Motion_Coefficients_for_Stylized_Audio-Driven_Single_CVPR_2023_paper.pdf">SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven<br />Single Image Talking Face Animation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">154</td>
<td style="text-align: center;">2023-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Vid2Seq_Large-Scale_Pretraining_of_a_Visual_Language_Model_for_Dense_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Vid2Seq_Large-Scale_Pretraining_of_a_Visual_Language_Model_for_Dense_CVPR_2023_paper.pdf">Vid2Seq: Large-Scale Pretraining of a Visual Language Model for<br />Dense Video Captioning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">154</td>
<td style="text-align: center;">2023-01-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Diffusion-Based_Generation_Optimization_and_Planning_in_3D_Scenes_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Diffusion-Based_Generation_Optimization_and_Planning_in_3D_Scenes_CVPR_2023_paper.pdf">Diffusion-based Generation, Optimization, and Planning in 3D Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">152</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Learning_a_Sparse_Transformer_Network_for_Effective_Image_Deraining_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Learning_a_Sparse_Transformer_Network_for_Effective_Image_Deraining_CVPR_2023_paper.pdf">Learning A Sparse Transformer Network for Effective Image Deraining</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">152</td>
<td style="text-align: center;">2022-07-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jia_DETRs_With_Hybrid_Matching_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jia_DETRs_With_Hybrid_Matching_CVPR_2023_paper.pdf">DETRs with Hybrid Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">151</td>
<td style="text-align: center;">2023-01-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_OmniObject3D_Large-Vocabulary_3D_Object_Dataset_for_Realistic_Perception_Reconstruction_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_OmniObject3D_Large-Vocabulary_3D_Object_Dataset_for_Realistic_Perception_Reconstruction_and_CVPR_2023_paper.pdf">OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction<br />and Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">151</td>
<td style="text-align: center;">2022-08-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Revisiting_Weak-to-Strong_Consistency_in_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Revisiting_Weak-to-Strong_Consistency_in_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">150</td>
<td style="text-align: center;">2022-12-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Dream3D_Zero-Shot_Text-to-3D_Synthesis_Using_3D_Shape_Prior_and_Text-to-Image_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Dream3D_Zero-Shot_Text-to-3D_Synthesis_Using_3D_Shape_Prior_and_Text-to-Image_CVPR_2023_paper.pdf">Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and<br />Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">150</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Language_in_a_Bottle_Language_Model_Guided_Concept_Bottlenecks_for_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Language_in_a_Bottle_Language_Model_Guided_Concept_Bottlenecks_for_CVPR_2023_paper.pdf">Language in a Bottle: Language Model Guided Concept Bottlenecks<br />for Interpretable Image Classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">148</td>
<td style="text-align: center;">2022-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MAGVIT_Masked_Generative_Video_Transformer_CVPR_2023_paper.pdf">MAGVIT: Masked Generative Video Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">147</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Deng_NeRDi_Single-View_NeRF_Synthesis_With_Language-Guided_Diffusion_As_General_Image_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Deng_NeRDi_Single-View_NeRF_Synthesis_With_Language-Guided_Diffusion_As_General_Image_CVPR_2023_paper.pdf">NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General<br />Image Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">142</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Learned_Image_Compression_With_Mixed_Transformer-CNN_Architectures_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Learned_Image_Compression_With_Mixed_Transformer-CNN_Architectures_CVPR_2023_paper.pdf">Learned Image Compression with Mixed Transformer-CNN Architectures</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">140</td>
<td style="text-align: center;">2023-02-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.pdf">Video Probabilistic Diffusion Models in Projected Latent Space</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">139</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_SimpleNet_A_Simple_Network_for_Image_Anomaly_Detection_and_Localization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_SimpleNet_A_Simple_Network_for_Image_Anomaly_Detection_and_Localization_CVPR_2023_paper.pdf">SimpleNet: A Simple Network for Image Anomaly Detection and<br />Localization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">138</td>
<td style="text-align: center;">2022-11-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_DynIBaR_Neural_Dynamic_Image-Based_Rendering_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DynIBaR_Neural_Dynamic_Image-Based_Rendering_CVPR_2023_paper.pdf">DynIBaR: Neural Dynamic Image-Based Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">138</td>
<td style="text-align: center;">2022-06-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_PIDNet_A_Real-Time_Semantic_Segmentation_Network_Inspired_by_PID_Controllers_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_PIDNet_A_Real-Time_Semantic_Segmentation_Network_Inspired_by_PID_Controllers_CVPR_2023_paper.pdf">PIDNet: A Real-time Semantic Segmentation Network Inspired by PID<br />Controllers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">135</td>
<td style="text-align: center;">2023-04-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Co-SLAM_Joint_Coordinate_and_Sparse_Parametric_Encodings_for_Neural_Real-Time_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Co-SLAM_Joint_Coordinate_and_Sparse_Parametric_Encodings_for_Neural_Real-Time_CVPR_2023_paper.pdf">Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for Neural<br />Real-Time SLAM</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">134</td>
<td style="text-align: center;">2022-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_NeuralLift-360_Lifting_an_In-the-Wild_2D_Photo_to_a_3D_Object_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_NeuralLift-360_Lifting_an_In-the-Wild_2D_Photo_to_a_3D_Object_CVPR_2023_paper.pdf">NeuralLift-360: Lifting an in-the-Wild 2D Photo to A 3D<br />Object with 360° Views</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">133</td>
<td style="text-align: center;">2023-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Prompt_Generate_Then_Cache_Cascade_of_Foundation_Models_Makes_Strong_CVPR_2023_paper.pdf">Prompt, Generate, Then Cache: Cascade of Foundation Models Makes<br />Strong Few-Shot Learners</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">133</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shao_Tensor4D_Efficient_Neural_4D_Decomposition_for_High-Fidelity_Dynamic_Reconstruction_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shao_Tensor4D_Efficient_Neural_4D_Decomposition_for_High-Fidelity_Dynamic_Reconstruction_and_CVPR_2023_paper.pdf">Tensor4D: Efficient Neural 4D Decomposition for High-Fidelity Dynamic Reconstruction<br />and Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">132</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.pdf">UniSim: A Neural Closed-Loop Sensor Simulator</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">132</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_SINE_SINgle_Image_Editing_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf">SINE: SINgle Image Editing with Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">132</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wallace_EDICT_Exact_Diffusion_Inversion_via_Coupled_Transformations_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wallace_EDICT_Exact_Diffusion_Inversion_via_Coupled_Transformations_CVPR_2023_paper.pdf">EDICT: Exact Diffusion Inversion via Coupled Transformations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">131</td>
<td style="text-align: center;">2022-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Imagen_Editor_and_EditBench_Advancing_and_Evaluating_Text-Guided_Image_Inpainting_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Imagen_Editor_and_EditBench_Advancing_and_Evaluating_Text-Guided_Image_Inpainting_CVPR_2023_paper.pdf">Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image<br />Inpainting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">129</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dabral_Mofusion_A_Framework_for_Denoising-Diffusion-Based_Motion_Synthesis_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dabral_Mofusion_A_Framework_for_Denoising-Diffusion-Based_Motion_Synthesis_CVPR_2023_paper.pdf">MoFusion: A Framework for Denoising-Diffusion-Based Motion Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">129</td>
<td style="text-align: center;">2023-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yan_Universal_Instance_Perception_As_Object_Discovery_and_Retrieval_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_Universal_Instance_Perception_As_Object_Discovery_and_Retrieval_CVPR_2023_paper.pdf">Universal Instance Perception as Object Discovery and Retrieval</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">129</td>
<td style="text-align: center;">2023-01-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Cut_and_Learn_for_Unsupervised_Object_Detection_and_Instance_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Cut_and_Learn_for_Unsupervised_Object_Detection_and_Instance_Segmentation_CVPR_2023_paper.pdf">Cut and Learn for Unsupervised Object Detection and Instance<br />Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">129</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Anciukevicius_RenderDiffusion_Image_Diffusion_for_3D_Reconstruction_Inpainting_and_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Anciukevicius_RenderDiffusion_Image_Diffusion_for_3D_Reconstruction_Inpainting_and_Generation_CVPR_2023_paper.pdf">RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">2023-02-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ojha_Towards_Universal_Fake_Image_Detectors_That_Generalize_Across_Generative_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ojha_Towards_Universal_Fake_Image_Detectors_That_Generalize_Across_Generative_Models_CVPR_2023_paper.pdf">Towards Universal Fake Image Detectors that Generalize Across Generative<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">127</td>
<td style="text-align: center;">2023-03-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Efficient_and_Explicit_Modelling_of_Image_Hierarchies_for_Image_Restoration_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Efficient_and_Explicit_Modelling_of_Image_Hierarchies_for_Image_Restoration_CVPR_2023_paper.pdf">Efficient and Explicit Modelling of Image Hierarchies for Image<br />Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">127</td>
<td style="text-align: center;">2022-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_ZegCLIP_Towards_Adapting_CLIP_for_Zero-Shot_Semantic_Segmentation_CVPR_2023_paper.pdf">ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">126</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_LayoutDiffusion_Controllable_Diffusion_Model_for_Layout-to-Image_Generation_CVPR_2023_paper.pdf">LayoutDiffusion: Controllable Diffusion Model for Layout-to-Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">124</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fei_Generative_Diffusion_Prior_for_Unified_Image_Restoration_and_Enhancement_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fei_Generative_Diffusion_Prior_for_Unified_Image_Restoration_and_Enhancement_CVPR_2023_paper.pdf">Generative Diffusion Prior for Unified Image Restoration and Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">123</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yao_Visual-Language_Prompt_Tuning_With_Knowledge-Guided_Context_Optimization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Visual-Language_Prompt_Tuning_With_Knowledge-Guided_Context_Optimization_CVPR_2023_paper.pdf">Visual-Language Prompt Tuning with Knowledge-Guided Context Optimization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">122</td>
<td style="text-align: center;">2022-12-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ruan_MM-Diffusion_Learning_Multi-Modal_Diffusion_Models_for_Joint_Audio_and_Video_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ruan_MM-Diffusion_Learning_Multi-Modal_Diffusion_Models_for_Joint_Audio_and_Video_CVPR_2023_paper.pdf">MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and<br />Video Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">121</td>
<td style="text-align: center;">2022-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sariyildiz_Fake_It_Till_You_Make_It_Learning_Transferable_Representations_From_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sariyildiz_Fake_It_Till_You_Make_It_Learning_Transferable_Representations_From_CVPR_2023_paper.pdf">Fake it Till You Make it: Learning Transferable Representations<br />from Synthetic ImageNet Clones</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">120</td>
<td style="text-align: center;">2022-11-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Seeing_Beyond_the_Brain_Conditional_Diffusion_Model_With_Sparse_Masked_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Seeing_Beyond_the_Brain_Conditional_Diffusion_Model_With_Sparse_Masked_CVPR_2023_paper.pdf">Seeing Beyond the Brain: Conditional Diffusion Model with Sparse<br />Masked Modeling for Vision Decoding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">118</td>
<td style="text-align: center;">2022-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ding_PLA_Language-Driven_Open-Vocabulary_3D_Scene_Understanding_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_PLA_Language-Driven_Open-Vocabulary_3D_Scene_Understanding_CVPR_2023_paper.pdf">PLA: Language-Driven Open-Vocabulary 3D Scene Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">118</td>
<td style="text-align: center;">2022-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xiu_ECON_Explicit_Clothed_Humans_Optimized_via_Normal_Integration_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiu_ECON_Explicit_Clothed_Humans_Optimized_via_Normal_Integration_CVPR_2023_paper.pdf">ECON: Explicit Clothed humans Optimized via Normal integration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">116</td>
<td style="text-align: center;">2022-08-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dong_MaskCLIP_Masked_Self-Distillation_Advances_Contrastive_Language-Image_Pretraining_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_MaskCLIP_Masked_Self-Distillation_Advances_Contrastive_Language-Image_Pretraining_CVPR_2023_paper.pdf">MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">116</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_SCConv_Spatial_and_Channel_Reconstruction_Convolution_for_Feature_Redundancy_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SCConv_Spatial_and_Channel_Reconstruction_Convolution_for_Feature_Redundancy_CVPR_2023_paper.pdf">SCConv: Spatial and Channel Reconstruction Convolution for Feature Redundancy</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">115</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Blind_Image_Quality_Assessment_via_Vision-Language_Correspondence_A_Multitask_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Blind_Image_Quality_Assessment_via_Vision-Language_Correspondence_A_Multitask_Learning_CVPR_2023_paper.pdf">Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask<br />Learning Perspective</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">115</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_Query-Centric_Trajectory_Prediction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Query-Centric_Trajectory_Prediction_CVPR_2023_paper.pdf">Query-Centric Trajectory Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">115</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Rasheed_Fine-Tuned_CLIP_Models_Are_Efficient_Video_Learners_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rasheed_Fine-Tuned_CLIP_Models_Are_Efficient_Video_Learners_CVPR_2023_paper.pdf">Fine-tuned CLIP Models are Efficient Video Learners</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">114</td>
<td style="text-align: center;">2022-05-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_Revealing_the_Dark_Secrets_of_Masked_Image_Modeling_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_Revealing_the_Dark_Secrets_of_Masked_Image_Modeling_CVPR_2023_paper.pdf">Revealing the Dark Secrets of Masked Image Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">114</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bahl_Affordances_From_Human_Videos_as_a_Versatile_Representation_for_Robotics_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bahl_Affordances_From_Human_Videos_as_a_Versatile_Representation_for_Robotics_CVPR_2023_paper.pdf">Affordances from Human Videos as a Versatile Representation for<br />Robotics</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">113</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Johari_ESLAM_Efficient_Dense_SLAM_System_Based_on_Hybrid_Representation_of_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Johari_ESLAM_Efficient_Dense_SLAM_System_Based_on_Hybrid_Representation_of_CVPR_2023_paper.pdf">ESLAM: Efficient Dense SLAM System Based on Hybrid Representation<br />of Signed Distance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">113</td>
<td style="text-align: center;">2023-03-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jeong_WinCLIP_Zero-Few-Shot_Anomaly_Classification_and_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jeong_WinCLIP_Zero-Few-Shot_Anomaly_Classification_and_Segmentation_CVPR_2023_paper.pdf">WinCLIP: Zero-/Few-Shot Anomaly Classification and Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">112</td>
<td style="text-align: center;">2023-03-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Iterative_Geometry_Encoding_Volume_for_Stereo_Matching_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Iterative_Geometry_Encoding_Volume_for_Stereo_Matching_CVPR_2023_paper.pdf">Iterative Geometry Encoding Volume for Stereo Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">112</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/He_Camouflaged_Object_Detection_With_Feature_Decomposition_and_Edge_Reconstruction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Camouflaged_Object_Detection_With_Feature_Decomposition_and_Edge_Reconstruction_CVPR_2023_paper.pdf">Camouflaged Object Detection with Feature Decomposition and Edge Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">112</td>
<td style="text-align: center;">2021-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mukhoti_Deep_Deterministic_Uncertainty_A_New_Simple_Baseline_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mukhoti_Deep_Deterministic_Uncertainty_A_New_Simple_Baseline_CVPR_2023_paper.pdf">Deep Deterministic Uncertainty: A New Simple Baseline</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">112</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Lite-Mono_A_Lightweight_CNN_and_Transformer_Architecture_for_Self-Supervised_Monocular_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Lite-Mono_A_Lightweight_CNN_and_Transformer_Architecture_for_Self-Supervised_Monocular_CVPR_2023_paper.pdf">Lite-Mono: A Lightweight CNN and Transformer Architecture for Self-Supervised<br />Monocular Depth Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">112</td>
<td style="text-align: center;">2023-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_MVImgNet_A_Large-Scale_Dataset_of_Multi-View_Images_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MVImgNet_A_Large-Scale_Dataset_of_Multi-View_Images_CVPR_2023_paper.pdf">MVImgNet: A Large-scale Dataset of Multi-view Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">111</td>
<td style="text-align: center;">2022-04-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.pdf">ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">111</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Robust_Dynamic_Radiance_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Robust_Dynamic_Radiance_Fields_CVPR_2023_paper.pdf">Robust Dynamic Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">110</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_ReCo_Region-Controlled_Text-to-Image_Generation_CVPR_2023_paper.pdf">ReCo: Region-Controlled Text-to-Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">109</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_F2-NeRF_Fast_Neural_Radiance_Field_Training_With_Free_Camera_Trajectories_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_F2-NeRF_Fast_Neural_Radiance_Field_Training_With_Free_Camera_Trajectories_CVPR_2023_paper.pdf">F2-NeRF: Fast Neural Radiance Field Training with Free Camera<br />Trajectories</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">109</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Goyal_Finetune_Like_You_Pretrain_Improved_Finetuning_of_Zero-Shot_Vision_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Goyal_Finetune_Like_You_Pretrain_Improved_Finetuning_of_Zero-Shot_Vision_Models_CVPR_2023_paper.pdf">Finetune like you pretrain: Improved finetuning of zero-shot vision<br />models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">108</td>
<td style="text-align: center;">2022-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_PointAvatar_Deformable_Point-Based_Head_Avatars_From_Videos_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_PointAvatar_Deformable_Point-Based_Head_Avatars_From_Videos_CVPR_2023_paper.pdf">PointAvatar: Deformable Point-Based Head Avatars from Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">108</td>
<td style="text-align: center;">2022-06-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Vasu_MobileOne_An_Improved_One_Millisecond_Mobile_Backbone_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Vasu_MobileOne_An_Improved_One_Millisecond_Mobile_Backbone_CVPR_2023_paper.pdf">MobileOne: An Improved One millisecond Mobile Backbone</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">107</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Visual_Prompt_Multi-Modal_Tracking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Visual_Prompt_Multi-Modal_Tracking_CVPR_2023_paper.pdf">Visual Prompt Multi-Modal Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">107</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_V2V4Real_A_Real-World_Large-Scale_Dataset_for_Vehicle-to-Vehicle_Cooperative_Perception_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_V2V4Real_A_Real-World_Large-Scale_Dataset_for_Vehicle-to-Vehicle_Cooperative_Perception_CVPR_2023_paper.pdf">V2V4Real: A Real-World Large-Scale Dataset for Vehicle-to-Vehicle Cooperative Perception</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">106</td>
<td style="text-align: center;">2023-01-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xing_CodeTalker_Speech-Driven_3D_Facial_Animation_With_Discrete_Motion_Prior_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xing_CodeTalker_Speech-Driven_3D_Facial_Animation_With_Discrete_Motion_Prior_CVPR_2023_paper.pdf">CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">106</td>
<td style="text-align: center;">2022-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gadre_CoWs_on_Pasture_Baselines_and_Benchmarks_for_Language-Driven_Zero-Shot_Object_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gadre_CoWs_on_Pasture_Baselines_and_Benchmarks_for_Language-Driven_Zero-Shot_Object_CVPR_2023_paper.pdf">CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot<br />Object Navigation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">106</td>
<td style="text-align: center;">2022-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Learning_3D_Representations_From_2D_Pre-Trained_Models_via_Image-to-Point_Masked_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_3D_Representations_From_2D_Pre-Trained_Models_via_Image-to-Point_Masked_CVPR_2023_paper.pdf">Learning 3D Representations from 2D Pre-Trained Models via Image-to-Point<br />Masked Autoencoders</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">105</td>
<td style="text-align: center;">2022-11-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_MAGE_MAsked_Generative_Encoder_To_Unify_Representation_Learning_and_Image_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MAGE_MAsked_Generative_Encoder_To_Unify_Representation_Learning_and_Image_CVPR_2023_paper.pdf">MAGE: MAsked Generative Encoder to Unify Representation Learning and<br />Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">103</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yi_Generating_Holistic_3D_Human_Motion_From_Speech_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_Generating_Holistic_3D_Human_Motion_From_Speech_CVPR_2023_paper.pdf">Generating Holistic 3D Human Motion from Speech</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">103</td>
<td style="text-align: center;">2023-01-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_CLIP2Scene_Towards_Label-Efficient_3D_Scene_Understanding_by_CLIP_CVPR_2023_paper.pdf">CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">102</td>
<td style="text-align: center;">2022-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shang_Post-Training_Quantization_on_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shang_Post-Training_Quantization_on_Diffusion_Models_CVPR_2023_paper.pdf">Post-Training Quantization on Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">101</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Diffusion-SDF_Text-To-Shape_via_Voxelized_Diffusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Diffusion-SDF_Text-To-Shape_via_Voxelized_Diffusion_CVPR_2023_paper.pdf">Diffusion-SDF: Text-to-Shape via Voxelized Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">2022-12-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Siddiqui_Panoptic_Lifting_for_3D_Scene_Understanding_With_Neural_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Siddiqui_Panoptic_Lifting_for_3D_Scene_Understanding_With_Neural_Fields_CVPR_2023_paper.pdf">Panoptic Lifting for 3D Scene Understanding with Neural Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">99</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_MOTRv2_Bootstrapping_End-to-End_Multi-Object_Tracking_by_Pretrained_Object_Detectors_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MOTRv2_Bootstrapping_End-to-End_Multi-Object_Tracking_by_Pretrained_Object_Detectors_CVPR_2023_paper.pdf">MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">98</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kang_Benchmarking_Self-Supervised_Learning_on_Diverse_Pathology_Datasets_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kang_Benchmarking_Self-Supervised_Learning_on_Diverse_Pathology_Datasets_CVPR_2023_paper.pdf">Benchmarking Self-Supervised Learning on Diverse Pathology Datasets</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">97</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wen_BundleSDF_Neural_6-DoF_Tracking_and_3D_Reconstruction_of_Unknown_Objects_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wen_BundleSDF_Neural_6-DoF_Tracking_and_3D_Reconstruction_of_Unknown_Objects_CVPR_2023_paper.pdf">BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown<br />Objects</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">97</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Next3D_Generative_Neural_Texture_Rasterization_for_3D-Aware_Head_Avatars_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Next3D_Generative_Neural_Texture_Rasterization_for_3D-Aware_Head_Avatars_CVPR_2023_paper.pdf">Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_GRES_Generalized_Referring_Expression_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GRES_Generalized_Referring_Expression_Segmentation_CVPR_2023_paper.pdf">GRES: Generalized Referring Expression Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">95</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Saragadam_WIRE_Wavelet_Implicit_Neural_Representations_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Saragadam_WIRE_Wavelet_Implicit_Neural_Representations_CVPR_2023_paper.pdf">WIRE: Wavelet Implicit Neural Representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">95</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Karnewar_HOLODIFFUSION_Training_a_3D_Diffusion_Model_Using_2D_Images_CVPR_2023_paper.pdf">HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">95</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Explicit_Visual_Prompting_for_Low-Level_Structure_Segmentations_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Explicit_Visual_Prompting_for_Low-Level_Structure_Segmentations_CVPR_2023_paper.pdf">Explicit Visual Prompting for Low-Level Structure Segmentations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">95</td>
<td style="text-align: center;">2023-02-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_PolyFormer_Referring_Image_Segmentation_As_Sequential_Polygon_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PolyFormer_Referring_Image_Segmentation_As_Sequential_Polygon_Generation_CVPR_2023_paper.pdf">PolyFormer: Referring Image Segmentation as Sequential Polygon Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">94</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_Cross-Modal_Implicit_Relation_Reasoning_and_Aligning_for_Text-to-Image_Person_Retrieval_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Cross-Modal_Implicit_Relation_Reasoning_and_Aligning_for_Text-to-Image_Person_Retrieval_CVPR_2023_paper.pdf">Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person<br />Retrieval</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">92</td>
<td style="text-align: center;">2022-11-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fan_OpenGait_Revisiting_Gait_Recognition_Towards_Better_Practicality_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_OpenGait_Revisiting_Gait_Recognition_Towards_Better_Practicality_CVPR_2023_paper.pdf">OpenGait: Revisiting Gait Recognition Toward Better Practicality</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">92</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lai_Spherical_Transformer_for_LiDAR-Based_3D_Recognition_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lai_Spherical_Transformer_for_LiDAR-Based_3D_Recognition_CVPR_2023_paper.pdf">Spherical Transformer for LiDAR-Based 3D Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">91</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tanida_Interactive_and_Explainable_Region-Guided_Radiology_Report_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tanida_Interactive_and_Explainable_Region-Guided_Radiology_Report_Generation_CVPR_2023_paper.pdf">Interactive and Explainable Region-guided Radiology Report Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">91</td>
<td style="text-align: center;">2022-05-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_BBDM_Image-to-Image_Translation_With_Brownian_Bridge_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_BBDM_Image-to-Image_Translation_With_Brownian_Bridge_Diffusion_Models_CVPR_2023_paper.pdf">BBDM: Image-to-Image Translation with Brownian Bridge Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">91</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mirzaei_SPIn-NeRF_Multiview_Segmentation_and_Perceptual_Inpainting_With_Neural_Radiance_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mirzaei_SPIn-NeRF_Multiview_Segmentation_and_Perceptual_Inpainting_With_Neural_Radiance_Fields_CVPR_2023_paper.pdf">SPIn-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural Radiance<br />Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">91</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shi_TriDet_Temporal_Action_Detection_With_Relative_Boundary_Modeling_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_TriDet_Temporal_Action_Detection_With_Relative_Boundary_Modeling_CVPR_2023_paper.pdf">TriDet: Temporal Action Detection with Relative Boundary Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">90</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_MotionDiffuser_Controllable_Multi-Agent_Motion_Prediction_Using_Diffusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_MotionDiffuser_Controllable_Multi-Agent_Motion_Prediction_Using_Diffusion_CVPR_2023_paper.pdf">MotionDiffuser: Controllable Multi-Agent Motion Prediction Using Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">88</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Black_BEDLAM_A_Synthetic_Dataset_of_Bodies_Exhibiting_Detailed_Lifelike_Animated_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Black_BEDLAM_A_Synthetic_Dataset_of_Bodies_Exhibiting_Detailed_Lifelike_Animated_CVPR_2023_paper.pdf">BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike<br />Animated Motion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">88</td>
<td style="text-align: center;">2023-04-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jin_TensoIR_Tensorial_Inverse_Rendering_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_TensoIR_Tensorial_Inverse_Rendering_CVPR_2023_paper.pdf">TensoIR: Tensorial Inverse Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">86</td>
<td style="text-align: center;">2022-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tang_Unifying_Vision_Text_and_Layout_for_Universal_Document_Processing_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_Unifying_Vision_Text_and_Layout_for_Universal_Document_Processing_CVPR_2023_paper.pdf">Unifying Vision, Text, and Layout for Universal Document Processing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">86</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_CORA_Adapting_CLIP_for_Open-Vocabulary_Detection_With_Region_Prompting_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_CORA_Adapting_CLIP_for_Open-Vocabulary_Detection_With_Region_Prompting_and_CVPR_2023_paper.pdf">CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting<br />and Anchor Pre-Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">86</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kong_Efficient_Frequency_Domain-Based_Transformers_for_High-Quality_Image_Deblurring_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_Efficient_Frequency_Domain-Based_Transformers_for_High-Quality_Image_Deblurring_CVPR_2023_paper.pdf">Efficient Frequency Domain-based Transformers for High-Quality Image Deblurring</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">85</td>
<td style="text-align: center;">2023-01-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_DSVT_Dynamic_Sparse_Voxel_Transformer_With_Rotated_Sets_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_DSVT_Dynamic_Sparse_Voxel_Transformer_With_Rotated_Sets_CVPR_2023_paper.pdf">DSVT: Dynamic Sparse Voxel Transformer with Rotated Sets</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">85</td>
<td style="text-align: center;">2022-06-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tao_Siamese_Image_Modeling_for_Self-Supervised_Vision_Representation_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_Siamese_Image_Modeling_for_Self-Supervised_Vision_Representation_Learning_CVPR_2023_paper.pdf">Siamese Image Modeling for Self-Supervised Vision Representation Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">85</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wei_Autoregressive_Visual_Tracking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Autoregressive_Visual_Tracking_CVPR_2023_paper.pdf">Autoregressive Visual Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">85</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Attal_HyperReel_High-Fidelity_6-DoF_Video_With_Ray-Conditioned_Sampling_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Attal_HyperReel_High-Fidelity_6-DoF_Video_With_Ray-Conditioned_Sampling_CVPR_2023_paper.pdf">HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">84</td>
<td style="text-align: center;">2022-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_CLIP_Is_Also_an_Efficient_Segmenter_A_Text-Driven_Approach_for_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_CLIP_Is_Also_an_Efficient_Segmenter_A_Text-Driven_Approach_for_CVPR_2023_paper.pdf">CLIP is Also an Efficient Segmenter: A Text-Driven Approach<br />for Weakly Supervised Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">84</td>
<td style="text-align: center;">2023-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wynn_DiffusioNeRF_Regularizing_Neural_Radiance_Fields_With_Denoising_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wynn_DiffusioNeRF_Regularizing_Neural_Radiance_Fields_With_Denoising_Diffusion_Models_CVPR_2023_paper.pdf">DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">84</td>
<td style="text-align: center;">2022-11-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Minimizing_the_Accumulated_Trajectory_Error_To_Improve_Dataset_Distillation_CVPR_2023_paper.pdf">Minimizing the Accumulated Trajectory Error to Improve Dataset Distillation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zielonka_Instant_Volumetric_Head_Avatars_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zielonka_Instant_Volumetric_Head_Avatars_CVPR_2023_paper.pdf">Instant Volumetric Head Avatars</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tseng_Consistent_View_Synthesis_With_Pose-Guided_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tseng_Consistent_View_Synthesis_With_Pose-Guided_Diffusion_Models_CVPR_2023_paper.pdf">Consistent View Synthesis with Pose-Guided Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2023-02-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Aligning_Bag_of_Regions_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Aligning_Bag_of_Regions_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf">Aligning Bag of Regions for Open-Vocabulary Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2022-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Girdhar_OmniMAE_Single_Model_Masked_Pretraining_on_Images_and_Videos_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Girdhar_OmniMAE_Single_Model_Masked_Pretraining_on_Images_and_Videos_CVPR_2023_paper.pdf">OmniMAE: Single Model Masked Pretraining on Images and Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2023-03-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Dynamic_Graph_Enhanced_Contrastive_Learning_for_Chest_X-Ray_Report_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Dynamic_Graph_Enhanced_Contrastive_Learning_for_Chest_X-Ray_Report_Generation_CVPR_2023_paper.pdf">Dynamic Graph Enhanced Contrastive Learning for Chest X-Ray Report<br />Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Turki_SUDS_Scalable_Urban_Dynamic_Scenes_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Turki_SUDS_Scalable_Urban_Dynamic_Scenes_CVPR_2023_paper.pdf">SUDS: Scalable Urban Dynamic Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">81</td>
<td style="text-align: center;">2022-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gehrig_Recurrent_Vision_Transformers_for_Object_Detection_With_Event_Cameras_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gehrig_Recurrent_Vision_Transformers_for_Object_Detection_With_Event_Cameras_CVPR_2023_paper.pdf">Recurrent Vision Transformers for Object Detection with Event Cameras</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">81</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.pdf">SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Inoue_LayoutDM_Discrete_Diffusion_Model_for_Controllable_Layout_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Inoue_LayoutDM_Discrete_Diffusion_Model_for_Controllable_Layout_Generation_CVPR_2023_paper.pdf">LayoutDM: Discrete Diffusion Model for Controllable Layout Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2023-01-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tao_GALIP_Generative_Adversarial_CLIPs_for_Text-to-Image_Synthesis_CVPR_2023_paper.pdf">GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2023-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Taming_Diffusion_Models_for_Audio-Driven_Co-Speech_Gesture_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Taming_Diffusion_Models_for_Audio-Driven_Co-Speech_Gesture_Generation_CVPR_2023_paper.pdf">Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">79</td>
<td style="text-align: center;">2022-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_InstantAvatar_Learning_Avatars_From_Monocular_Video_in_60_Seconds_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_InstantAvatar_Learning_Avatars_From_Monocular_Video_in_60_Seconds_CVPR_2023_paper.pdf">InstantAvatar: Learning Avatars from Monocular Video in 60 Seconds</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">79</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yuan_Robust_Test-Time_Adaptation_in_Dynamic_Scenarios_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yuan_Robust_Test-Time_Adaptation_in_Dynamic_Scenarios_CVPR_2023_paper.pdf">Robust Test-Time Adaptation in Dynamic Scenarios</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">79</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_DeSTSeg_Segmentation_Guided_Denoising_Student-Teacher_for_Anomaly_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_DeSTSeg_Segmentation_Guided_Denoising_Student-Teacher_for_Anomaly_Detection_CVPR_2023_paper.pdf">DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_Curricular_Contrastive_Regularization_for_Physics-Aware_Single_Image_Dehazing_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_Curricular_Contrastive_Regularization_for_Physics-Aware_Single_Image_Dehazing_CVPR_2023_paper.pdf">Curricular Contrastive Regularization for Physics-Aware Single Image Dehazing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2022-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chung_Solving_3D_Inverse_Problems_Using_Pre-Trained_2D_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chung_Solving_3D_Inverse_Problems_Using_Pre-Trained_2D_Diffusion_Models_CVPR_2023_paper.pdf">Solving 3D Inverse Problems Using Pre-Trained 2D Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2022-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Uncovering_the_Disentanglement_Capability_in_Text-to-Image_Diffusion_Models_CVPR_2023_paper.pdf">Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2023-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Rempe_Trace_and_Pace_Controllable_Pedestrian_Animation_via_Guided_Trajectory_Diffusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rempe_Trace_and_Pace_Controllable_Pedestrian_Animation_via_Guided_Trajectory_Diffusion_CVPR_2023_paper.pdf">Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory<br />Diffusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Guo_Hierarchical_Fine-Grained_Image_Forgery_Detection_and_Localization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Hierarchical_Fine-Grained_Image_Forgery_Detection_and_Localization_CVPR_2023_paper.pdf">Hierarchical Fine-Grained Image Forgery Detection and Localization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2023-03-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Virtual_Sparse_Convolution_for_Multimodal_3D_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Virtual_Sparse_Convolution_for_Multimodal_3D_Object_Detection_CVPR_2023_paper.pdf">Virtual Sparse Convolution for Multimodal 3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Meuleman_Progressively_Optimized_Local_Radiance_Fields_for_Robust_View_Synthesis_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Meuleman_Progressively_Optimized_Local_Radiance_Fields_for_Robust_View_Synthesis_CVPR_2023_paper.pdf">Progressively Optimized Local Radiance Fields for Robust View Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Dense_Distinct_Query_for_End-to-End_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Dense_Distinct_Query_for_End-to-End_Object_Detection_CVPR_2023_paper.pdf">Dense Distinct Query for End-to-End Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mao_Leapfrog_Diffusion_Model_for_Stochastic_Trajectory_Prediction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mao_Leapfrog_Diffusion_Model_for_Stochastic_Trajectory_Prediction_CVPR_2023_paper.pdf">Leapfrog Diffusion Model for Stochastic Trajectory Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">76</td>
<td style="text-align: center;">2023-04-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Collaborative_Diffusion_for_Multi-Modal_Face_Generation_and_Editing_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Collaborative_Diffusion_for_Multi-Modal_Face_Generation_and_Editing_CVPR_2023_paper.pdf">Collaborative Diffusion for Multi-Modal Face Generation and Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">76</td>
<td style="text-align: center;">2023-05-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_3D_Registration_With_Maximal_Cliques_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_3D_Registration_With_Maximal_Cliques_CVPR_2023_paper.pdf">3D Registration with Maximal Cliques</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bhunia_Person_Image_Synthesis_via_Denoising_Diffusion_Model_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bhunia_Person_Image_Synthesis_via_Denoising_Diffusion_Model_CVPR_2023_paper.pdf">Person Image Synthesis via Denoising Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">2022-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Diffusion_Probabilistic_Model_Made_Slim_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Diffusion_Probabilistic_Model_Made_Slim_CVPR_2023_paper.pdf">Diffusion Probabilistic Model Made Slim</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">2023-05-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bai_Bidirectional_Copy-Paste_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bai_Bidirectional_Copy-Paste_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf">Bidirectional Copy-Paste for Semi-Supervised Medical Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">74</td>
<td style="text-align: center;">2023-04-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kim_DCFace_Synthetic_Face_Generation_With_Dual_Condition_Diffusion_Model_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_DCFace_Synthetic_Face_Generation_With_Dual_Condition_Diffusion_Model_CVPR_2023_paper.pdf">DCFace: Synthetic Face Generation with Dual Condition Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2023-02-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Guo_Vid2Avatar_3D_Avatar_Reconstruction_From_Videos_in_the_Wild_via_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Vid2Avatar_3D_Avatar_Reconstruction_From_Videos_in_the_Wild_via_CVPR_2023_paper.pdf">Vid2Avatar: 3D Avatar Reconstruction from Videos in the Wild<br />via Self-supervised Scene Decomposition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Masked_Video_Distillation_Rethinking_Masked_Feature_Modeling_for_Self-Supervised_Video_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Masked_Video_Distillation_Rethinking_Masked_Feature_Modeling_for_Self-Supervised_Video_CVPR_2023_paper.pdf">Masked Video Distillation: Rethinking Masked Feature Modeling for Self-supervised<br />Video Representation Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2022-07-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xiong_FedDM_Iterative_Distribution_Matching_for_Communication-Efficient_Federated_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_FedDM_Iterative_Distribution_Matching_for_Communication-Efficient_Federated_Learning_CVPR_2023_paper.pdf">FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Rethinking_Federated_Learning_With_Domain_Shift_A_Prototype_View_CVPR_2023_paper.pdf">Rethinking Federated Learning with Domain Shift: A Prototype View</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2022-10-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dong_Implicit_Identity_Leakage_The_Stumbling_Block_to_Improving_Deepfake_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Implicit_Identity_Leakage_The_Stumbling_Block_to_Improving_Deepfake_Detection_CVPR_2023_paper.pdf">Implicit Identity Leakage: The Stumbling Block to Improving Deepfake<br />Detection Generalization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Beyer_FlexiViT_One_Model_for_All_Patch_Sizes_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Beyer_FlexiViT_One_Model_for_All_Patch_Sizes_CVPR_2023_paper.pdf">FlexiViT: One Model for All Patch Sizes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fel_CRAFT_Concept_Recursive_Activation_FacTorization_for_Explainability_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fel_CRAFT_Concept_Recursive_Activation_FacTorization_for_Explainability_CVPR_2023_paper.pdf">CRAFT: Concept Recursive Activation FacTorization for Explainability</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2023-04-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_CompletionFormer_Depth_Completion_With_Convolutions_and_Vision_Transformers_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_CompletionFormer_Depth_Completion_With_Convolutions_and_Vision_Transformers_CVPR_2023_paper.pdf">CompletionFormer: Depth Completion with Convolutions and Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2023-01-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shen_DiffTalk_Crafting_Diffusion_Models_for_Generalized_Audio-Driven_Portraits_Animation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_DiffTalk_Crafting_Diffusion_Models_for_Generalized_Audio-Driven_Portraits_Animation_CVPR_2023_paper.pdf">DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2022-02-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Edstedt_DKM_Dense_Kernelized_Feature_Matching_for_Geometry_Estimation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Edstedt_DKM_Dense_Kernelized_Feature_Matching_for_Geometry_Estimation_CVPR_2023_paper.pdf">DKM: Dense Kernelized Feature Matching for Geometry Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2023-03-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Feature_Shrinkage_Pyramid_for_Camouflaged_Object_Detection_With_Transformers_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Feature_Shrinkage_Pyramid_for_Camouflaged_Object_Detection_With_Transformers_CVPR_2023_paper.pdf">Feature Shrinkage Pyramid for Camouflaged Object Detection with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2023-03-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_UniDexGrasp_Universal_Robotic_Dexterous_Grasping_via_Learning_Diverse_Proposal_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_UniDexGrasp_Universal_Robotic_Dexterous_Grasping_via_Learning_Diverse_Proposal_Generation_CVPR_2023_paper.pdf">UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal<br />Generation and Goal-Conditioned Policy</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2022-06-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_LAVENDER_Unifying_Video-Language_Understanding_As_Masked_Language_Modeling_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_LAVENDER_Unifying_Video-Language_Understanding_As_Masked_Language_Modeling_CVPR_2023_paper.pdf">LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2023-04-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Rahman_Ambiguous_Medical_Image_Segmentation_Using_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rahman_Ambiguous_Medical_Image_Segmentation_Using_Diffusion_Models_CVPR_2023_paper.pdf">Ambiguous Medical Image Segmentation Using Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2022-07-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Brazil_Omni3D_A_Large_Benchmark_and_Model_for_3D_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Brazil_Omni3D_A_Large_Benchmark_and_Model_for_3D_Object_Detection_CVPR_2023_paper.pdf">Omni3D: A Large Benchmark and Model for 3D Object<br />Detection in the Wild</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">70</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_TryOnDiffusion_A_Tale_of_Two_UNets_CVPR_2023_paper.pdf">TryOnDiffusion: A Tale of Two UNets</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">70</td>
<td style="text-align: center;">2023-02-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Saito_Pic2Word_Mapping_Pictures_to_Words_for_Zero-Shot_Composed_Image_Retrieval_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Saito_Pic2Word_Mapping_Pictures_to_Words_for_Zero-Shot_Composed_Image_Retrieval_CVPR_2023_paper.pdf">Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image<br />Retrieval</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">70</td>
<td style="text-align: center;">2022-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_NICO_Towards_Better_Benchmarking_for_Domain_Generalization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_NICO_Towards_Better_Benchmarking_for_Domain_Generalization_CVPR_2023_paper.pdf">NICO++: Towards Better Benchmarking for Domain Generalization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_One-Stage_3D_Whole-Body_Mesh_Recovery_With_Component_Aware_Transformer_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_One-Stage_3D_Whole-Body_Mesh_Recovery_With_Component_Aware_Transformer_CVPR_2023_paper.pdf">One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Beyond_Appearance_A_Semantic_Controllable_Self-Supervised_Learning_Framework_for_Human-Centric_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Beyond_Appearance_A_Semantic_Controllable_Self-Supervised_Learning_Framework_for_Human-Centric_CVPR_2023_paper.pdf">Beyond Appearance: A Semantic Controllable Self-Supervised Learning Framework for<br />Human-Centric Visual Tasks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2022-06-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tan_Temporal_Attention_Unit_Towards_Efficient_Spatiotemporal_Predictive_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Temporal_Attention_Unit_Towards_Efficient_Spatiotemporal_Predictive_Learning_CVPR_2023_paper.pdf">Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2022-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_REVEAL_Retrieval-Augmented_Visual-Language_Pre-Training_With_Multi-Source_Multimodal_Knowledge_Memory_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_REVEAL_Retrieval-Augmented_Visual-Language_Pre-Training_With_Multi-Source_Multimodal_Knowledge_Memory_CVPR_2023_paper.pdf">Reveal: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Guo_ShadowDiffusion_When_Degradation_Prior_Meets_Diffusion_Model_for_Shadow_Removal_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_ShadowDiffusion_When_Degradation_Prior_Meets_Diffusion_Model_for_Shadow_Removal_CVPR_2023_paper.pdf">ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow<br />Removal</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Selective_Structured_State-Spaces_for_Long-Form_Video_Understanding_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Selective_Structured_State-Spaces_for_Long-Form_Video_Understanding_CVPR_2023_paper.pdf">Selective Structured State-Spaces for Long-Form Video Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2023-01-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ramanathan_PACO_Parts_and_Attributes_of_Common_Objects_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ramanathan_PACO_Parts_and_Attributes_of_Common_Objects_CVPR_2023_paper.pdf">PACO: Parts and Attributes of Common Objects</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2022-08-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gu_ViP3D_End-to-End_Visual_Trajectory_Prediction_via_3D_Agent_Queries_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gu_ViP3D_End-to-End_Visual_Trajectory_Prediction_via_3D_Agent_Queries_CVPR_2023_paper.pdf">ViP3D: End-to-End Visual Trajectory Prediction via 3D Agent Queries</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2022-11-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Geng_GAPartNet_Cross-Category_Domain-Generalizable_Object_Perception_and_Manipulation_via_Generalizable_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Geng_GAPartNet_Cross-Category_Domain-Generalizable_Object_Perception_and_Manipulation_via_Generalizable_and_CVPR_2023_paper.pdf">GAPartNet: Cross-Category Domain-Generalizable Object Perception and Manipulation via Generalizable<br />and Actionable Parts</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Grid-Guided_Neural_Radiance_Fields_for_Large_Urban_Scenes_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Grid-Guided_Neural_Radiance_Fields_for_Large_Urban_Scenes_CVPR_2023_paper.pdf">Grid-guided Neural Radiance Fields for Large Urban Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Radenovic_Filtering_Distillation_and_Hard_Negatives_for_Vision-Language_Pre-Training_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Radenovic_Filtering_Distillation_and_Hard_Negatives_for_Vision-Language_Pre-Training_CVPR_2023_paper.pdf">Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2023-03-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shi_FlowFormer_Masked_Cost_Volume_Autoencoding_for_Pretraining_Optical_Flow_Estimation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_FlowFormer_Masked_Cost_Volume_Autoencoding_for_Pretraining_Optical_Flow_Estimation_CVPR_2023_paper.pdf">FlowFormer++: Masked Cost Volume Autoencoding for Pretraining Optical Flow<br />Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2022-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chung_Parallel_Diffusion_Models_of_Operator_and_Image_for_Blind_Inverse_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chung_Parallel_Diffusion_Models_of_Operator_and_Image_for_Blind_Inverse_CVPR_2023_paper.pdf">Parallel Diffusion Models of Operator and Image for Blind<br />Inverse Problems</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2023-02-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kong_vMAP_Vectorised_Object_Mapping_for_Neural_Field_SLAM_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_vMAP_Vectorised_Object_Mapping_for_Neural_Field_SLAM_CVPR_2023_paper.pdf">vMAP: Vectorised Object Mapping for Neural Field SLAM</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2023-01-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Multimodality_Helps_Unimodality_Cross-Modal_Few-Shot_Learning_With_Multimodal_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Multimodality_Helps_Unimodality_Cross-Modal_Few-Shot_Learning_With_Multimodal_Models_CVPR_2023_paper.pdf">Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.pdf">Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cheng_VindLU_A_Recipe_for_Effective_Video-and-Language_Pretraining_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_VindLU_A_Recipe_for_Effective_Video-and-Language_Pretraining_CVPR_2023_paper.pdf">VindLU: A Recipe for Effective Video-and-Language Pretraining</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-06-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kong_LaserMix_for_Semi-Supervised_LiDAR_Semantic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_LaserMix_for_Semi-Supervised_LiDAR_Semantic_Segmentation_CVPR_2023_paper.pdf">LaserMix for Semi-Supervised LiDAR Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chou_How_to_Backdoor_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chou_How_to_Backdoor_Diffusion_Models_CVPR_2023_paper.pdf">How to Backdoor Diffusion Models?</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2023-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Neural_Fields_Meet_Explicit_Geometric_Representations_for_Inverse_Rendering_of_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Neural_Fields_Meet_Explicit_Geometric_Representations_for_Inverse_Rendering_of_CVPR_2023_paper.pdf">Neural Fields Meet Explicit Geometric Representations for Inverse Rendering<br />of Urban Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2023-03-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lee_Multimodal_Prompting_With_Missing_Modalities_for_Visual_Recognition_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Multimodal_Prompting_With_Missing_Modalities_for_Visual_Recognition_CVPR_2023_paper.pdf">Multimodal Prompting with Missing Modalities for Visual Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Visual_Language_Pretrained_Multiple_Instance_Zero-Shot_Transfer_for_Histopathology_Images_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Visual_Language_Pretrained_Multiple_Instance_Zero-Shot_Transfer_for_Histopathology_Images_CVPR_2023_paper.pdf">Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology<br />Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jain_VectorFusion_Text-to-SVG_by_Abstracting_Pixel-Based_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jain_VectorFusion_Text-to-SVG_by_Abstracting_Pixel-Based_Diffusion_Models_CVPR_2023_paper.pdf">VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cha_Learning_To_Generate_Text-Grounded_Mask_for_Open-World_Semantic_Segmentation_From_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cha_Learning_To_Generate_Text-Grounded_Mask_for_Open-World_Semantic_Segmentation_From_CVPR_2023_paper.pdf">Learning to Generate Text-Grounded Mask for Open-World Semantic Segmentation<br />from Only Image-Text Pairs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2022-10-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sohn_Visual_Prompt_Tuning_for_Generative_Transfer_Learning_CVPR_2023_paper.pdf">Visual Prompt Tuning for Generative Transfer Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dobler_Robust_Mean_Teacher_for_Continual_and_Gradual_Test-Time_Adaptation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dobler_Robust_Mean_Teacher_for_Continual_and_Gradual_Test-Time_Adaptation_CVPR_2023_paper.pdf">Robust Mean Teacher for Continual and Gradual Test-Time Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_EqMotion_Equivariant_Multi-Agent_Motion_Prediction_With_Invariant_Interaction_Reasoning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_EqMotion_Equivariant_Multi-Agent_Motion_Prediction_With_Invariant_Interaction_Reasoning_CVPR_2023_paper.pdf">EqMotion: Equivariant Multi-Agent Motion Prediction with Invariant Interaction Reasoning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Qin_FreeSeg_Unified_Universal_and_Open-Vocabulary_Image_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_FreeSeg_Unified_Universal_and_Open-Vocabulary_Image_Segmentation_CVPR_2023_paper.pdf">FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2023-05-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jia_Think_Twice_Before_Driving_Towards_Scalable_Decoders_for_End-to-End_Autonomous_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jia_Think_Twice_Before_Driving_Towards_Scalable_Decoders_for_End-to-End_Autonomous_CVPR_2023_paper.pdf">Think Twice before Driving: Towards Scalable Decoders for End-to-End<br />Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kollias_Multi-Label_Compound_Expression_Recognition_C-EXPR_Database__Network_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kollias_Multi-Label_Compound_Expression_Recognition_C-EXPR_Database__Network_CVPR_2023_paper.pdf">Multi-Label Compound Expression Recognition: C-EXPR Database &amp; Network</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dong_Benchmarking_Robustness_of_3D_Object_Detection_to_Common_Corruptions_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Benchmarking_Robustness_of_3D_Object_Detection_to_Common_Corruptions_CVPR_2023_paper.pdf">Benchmarking Robustness of 3D Object Detection to Common Corruptions<br />in Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2022-11-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_High-Fidelity_3D_GAN_Inversion_by_Pseudo-Multi-View_Optimization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_High-Fidelity_3D_GAN_Inversion_by_Pseudo-Multi-View_Optimization_CVPR_2023_paper.pdf">High-fidelity 3D GAN Inversion by Pseudo-multi-view Optimization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2022-09-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ramos_SmallCap_Lightweight_Image_Captioning_Prompted_With_Retrieval_Augmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ramos_SmallCap_Lightweight_Image_Captioning_Prompted_With_Retrieval_Augmentation_CVPR_2023_paper.pdf">Smallcap: Lightweight Image Captioning Prompted with Retrieval Augmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_GP-VTON_Towards_General_Purpose_Virtual_Try-On_via_Collaborative_Local-Flow_Global-Parsing_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_GP-VTON_Towards_General_Purpose_Virtual_Try-On_via_Collaborative_Local-Flow_Global-Parsing_CVPR_2023_paper.pdf">GP-VTON: Towards General Purpose Virtual Try-On via Collaborative Local-Flow<br />Global-Parsing Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2023-03-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ru_Token_Contrast_for_Weakly-Supervised_Semantic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ru_Token_Contrast_for_Weakly-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">Token Contrast for Weakly-Supervised Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tien_Revisiting_Reverse_Distillation_for_Anomaly_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tien_Revisiting_Reverse_Distillation_for_Anomaly_Detection_CVPR_2023_paper.pdf">Revisiting Reverse Distillation for Anomaly Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Detecting_Everything_in_the_Open_World_Towards_Universal_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Detecting_Everything_in_the_Open_World_Towards_Universal_Object_Detection_CVPR_2023_paper.pdf">Detecting Everything in the Open World: Towards Universal Object<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Piccinelli_iDisc_Internal_Discretization_for_Monocular_Depth_Estimation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Piccinelli_iDisc_Internal_Discretization_for_Monocular_Depth_Estimation_CVPR_2023_paper.pdf">iDisc: Internal Discretization for Monocular Depth Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2022-06-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Rudnev_EventNeRF_Neural_Radiance_Fields_From_a_Single_Colour_Event_Camera_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rudnev_EventNeRF_Neural_Radiance_Fields_From_a_Single_Colour_Event_Camera_CVPR_2023_paper.pdf">EventNeRF: Neural Radiance Fields from a Single Colour Event<br />Camera</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gao_Generalized_Relation_Modeling_for_Transformer_Tracking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Generalized_Relation_Modeling_for_Transformer_Tracking_CVPR_2023_paper.pdf">Generalized Relation Modeling for Transformer Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_DiffCollage_Parallel_Generation_of_Large_Content_With_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_DiffCollage_Parallel_Generation_of_Large_Content_With_Diffusion_Models_CVPR_2023_paper.pdf">DiffCollage: Parallel Generation of Large Content with Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2023-05-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cazenavette_Generalizing_Dataset_Distillation_via_Deep_Generative_Prior_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cazenavette_Generalizing_Dataset_Distillation_via_Deep_Generative_Prior_CVPR_2023_paper.pdf">Generalizing Dataset Distillation via Deep Generative Prior</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_MagicPony_Learning_Articulated_3D_Animals_in_the_Wild_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_MagicPony_Learning_Articulated_3D_Animals_in_the_Wild_CVPR_2023_paper.pdf">MagicPony: Learning Articulated 3D Animals in the Wild</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2023-04-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Saha_Re-IQA_Unsupervised_Learning_for_Image_Quality_Assessment_in_the_Wild_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Saha_Re-IQA_Unsupervised_Learning_for_Image_Quality_Assessment_in_the_Wild_CVPR_2023_paper.pdf">Re-IQA: Unsupervised Learning for Image Quality Assessment in the<br />Wild</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Seeing_What_You_Said_Talking_Face_Generation_Guided_by_a_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Seeing_What_You_Said_Talking_Face_Generation_Guided_by_a_CVPR_2023_paper.pdf">Seeing What You Said: Talking Face Generation Guided by<br />a Lip Reading Expert</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Implicit_Identity_Driven_Deepfake_Face_Swapping_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Implicit_Identity_Driven_Deepfake_Face_Swapping_Detection_CVPR_2023_paper.pdf">Implicit Identity Driven Deepfake Face Swapping Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2022-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Phung_Wavelet_Diffusion_Models_Are_Fast_and_Scalable_Image_Generators_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Phung_Wavelet_Diffusion_Models_Are_Fast_and_Scalable_Image_Generators_CVPR_2023_paper.pdf">Wavelet Diffusion Models are fast and scalable Image Generators</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2021-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Boutros_CR-FIQA_Face_Image_Quality_Assessment_by_Learning_Sample_Relative_Classifiability_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Boutros_CR-FIQA_Face_Image_Quality_Assessment_by_Learning_Sample_Relative_Classifiability_CVPR_2023_paper.pdf">CR-FIQA: Face Image Quality Assessment by Learning Sample Relative<br />Classifiability</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2023-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Song_EcoTTA_Memory-Efficient_Continual_Test-Time_Adaptation_via_Self-Distilled_Regularization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_EcoTTA_Memory-Efficient_Continual_Test-Time_Adaptation_via_Self-Distilled_Regularization_CVPR_2023_paper.pdf">EcoTTA: Memory-Efficient Continual Test-Time Adaptation via Self-Distilled Regularization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2023-05-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_V2X-Seq_A_Large-Scale_Sequential_Dataset_for_Vehicle-Infrastructure_Cooperative_Perception_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_V2X-Seq_A_Large-Scale_Sequential_Dataset_for_Vehicle-Infrastructure_Cooperative_Perception_and_CVPR_2023_paper.pdf">V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception<br />and Forecasting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2023-04-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yao_DetCLIPv2_Scalable_Open-Vocabulary_Object_Detection_Pre-Training_via_Word-Region_Alignment_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_DetCLIPv2_Scalable_Open-Vocabulary_Object_Detection_Pre-Training_via_Word-Region_Alignment_CVPR_2023_paper.pdf">DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-training via Word-Region Alignment</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2023-03-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Delivering_Arbitrary-Modal_Semantic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Delivering_Arbitrary-Modal_Semantic_Segmentation_CVPR_2023_paper.pdf">Delivering Arbitrary-Modal Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2023-02-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ye_Decoupling_Human_and_Camera_Motion_From_Videos_in_the_Wild_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Decoupling_Human_and_Camera_Motion_From_Videos_in_the_Wild_CVPR_2023_paper.pdf">Decoupling Human and Camera Motion from Videos in the<br />Wild</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2023-03-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_LoGoNet_Towards_Accurate_3D_Object_Detection_With_Local-to-Global_Cross-Modal_Fusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_LoGoNet_Towards_Accurate_3D_Object_Detection_With_Local-to-Global_Cross-Modal_Fusion_CVPR_2023_paper.pdf">LoGoNet: Towards Accurate 3D Object Detection with Local-to-Global Cross-<br />Modal Fusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2022-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Guillaro_TruFor_Leveraging_All-Round_Clues_for_Trustworthy_Image_Forgery_Detection_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guillaro_TruFor_Leveraging_All-Round_Clues_for_Trustworthy_Image_Forgery_Detection_and_CVPR_2023_paper.pdf">TruFor: Leveraging All-Round Clues for Trustworthy Image Forgery Detection<br />and Localization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Diverse_Embedding_Expansion_Network_and_Low-Light_Cross-Modality_Benchmark_for_Visible-Infrared_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Diverse_Embedding_Expansion_Network_and_Low-Light_Cross-Modality_Benchmark_for_Visible-Infrared_CVPR_2023_paper.pdf">Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for<br />Visible-Infrared Person Re-identification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2023-03-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Extracting_Motion_and_Appearance_via_Inter-Frame_Attention_for_Efficient_Video_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Extracting_Motion_and_Appearance_via_Inter-Frame_Attention_for_Efficient_Video_CVPR_2023_paper.pdf">Extracting Motion and Appearance via Inter-Frame Attention for Efficient<br />Video Frame Interpolation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Interventional_Bag_Multi-Instance_Learning_on_Whole-Slide_Pathological_Images_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Interventional_Bag_Multi-Instance_Learning_on_Whole-Slide_Pathological_Images_CVPR_2023_paper.pdf">Interventional Bag Multi-Instance Learning On Whole-Slide Pathological Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Rosu_PermutoSDF_Fast_Multi-View_Reconstruction_With_Implicit_Surfaces_Using_Permutohedral_Lattices_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rosu_PermutoSDF_Fast_Multi-View_Reconstruction_With_Implicit_Surfaces_Using_Permutohedral_Lattices_CVPR_2023_paper.pdf">PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces Using Permutohedral<br />Lattices</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">59</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Improved_Distribution_Matching_for_Dataset_Condensation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Improved_Distribution_Matching_for_Dataset_Condensation_CVPR_2023_paper.pdf">Improved Distribution Matching for Dataset Condensation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">58</td>
<td style="text-align: center;">2023-04-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_AMT_All-Pairs_Multi-Field_Transforms_for_Efficient_Frame_Interpolation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_AMT_All-Pairs_Multi-Field_Transforms_for_Efficient_Frame_Interpolation_CVPR_2023_paper.pdf">AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">58</td>
<td style="text-align: center;">2023-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_A_Dynamic_Multi-Scale_Voxel_Flow_Network_for_Video_Prediction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_A_Dynamic_Multi-Scale_Voxel_Flow_Network_for_Video_Prediction_CVPR_2023_paper.pdf">A Dynamic Multi-Scale Voxel Flow Network for Video Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">58</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/An_PanoHead_Geometry-Aware_3D_Full-Head_Synthesis_in_360deg_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/An_PanoHead_Geometry-Aware_3D_Full-Head_Synthesis_in_360deg_CVPR_2023_paper.pdf">PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360°</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">58</td>
<td style="text-align: center;">2023-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_MonoHuman_Animatable_Human_Neural_Field_From_Monocular_Video_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_MonoHuman_Animatable_Human_Neural_Field_From_Monocular_Video_CVPR_2023_paper.pdf">MonoHuman: Animatable Human Neural Field from Monocular Video</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">58</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sain_CLIP_for_All_Things_Zero-Shot_Sketch-Based_Image_Retrieval_Fine-Grained_or_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sain_CLIP_for_All_Things_Zero-Shot_Sketch-Based_Image_Retrieval_Fine-Grained_or_CVPR_2023_paper.pdf">CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained<br />or Not</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">58</td>
<td style="text-align: center;">2023-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wasim_Vita-CLIP_Video_and_Text_Adaptive_CLIP_via_Multimodal_Prompting_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wasim_Vita-CLIP_Video_and_Text_Adaptive_CLIP_via_Multimodal_Prompting_CVPR_2023_paper.pdf">Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">58</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Du_Avatars_Grow_Legs_Generating_Smooth_Human_Motion_From_Sparse_Tracking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Avatars_Grow_Legs_Generating_Smooth_Human_Motion_From_Sparse_Tracking_CVPR_2023_paper.pdf">Avatars Grow Legs: Generating Smooth Human Motion from Sparse<br />Tracking Inputs with Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zeng_CLIP2_Contrastive_Language-Image-Point_Pretraining_From_Real-World_Point_Cloud_Data_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_CLIP2_Contrastive_Language-Image-Point_Pretraining_From_Real-World_Point_Cloud_Data_CVPR_2023_paper.pdf">CLIP2: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yan_NeRF-DS_Neural_Radiance_Fields_for_Dynamic_Specular_Objects_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yan_NeRF-DS_Neural_Radiance_Fields_for_Dynamic_Specular_Objects_CVPR_2023_paper.pdf">NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Understanding_and_Improving_Visual_Prompting_A_Label-Mapping_Perspective_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Understanding_and_Improving_Visual_Prompting_A_Label-Mapping_Perspective_CVPR_2023_paper.pdf">Understanding and Improving Visual Prompting: A Label-Mapping Perspective</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2023-01-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Multiview_Compressive_Coding_for_3D_Reconstruction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Multiview_Compressive_Coding_for_3D_Reconstruction_CVPR_2023_paper.pdf">Multiview Compressive Coding for 3D Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2023-03-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Qin_MotionTrack_Learning_Robust_Short-Term_and_Long-Term_Motions_for_Multi-Object_Tracking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qin_MotionTrack_Learning_Robust_Short-Term_and_Long-Term_Motions_for_Multi-Object_Tracking_CVPR_2023_paper.pdf">MotionTrack: Learning Robust Short-Term and Long-Term Motions for Multi-Object<br />Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ye_Affordance_Diffusion_Synthesizing_Hand-Object_Interactions_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_Affordance_Diffusion_Synthesizing_Hand-Object_Interactions_CVPR_2023_paper.pdf">Affordance Diffusion: Synthesizing Hand-Object Interactions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Qiao_End-to-End_Vectorized_HD-Map_Construction_With_Piecewise_Bezier_Curve_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Qiao_End-to-End_Vectorized_HD-Map_Construction_With_Piecewise_Bezier_Curve_CVPR_2023_paper.pdf">End-to-End Vectorized HD-map Construction with Piecewise Bézier Curve</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2022-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.pdf">Task Residual for Tuning Vision-Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2023-03-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Multimodal_Industrial_Anomaly_Detection_via_Hybrid_Fusion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multimodal_Industrial_Anomaly_Detection_via_Hybrid_Fusion_CVPR_2023_paper.pdf">Multimodal Industrial Anomaly Detection via Hybrid Fusion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_A_Whac-a-Mole_Dilemma_Shortcuts_Come_in_Multiples_Where_Mitigating_One_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_A_Whac-a-Mole_Dilemma_Shortcuts_Come_in_Multiples_Where_Mitigating_One_CVPR_2023_paper.pdf">A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating<br />One Amplifies Others</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">57</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Doveh_Teaching_Structured_Vision__Language_Concepts_to_Vision__Language_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Doveh_Teaching_Structured_Vision__Language_Concepts_to_Vision__Language_CVPR_2023_paper.pdf">Teaching Structured Vision &amp; Language Concepts to Vision &amp;<br />Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">56</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tan_Learning_on_Gradients_Generalized_Artifacts_Representation_for_GAN-Generated_Images_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Learning_on_Gradients_Generalized_Artifacts_Representation_for_GAN-Generated_Images_Detection_CVPR_2023_paper.pdf">Learning on Gradients: Generalized Artifacts Representation for GAN-Generated Images<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">56</td>
<td style="text-align: center;">2022-12-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Agustsson_Multi-Realism_Image_Compression_With_a_Conditional_Generator_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Agustsson_Multi-Realism_Image_Compression_With_a_Conditional_Generator_CVPR_2023_paper.pdf">Multi-Realism Image Compression with a Conditional Generator</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">56</td>
<td style="text-align: center;">2023-03-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Sharpness-Aware_Gradient_Matching_for_Domain_Generalization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Sharpness-Aware_Gradient_Matching_for_Domain_Generalization_CVPR_2023_paper.pdf">Sharpness-Aware Gradient Matching for Domain Generalization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">56</td>
<td style="text-align: center;">2022-06-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_LargeKernel3D_Scaling_Up_Kernels_in_3D_Sparse_CNNs_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_LargeKernel3D_Scaling_Up_Kernels_in_3D_Sparse_CNNs_CVPR_2023_paper.pdf">LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">56</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Ingredient-Oriented_Multi-Degradation_Learning_for_Image_Restoration_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Ingredient-Oriented_Multi-Degradation_Learning_for_Image_Restoration_CVPR_2023_paper.pdf">Ingredient-oriented Multi-Degradation Learning for Image Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">55</td>
<td style="text-align: center;">2022-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ye_DeepSolo_Let_Transformer_Decoder_With_Explicit_Points_Solo_for_Text_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_DeepSolo_Let_Transformer_Decoder_With_Explicit_Points_Solo_for_Text_CVPR_2023_paper.pdf">DeepSolo: Let Transformer Decoder with Explicit Points Solo for<br />Text Spotting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">55</td>
<td style="text-align: center;">2023-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_METransformer_Radiology_Report_Generation_by_Transformer_With_Multiple_Learnable_Expert_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_METransformer_Radiology_Report_Generation_by_Transformer_With_Multiple_Learnable_Expert_CVPR_2023_paper.pdf">METransformer: Radiology Report Generation by Transformer with Multiple Learnable<br />Expert Tokens</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">55</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Vision_Transformers_Are_Parameter-Efficient_Audio-Visual_Learners_CVPR_2023_paper.pdf">Vision Transformers are Parameter-Efficient Audio-Visual Learners</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">55</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_PoseFormerV2_Exploring_Frequency_Domain_for_Efficient_and_Robust_3D_Human_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_PoseFormerV2_Exploring_Frequency_Domain_for_Efficient_and_Robust_3D_Human_CVPR_2023_paper.pdf">PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D<br />Human Pose Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">54</td>
<td style="text-align: center;">2023-04-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_RIDCP_Revitalizing_Real_Image_Dehazing_via_High-Quality_Codebook_Priors_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_RIDCP_Revitalizing_Real_Image_Dehazing_via_High-Quality_Codebook_Priors_CVPR_2023_paper.pdf">RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">54</td>
<td style="text-align: center;">2023-05-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shao_ReasonNet_End-to-End_Driving_With_Temporal_and_Global_Reasoning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shao_ReasonNet_End-to-End_Driving_With_Temporal_and_Global_Reasoning_CVPR_2023_paper.pdf">ReasonNet: End-to-End Driving with Temporal and Global Reasoning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">54</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Choi_N-Gram_in_Swin_Transformers_for_Efficient_Lightweight_Image_Super-Resolution_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Choi_N-Gram_in_Swin_Transformers_for_Efficient_Lightweight_Image_Super-Resolution_CVPR_2023_paper.pdf">N-Gram in Swin Transformers for Efficient Lightweight Image Super-Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">54</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Rahman_Make-a-Story_Visual_Memory_Conditioned_Consistent_Story_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rahman_Make-a-Story_Visual_Memory_Conditioned_Consistent_Story_Generation_CVPR_2023_paper.pdf">Make-A-Story: Visual Memory Conditioned Consistent Story Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">54</td>
<td style="text-align: center;">2023-04-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_DropMAE_Masked_Autoencoders_With_Spatial-Attention_Dropout_for_Tracking_Tasks_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_DropMAE_Masked_Autoencoders_With_Spatial-Attention_Dropout_for_Tracking_Tasks_CVPR_2023_paper.pdf">DropMAE: Masked Autoencoders with Spatial-Attention Dropout for Tracking Tasks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">54</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Song_ObjectStitch_Object_Compositing_With_Diffusion_Model_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_ObjectStitch_Object_Compositing_With_Diffusion_Model_CVPR_2023_paper.pdf">ObjectStitch: Object Compositing with Diffusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2022-12-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_PartSLIP_Low-Shot_Part_Segmentation_for_3D_Point_Clouds_via_Pretrained_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PartSLIP_Low-Shot_Part_Segmentation_for_3D_Point_Clouds_via_Pretrained_CVPR_2023_paper.pdf">PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via<br />Pretrained Image-Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-04-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Omni_Aggregation_Networks_for_Lightweight_Image_Super-Resolution_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Omni_Aggregation_Networks_for_Lightweight_Image_Super-Resolution_CVPR_2023_paper.pdf">Omni Aggregation Networks for Lightweight Image Super-Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-03-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Peng_Hierarchical_Dense_Correlation_Distillation_for_Few-Shot_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_Hierarchical_Dense_Correlation_Distillation_for_Few-Shot_Segmentation_CVPR_2023_paper.pdf">Hierarchical Dense Correlation Distillation for Few-Shot Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Patch-Mix_Transformer_for_Unsupervised_Domain_Adaptation_A_Game_Perspective_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Patch-Mix_Transformer_for_Unsupervised_Domain_Adaptation_A_Game_Perspective_CVPR_2023_paper.pdf">Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xing_SVFormer_Semi-Supervised_Video_Transformer_for_Action_Recognition_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xing_SVFormer_Semi-Supervised_Video_Transformer_for_Action_Recognition_CVPR_2023_paper.pdf">SVFormer: Semi-supervised Video Transformer for Action Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-04-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Learning_Semantic-Aware_Knowledge_Guidance_for_Low-Light_Image_Enhancement_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Learning_Semantic-Aware_Knowledge_Guidance_for_Low-Light_Image_Enhancement_CVPR_2023_paper.pdf">Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-05-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.pdf">Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Leclerc_FFCV_Accelerating_Training_by_Removing_Data_Bottlenecks_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Leclerc_FFCV_Accelerating_Training_by_Removing_Data_Bottlenecks_CVPR_2023_paper.pdf">FFCV: Accelerating Training by Removing Data Bottlenecks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.pdf">PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2022-09-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fu_An_Empirical_Study_of_End-to-End_Video-Language_Transformers_With_Masked_Visual_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fu_An_Empirical_Study_of_End-to-End_Video-Language_Transformers_With_Masked_Visual_CVPR_2023_paper.pdf">An Empirical Study of End-to-End Video-Language Transformers with Masked<br />Visual Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Multi-Modal_Learning_With_Missing_Modality_via_Shared-Specific_Feature_Modelling_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Multi-Modal_Learning_With_Missing_Modality_via_Shared-Specific_Feature_Modelling_CVPR_2023_paper.pdf">Multi-Modal Learning with Missing Modality via Shared-Specific Feature Modelling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">53</td>
<td style="text-align: center;">2023-02-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sabour_RobustNeRF_Ignoring_Distractors_With_Robust_Losses_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sabour_RobustNeRF_Ignoring_Distractors_With_Robust_Losses_CVPR_2023_paper.pdf">RobustNeRF: Ignoring Distractors with Robust Losses</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">52</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_PiMAE_Point_Cloud_and_Image_Interactive_Masked_Autoencoders_for_3D_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_PiMAE_Point_Cloud_and_Image_Interactive_Masked_Autoencoders_for_3D_CVPR_2023_paper.pdf">PiMAE: Point Cloud and Image Interactive Masked Autoencoders for<br />3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">52</td>
<td style="text-align: center;">2022-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Compressing_Volumetric_Radiance_Fields_to_1_MB_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Compressing_Volumetric_Radiance_Fields_to_1_MB_CVPR_2023_paper.pdf">Compressing Volumetric Radiance Fields to 1 MB</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">52</td>
<td style="text-align: center;">2023-03-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tripathi_3D_Human_Pose_Estimation_via_Intuitive_Physics_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tripathi_3D_Human_Pose_Estimation_via_Intuitive_Physics_CVPR_2023_paper.pdf">3D Human Pose Estimation via Intuitive Physics</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">52</td>
<td style="text-align: center;">2022-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/VS_Instance_Relation_Graph_Guided_Source-Free_Domain_Adaptive_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/VS_Instance_Relation_Graph_Guided_Source-Free_Domain_Adaptive_Object_Detection_CVPR_2023_paper.pdf">Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">52</td>
<td style="text-align: center;">2023-01-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_FlatFormer_Flattened_Window_Attention_for_Efficient_Point_Cloud_Transformer_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_FlatFormer_Flattened_Window_Attention_for_Efficient_Point_Cloud_Transformer_CVPR_2023_paper.pdf">FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">52</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Du_Learning_To_Render_Novel_Views_From_Wide-Baseline_Stereo_Pairs_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Learning_To_Render_Novel_Views_From_Wide-Baseline_Stereo_Pairs_CVPR_2023_paper.pdf">Learning to Render Novel Views from Wide-Baseline Stereo Pairs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_BEVHeight_A_Robust_Framework_for_Vision-Based_Roadside_3D_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_BEVHeight_A_Robust_Framework_for_Vision-Based_Roadside_3D_Object_Detection_CVPR_2023_paper.pdf">BEVHeight: A Robust Framework for Vision-based Roadside 3D Object<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2022-09-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jiao_MSMDFusion_Fusing_LiDAR_and_Camera_at_Multiple_Scales_With_Multi-Depth_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiao_MSMDFusion_Fusing_LiDAR_and_Camera_at_Multiple_Scales_With_Multi-Depth_CVPR_2023_paper.pdf">MSMDFusion: Fusing LiDAR and Camera at Multiple Scales with<br />Multi-Depth Seeds for 3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_DR2_Diffusion-Based_Robust_Degradation_Remover_for_Blind_Face_Restoration_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_DR2_Diffusion-Based_Robust_Degradation_Remover_for_Blind_Face_Restoration_CVPR_2023_paper.pdf">DR2: Diffusion-Based Robust Degradation Remover for Blind Face Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2023-04-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kim_NeuralField-LDM_Scene_Generation_With_Hierarchical_Latent_Diffusion_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_NeuralField-LDM_Scene_Generation_With_Hierarchical_Latent_Diffusion_Models_CVPR_2023_paper.pdf">NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2023-02-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Melas-Kyriazi_PC2_Projection-Conditioned_Point_Cloud_Diffusion_for_Single-Image_3D_Reconstruction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Melas-Kyriazi_PC2_Projection-Conditioned_Point_Cloud_Diffusion_for_Single-Image_3D_Reconstruction_CVPR_2023_paper.pdf">PC2: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2023-05-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhong_Identity-Preserving_Talking_Face_Generation_With_Landmark_and_Appearance_Priors_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhong_Identity-Preserving_Talking_Face_Generation_With_Landmark_and_Appearance_Priors_CVPR_2023_paper.pdf">Identity-Preserving Talking Face Generation with Landmark and Appearance Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2022-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Long_NeuralUDF_Learning_Unsigned_Distance_Fields_for_Multi-View_Reconstruction_of_Surfaces_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Long_NeuralUDF_Learning_Unsigned_Distance_Fields_for_Multi-View_Reconstruction_of_Surfaces_CVPR_2023_paper.pdf">NeuralUDF: Learning Unsigned Distance Fields for Multi-View Reconstruction of<br />Surfaces with Arbitrary Topologies</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2022-11-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Phase-Shifting_Coder_Predicting_Accurate_Orientation_in_Oriented_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Phase-Shifting_Coder_Predicting_Accurate_Orientation_in_Oriented_Object_Detection_CVPR_2023_paper.pdf">Phase-Shifting Coder: Predicting Accurate Orientation in Oriented Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">51</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_GD-MAE_Generative_Decoder_for_MAE_Pre-Training_on_LiDAR_Point_Clouds_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_GD-MAE_Generative_Decoder_for_MAE_Pre-Training_on_LiDAR_Point_Clouds_CVPR_2023_paper.pdf">GD-MAE: Generative Decoder for MAE Pre-Training on LiDAR Point<br />Clouds</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Rotation-Invariant_Transformer_for_Point_Cloud_Matching_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Rotation-Invariant_Transformer_for_Point_Cloud_Matching_CVPR_2023_paper.pdf">Rotation-Invariant Transformer for Point Cloud Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2023-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_TrojDiff_Trojan_Attacks_on_Diffusion_Models_With_Diverse_Targets_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_TrojDiff_Trojan_Attacks_on_Diffusion_Models_With_Diverse_Targets_CVPR_2023_paper.pdf">TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xia_SCPNet_Semantic_Scene_Completion_on_Point_Cloud_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xia_SCPNet_Semantic_Scene_Completion_on_Point_Cloud_CVPR_2023_paper.pdf">SCPNet: Semantic Scene Completion on Point Cloud</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xue_Freestyle_Layout-to-Image_Synthesis_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xue_Freestyle_Layout-to-Image_Synthesis_CVPR_2023_paper.pdf">Freestyle Layout-to-Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.pdf">Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and<br />Vision-Language Tasks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2023-03-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Referring_Multi-Object_Tracking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Referring_Multi-Object_Tracking_CVPR_2023_paper.pdf">Referring Multi-Object Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2022-12-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_DisCoScene_Spatially_Disentangled_Generative_Radiance_Fields_for_Controllable_3D-Aware_Scene_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_DisCoScene_Spatially_Disentangled_Generative_Radiance_Fields_for_Controllable_3D-Aware_Scene_CVPR_2023_paper.pdf">DisCoScene: Spatially Disentangled Generative Radiance Fields for Controllable 3D-aware<br />Scene Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Object-Aware_Distillation_Pyramid_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.pdf">Object-Aware Distillation Pyramid for Open-Vocabulary Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2022-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Accelerating_Dataset_Distillation_via_Model_Augmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Accelerating_Dataset_Distillation_via_Model_Augmentation_CVPR_2023_paper.pdf">Accelerating Dataset Distillation via Model Augmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_HNeRV_A_Hybrid_Neural_Representation_for_Videos_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_HNeRV_A_Hybrid_Neural_Representation_for_Videos_CVPR_2023_paper.pdf">HNeRV: A Hybrid Neural Representation for Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Good_Is_Bad_Causality_Inspired_Cloth-Debiasing_for_Cloth-Changing_Person_Re-Identification_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Good_Is_Bad_Causality_Inspired_Cloth-Debiasing_for_Cloth-Changing_Person_Re-Identification_CVPR_2023_paper.pdf">Good is Bad: Causality Inspired Cloth-debiasing for Cloth-changing Person<br />Re-identification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_CVT-SLR_Contrastive_Visual-Textual_Transformation_for_Sign_Language_Recognition_With_Variational_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_CVT-SLR_Contrastive_Visual-Textual_Transformation_for_Sign_Language_Recognition_With_Variational_CVPR_2023_paper.pdf">CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with<br />Variational Alignment</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sarlin_OrienterNet_Visual_Localization_in_2D_Public_Maps_With_Neural_Matching_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sarlin_OrienterNet_Visual_Localization_in_2D_Public_Maps_With_Neural_Matching_CVPR_2023_paper.pdf">OrienterNet: Visual Localization in 2D Public Maps with Neural<br />Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Dynamic_Graph_Learning_With_Content-Guided_Spatial-Frequency_Relation_Reasoning_for_Deepfake_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Dynamic_Graph_Learning_With_Content-Guided_Spatial-Frequency_Relation_Reasoning_for_Deepfake_CVPR_2023_paper.pdf">Dynamic Graph Learning with Content-guided Spatial-Frequency Relation Reasoning for<br />Deepfake Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-01-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Vidit_CLIP_the_Gap_A_Single_Domain_Generalization_Approach_for_Object_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Vidit_CLIP_the_Gap_A_Single_Domain_Generalization_Approach_for_Object_CVPR_2023_paper.pdf">CLIP the Gap: A Single Domain Generalization Approach for<br />Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">49</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Boundary_Unlearning_Rapid_Forgetting_of_Deep_Networks_via_Shifting_the_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Boundary_Unlearning_Rapid_Forgetting_of_Deep_Networks_via_Shifting_the_CVPR_2023_paper.pdf">Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting<br />the Decision Boundary</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-01-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wei_LEGO-Net_Learning_Regular_Rearrangements_of_Objects_in_Rooms_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_LEGO-Net_Learning_Regular_Rearrangements_of_Objects_in_Rooms_CVPR_2023_paper.pdf">LEGO-Net: Learning Regular Rearrangements of Objects in Rooms</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_MetaFusion_Infrared_and_Visible_Image_Fusion_via_Meta-Feature_Embedding_From_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_MetaFusion_Infrared_and_Visible_Image_Fusion_via_Meta-Feature_Embedding_From_CVPR_2023_paper.pdf">MetaFusion: Infrared and Visible Image Fusion via Meta-Feature Embedding<br />from Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-05-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Low-Light_Image_Enhancement_via_Structure_Modeling_and_Guidance_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Low-Light_Image_Enhancement_via_Structure_Modeling_and_Guidance_CVPR_2023_paper.pdf">Low-Light Image Enhancement via Structure Modeling and Guidance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2022-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ci_GFPose_Learning_3D_Human_Pose_Prior_With_Gradient_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ci_GFPose_Learning_3D_Human_Pose_Prior_With_Gradient_Fields_CVPR_2023_paper.pdf">GFPose: Learning 3D Human Pose Prior with Gradient Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_MSeg3D_Multi-Modal_3D_Semantic_Segmentation_for_Autonomous_Driving_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_MSeg3D_Multi-Modal_3D_Semantic_Segmentation_for_Autonomous_Driving_CVPR_2023_paper.pdf">MSeg3D: Multi-Modal 3D Semantic Segmentation for Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Piergiovanni_Rethinking_Video_ViTs_Sparse_Video_Tubes_for_Joint_Image_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Piergiovanni_Rethinking_Video_ViTs_Sparse_Video_Tubes_for_Joint_Image_and_CVPR_2023_paper.pdf">Rethinking Video ViTs: Sparse Video Tubes for Joint Image<br />and Video Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-01-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Self-Supervised_Video_Forensics_by_Audio-Visual_Anomaly_Detection_CVPR_2023_paper.pdf">Self-Supervised Video Forensics by Audio-Visual Anomaly Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2023-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Constructing_Deep_Spiking_Neural_Networks_From_Artificial_Neural_Networks_With_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Constructing_Deep_Spiking_Neural_Networks_From_Artificial_Neural_Networks_With_CVPR_2023_paper.pdf">Constructing Deep Spiking Neural Networks from Artificial Neural Networks<br />with Knowledge Distillation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_BAD-NeRF_Bundle_Adjusted_Deblur_Neural_Radiance_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_BAD-NeRF_Bundle_Adjusted_Deblur_Neural_Radiance_Fields_CVPR_2023_paper.pdf">BAD-NeRF: Bundle Adjusted Deblur Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">48</td>
<td style="text-align: center;">2022-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_PD-Quant_Post-Training_Quantization_Based_on_Prediction_Difference_Metric_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_PD-Quant_Post-Training_Quantization_Based_on_Prediction_Difference_Metric_CVPR_2023_paper.pdf">PD-Quant: Post-Training Quantization Based on Prediction Difference Metric</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Local-to-Global_Registration_for_Bundle-Adjusting_Neural_Radiance_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Local-to-Global_Registration_for_Bundle-Adjusting_Neural_Radiance_Fields_CVPR_2023_paper.pdf">Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Neural_Volumetric_Memory_for_Visual_Locomotion_Control_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Neural_Volumetric_Memory_for_Visual_Locomotion_Control_CVPR_2023_paper.pdf">Neural Volumetric Memory for Visual Locomotion Control</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2023-05-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_PillarNeXt_Rethinking_Network_Designs_for_3D_Object_Detection_in_LiDAR_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_PillarNeXt_Rethinking_Network_Designs_for_3D_Object_Detection_in_LiDAR_CVPR_2023_paper.pdf">PillarNeXt: Rethinking Network Designs for 3D Object Detection in<br />LiDAR Point Clouds</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2022-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Prototypical_Residual_Networks_for_Anomaly_Detection_and_Localization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Prototypical_Residual_Networks_for_Anomaly_Detection_and_Localization_CVPR_2023_paper.pdf">Prototypical Residual Networks for Anomaly Detection and Localization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Decoupled_Multimodal_Distilling_for_Emotion_Recognition_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Decoupled_Multimodal_Distilling_for_Emotion_Recognition_CVPR_2023_paper.pdf">Decoupled Multimodal Distilling for Emotion Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2023-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Task-Specific_Fine-Tuning_via_Variational_Information_Bottleneck_for_Weakly-Supervised_Pathology_Whole_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Task-Specific_Fine-Tuning_via_Variational_Information_Bottleneck_for_Weakly-Supervised_Pathology_Whole_CVPR_2023_paper.pdf">Task-Specific Fine-Tuning via Variational Information Bottleneck for Weakly-Supervised Pathology<br />Whole Slide Image Classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2022-12-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Weder_Removing_Objects_From_Neural_Radiance_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Weder_Removing_Objects_From_Neural_Radiance_Fields_CVPR_2023_paper.pdf">Removing Objects From Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">47</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ren_VolRecon_Volume_Rendering_of_Signed_Ray_Distance_Functions_for_Generalizable_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ren_VolRecon_Volume_Rendering_of_Signed_Ray_Distance_Functions_for_Generalizable_CVPR_2023_paper.pdf">VolRecon: Volume Rendering of Signed Ray Distance Functions for<br />Generalizable Multi-View Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Karim_C-SFDA_A_Curriculum_Learning_Aided_Self-Training_Framework_for_Efficient_Source_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Karim_C-SFDA_A_Curriculum_Learning_Aided_Self-Training_Framework_for_Efficient_Source_CVPR_2023_paper.pdf">C-SFDA: A Curriculum Learning Aided Self-Training Framework for Efficient<br />Source Free Domain Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2022-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Progressive_Disentangled_Representation_Learning_for_Fine-Grained_Controllable_Talking_Head_Synthesis_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Progressive_Disentangled_Representation_Learning_for_Fine-Grained_Controllable_Talking_Head_Synthesis_CVPR_2023_paper.pdf">Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head<br />Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2022-12-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_PromptCAL_Contrastive_Affinity_Learning_via_Auxiliary_Prompts_for_Generalized_Novel_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PromptCAL_Contrastive_Affinity_Learning_via_Auxiliary_Prompts_for_Generalized_Novel_CVPR_2023_paper.pdf">PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for Generalized<br />Novel Category Discovery</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shabani_HouseDiffusion_Vector_Floorplan_Generation_via_a_Diffusion_Model_With_Discrete_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shabani_HouseDiffusion_Vector_Floorplan_Generation_via_a_Diffusion_Model_With_Discrete_CVPR_2023_paper.pdf">HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with<br />Discrete and Continuous Denoising</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-01-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ando_RangeViT_Towards_Vision_Transformers_for_3D_Semantic_Segmentation_in_Autonomous_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ando_RangeViT_Towards_Vision_Transformers_for_3D_Semantic_Segmentation_in_Autonomous_CVPR_2023_paper.pdf">RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in<br />Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Otani_Toward_Verifiable_and_Reproducible_Human_Evaluation_for_Text-to-Image_Generation_CVPR_2023_paper.pdf">Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Prabhu_Computationally_Budgeted_Continual_Learning_What_Does_Matter_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Prabhu_Computationally_Budgeted_Continual_Learning_What_Does_Matter_CVPR_2023_paper.pdf">Computationally Budgeted Continual Learning: What Does Matter?</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-04-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wei_Joint_Token_Pruning_and_Squeezing_Towards_More_Aggressive_Compression_of_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_Joint_Token_Pruning_and_Squeezing_Towards_More_Aggressive_Compression_of_CVPR_2023_paper.pdf">Joint Token Pruning and Squeezing Towards More Aggressive Compression<br />of Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2022-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zohar_PROB_Probabilistic_Objectness_for_Open_World_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zohar_PROB_Probabilistic_Objectness_for_Open_World_Object_Detection_CVPR_2023_paper.pdf">PROB: Probabilistic Objectness for Open World Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_NeRF-RPN_A_General_Framework_for_Object_Detection_in_NeRFs_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_NeRF-RPN_A_General_Framework_for_Object_Detection_in_NeRFs_CVPR_2023_paper.pdf">NeRF-RPN: A general framework for object detection in NeRFs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_KiUT_Knowledge-Injected_U-Transformer_for_Radiology_Report_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_KiUT_Knowledge-Injected_U-Transformer_for_Radiology_Report_Generation_CVPR_2023_paper.pdf">KiUT: Knowledge-injected U-Transformer for Radiology Report Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">46</td>
<td style="text-align: center;">2023-02-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lei_A_Hierarchical_Representation_Network_for_Accurate_and_Detailed_Face_Reconstruction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lei_A_Hierarchical_Representation_Network_for_Accurate_and_Detailed_Face_Reconstruction_CVPR_2023_paper.pdf">A Hierarchical Representation Network for Accurate and Detailed Face<br />Reconstruction from In-The-Wild Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_AltFreezing_for_More_General_Video_Face_Forgery_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_AltFreezing_for_More_General_Video_Face_Forgery_Detection_CVPR_2023_paper.pdf">AltFreezing for More General Video Face Forgery Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_MP-Former_Mask-Piloted_Transformer_for_Image_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MP-Former_Mask-Piloted_Transformer_for_Image_Segmentation_CVPR_2023_paper.pdf">MP-Former: Mask-Piloted Transformer for Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2022-12-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Boulch_ALSO_Automotive_Lidar_Self-Supervision_by_Occupancy_Estimation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Boulch_ALSO_Automotive_Lidar_Self-Supervision_by_Occupancy_Estimation_CVPR_2023_paper.pdf">ALSO: Automotive Lidar Self-Supervision by Occupancy Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Mod-Squad_Designing_Mixtures_of_Experts_As_Modular_Multi-Task_Learners_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Mod-Squad_Designing_Mixtures_of_Experts_As_Modular_Multi-Task_Learners_CVPR_2023_paper.pdf">Mod-Squad: Designing Mixtures of Experts As Modular Multi-Task Learners</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2023-04-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_One-Shot_High-Fidelity_Talking-Head_Synthesis_With_Deformable_Neural_Radiance_Field_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_One-Shot_High-Fidelity_Talking-Head_Synthesis_With_Deformable_Neural_Radiance_Field_CVPR_2023_paper.pdf">One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2023-03-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kwon_Renderable_Neural_Radiance_Map_for_Visual_Navigation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kwon_Renderable_Neural_Radiance_Map_for_Visual_Navigation_CVPR_2023_paper.pdf">Renderable Neural Radiance Map for Visual Navigation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2023-03-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_You_Only_Segment_Once_Towards_Real-Time_Panoptic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_You_Only_Segment_Once_Towards_Real-Time_Panoptic_Segmentation_CVPR_2023_paper.pdf">You Only Segment Once: Towards Real-Time Panoptic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Learning_Weather-General_and_Weather-Specific_Features_for_Image_Restoration_Under_Multiple_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Learning_Weather-General_and_Weather-Specific_Features_for_Image_Restoration_Under_Multiple_CVPR_2023_paper.pdf">Learning Weather-General and Weather-Specific Features for Image Restoration Under<br />Multiple Adverse Weather Conditions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">45</td>
<td style="text-align: center;">2023-03-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_StyleRF_Zero-Shot_3D_Style_Transfer_of_Neural_Radiance_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_StyleRF_Zero-Shot_3D_Style_Transfer_of_Neural_Radiance_Fields_CVPR_2023_paper.pdf">StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.pdf">3D Human Pose Estimation with Spatio-Temporal Criss-Cross Attention</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Collaboration_Helps_Camera_Overtake_LiDAR_in_3D_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Collaboration_Helps_Camera_Overtake_LiDAR_in_3D_Detection_CVPR_2023_paper.pdf">Collaboration Helps Camera Overtake LiDAR in 3D Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2022-07-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yao_Explicit_Boundary_Guided_Semi-Push-Pull_Contrastive_Learning_for_Supervised_Anomaly_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yao_Explicit_Boundary_Guided_Semi-Push-Pull_Contrastive_Learning_for_Supervised_Anomaly_Detection_CVPR_2023_paper.pdf">Explicit Boundary Guided Semi-Push-Pull Contrastive Learning for Supervised Anomaly<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-05-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Brachmann_Accelerated_Coordinate_Encoding_Learning_to_Relocalize_in_Minutes_Using_RGB_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Brachmann_Accelerated_Coordinate_Encoding_Learning_to_Relocalize_in_Minutes_Using_RGB_CVPR_2023_paper.pdf">Accelerated Coordinate Encoding: Learning to Relocalize in Minutes Using<br />RGB and Poses</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Dong_DisWOT_Student_Architecture_Search_for_Distillation_WithOut_Training_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_DisWOT_Student_Architecture_Search_for_Distillation_WithOut_Training_CVPR_2023_paper.pdf">DisWOT: Student Architecture Search for Distillation WithOut Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-01-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ramrakhya_PIRLNav_Pretraining_With_Imitation_and_RL_Finetuning_for_ObjectNav_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ramrakhya_PIRLNav_Pretraining_With_Imitation_and_RL_Finetuning_for_ObjectNav_CVPR_2023_paper.pdf">PIRLNav: Pretraining with Imitation and RL Finetuning for OBJECTNAV</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Label-Free_Liver_Tumor_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Label-Free_Liver_Tumor_Segmentation_CVPR_2023_paper.pdf">Label-Free Liver Tumor Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Garcia_Uncurated_Image-Text_Datasets_Shedding_Light_on_Demographic_Bias_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Garcia_Uncurated_Image-Text_Datasets_Shedding_Light_on_Demographic_Bias_CVPR_2023_paper.pdf">Uncurated Image-Text Datasets: Shedding Light on Demographic Bias</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Simeoni_Unsupervised_Object_Localization_Observing_the_Background_To_Discover_Objects_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Simeoni_Unsupervised_Object_Localization_Observing_the_Background_To_Discover_Objects_CVPR_2023_paper.pdf">Unsupervised Object Localization: Observing the Background to Discover Objects</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Pu_Dynamic_Conceptional_Contrastive_Learning_for_Generalized_Category_Discovery_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pu_Dynamic_Conceptional_Contrastive_Learning_for_Generalized_Category_Discovery_CVPR_2023_paper.pdf">Dynamic Conceptional Contrastive Learning for Generalized Category Discovery</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-03-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ma_OTAvatar_One-Shot_Talking_Face_Avatar_With_Controllable_Tri-Plane_Rendering_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_OTAvatar_One-Shot_Talking_Face_Avatar_With_Controllable_Tri-Plane_Rendering_CVPR_2023_paper.pdf">OTAvatar: One-Shot Talking Face Avatar with Controllable Tri-Plane Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Federated_Domain_Generalization_With_Generalization_Adjustment_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Federated_Domain_Generalization_With_Generalization_Adjustment_CVPR_2023_paper.pdf">Federated Domain Generalization with Generalization Adjustment</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">2022-11-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cai_MARLIN_Masked_Autoencoder_for_Facial_Video_Representation_LearnINg_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cai_MARLIN_Masked_Autoencoder_for_Facial_Video_Representation_LearnINg_CVPR_2023_paper.pdf">MARLIN: Masked Autoencoder for facial video Representation LearnINg</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_MetaPortrait_Identity-Preserving_Talking_Head_Generation_With_Fast_Personalized_Adaptation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_MetaPortrait_Identity-Preserving_Talking_Head_Generation_With_Fast_Personalized_Adaptation_CVPR_2023_paper.pdf">MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2023-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ying_Mapping_Degeneration_Meets_Label_Evolution_Learning_Infrared_Small_Target_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ying_Mapping_Degeneration_Meets_Label_Evolution_Learning_Infrared_Small_Target_Detection_CVPR_2023_paper.pdf">Mapping Degeneration Meets Label Evolution: Learning Infrared Small Target<br />Detection with Single Point Supervision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2022-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_UDE_A_Unified_Driving_Engine_for_Human_Motion_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_UDE_A_Unified_Driving_Engine_for_Human_Motion_Generation_CVPR_2023_paper.pdf">UDE: A Unified Driving Engine for Human Motion Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cetintas_Unifying_Short_and_Long-Term_Tracking_With_Graph_Hierarchies_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cetintas_Unifying_Short_and_Long-Term_Tracking_With_Graph_Hierarchies_CVPR_2023_paper.pdf">Unifying Short and Long-Term Tracking with Graph Hierarchies</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2023-03-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Oh_BlackVIP_Black-Box_Visual_Prompting_for_Robust_Transfer_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Oh_BlackVIP_Black-Box_Visual_Prompting_for_Robust_Transfer_Learning_CVPR_2023_paper.pdf">BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Diversity-Aware_Meta_Visual_Prompting_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Diversity-Aware_Meta_Visual_Prompting_CVPR_2023_paper.pdf">Diversity-Aware Meta Visual Prompting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/He_Align_and_Attend_Multimodal_Summarization_With_Dual_Contrastive_Losses_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/He_Align_and_Attend_Multimodal_Summarization_With_Dual_Contrastive_Losses_CVPR_2023_paper.pdf">Align and Attend: Multimodal Summarization with Dual Contrastive Losses</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Guo_Texts_as_Images_in_Prompt_Tuning_for_Multi-Label_Image_Recognition_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_Texts_as_Images_in_Prompt_Tuning_for_Multi-Label_Image_Recognition_CVPR_2023_paper.pdf">Texts as Images in Prompt Tuning for Multi-Label Image<br />Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2022-06-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xie_On_Data_Scaling_in_Masked_Image_Modeling_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_On_Data_Scaling_in_Masked_Image_Modeling_CVPR_2023_paper.pdf">On Data Scaling in Masked Image Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2022-07-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Clover_Towards_a_Unified_Video-Language_Alignment_and_Fusion_Model_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Clover_Towards_a_Unified_Video-Language_Alignment_and_Fusion_Model_CVPR_2023_paper.pdf">Clover: Towards A Unified Video-Language Alignment and Fusion Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Augmentation_Matters_A_Simple-Yet-Effective_Approach_to_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Augmentation_Matters_A_Simple-Yet-Effective_Approach_to_Semi-Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">Augmentation Matters: A Simple-Yet-Effective Approach to Semi-Supervised Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2023-05-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Neural_Kernel_Surface_Reconstruction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Neural_Kernel_Surface_Reconstruction_CVPR_2023_paper.pdf">Neural Kernel Surface Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Spectral_Enhanced_Rectangle_Transformer_for_Hyperspectral_Image_Denoising_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Spectral_Enhanced_Rectangle_Transformer_for_Hyperspectral_Image_Denoising_CVPR_2023_paper.pdf">Spectral Enhanced Rectangle Transformer for Hyperspectral Image Denoising</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2022-08-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kong_Understanding_Masked_Image_Modeling_via_Learning_Occlusion_Invariant_Feature_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kong_Understanding_Masked_Image_Modeling_via_Learning_Occlusion_Invariant_Feature_CVPR_2023_paper.pdf">Understanding Masked Image Modeling via Learning Occlusion Invariant Feature</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Peng_Representing_Volumetric_Videos_As_Dynamic_MLP_Maps_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Peng_Representing_Volumetric_Videos_As_Dynamic_MLP_Maps_CVPR_2023_paper.pdf">Representing Volumetric Videos as Dynamic MLP Maps</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2022-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lee_DP-NeRF_Deblurred_Neural_Radiance_Field_With_Physical_Scene_Priors_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_DP-NeRF_Deblurred_Neural_Radiance_Field_With_Physical_Scene_Priors_CVPR_2023_paper.pdf">DP-NeRF: Deblurred Neural Radiance Field with Physical Scene Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2022-11-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sanghi_CLIP-Sculptor_Zero-Shot_Generation_of_High-Fidelity_and_Diverse_Shapes_From_Natural_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sanghi_CLIP-Sculptor_Zero-Shot_Generation_of_High-Fidelity_and_Diverse_Shapes_From_Natural_CVPR_2023_paper.pdf">CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from<br />Natural Language</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-01-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Abdal_3DAvatarGAN_Bridging_Domains_for_Personalized_Editable_Avatars_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Abdal_3DAvatarGAN_Bridging_Domains_for_Personalized_Editable_Avatars_CVPR_2023_paper.pdf">3DAvatarGAN: Bridging Domains for Personalized Editable Avatars</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-03-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Litrico_Guiding_Pseudo-Labels_With_Uncertainty_Estimation_for_Source-Free_Unsupervised_Domain_Adaptation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Litrico_Guiding_Pseudo-Labels_With_Uncertainty_Estimation_for_Source-Free_Unsupervised_Domain_Adaptation_CVPR_2023_paper.pdf">Guiding Pseudo-labels with Uncertainty Estimation for Source-free Unsupervised Domain<br />Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-05-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_NIKI_Neural_Inverse_Kinematics_With_Invertible_Neural_Networks_for_3D_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_NIKI_Neural_Inverse_Kinematics_With_Invertible_Neural_Networks_for_3D_CVPR_2023_paper.pdf">NIKI: Neural Inverse Kinematics with Invertible Neural Networks for<br />3D Human Pose and Shape Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2022-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yin_3D_GAN_Inversion_With_Facial_Symmetry_Prior_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_3D_GAN_Inversion_With_Facial_Symmetry_Prior_CVPR_2023_paper.pdf">3D GAN Inversion with Facial Symmetry Prior</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-03-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lei_PyramidFlow_High-Resolution_Defect_Contrastive_Localization_Using_Pyramid_Normalizing_Flow_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lei_PyramidFlow_High-Resolution_Defect_Contrastive_Localization_Using_Pyramid_Normalizing_Flow_CVPR_2023_paper.pdf">PyramidFlow: High-Resolution Defect Contrastive Localization Using Pyramid Normalizing Flow</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cao_Real-Time_Neural_Light_Field_on_Mobile_Devices_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Real-Time_Neural_Light_Field_on_Mobile_Devices_CVPR_2023_paper.pdf">Real-Time Neural Light Field on Mobile Devices</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Luo_Semantic-Conditional_Diffusion_Networks_for_Image_Captioning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Semantic-Conditional_Diffusion_Networks_for_Image_Captioning_CVPR_2023_paper.pdf">Semantic-Conditional Diffusion Networks for Image Captioning*</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-04-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Video_Event_Restoration_Based_on_Keyframes_for_Video_Anomaly_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Video_Event_Restoration_Based_on_Keyframes_for_Video_Anomaly_Detection_CVPR_2023_paper.pdf">Video Event Restoration Based on Keyframes for Video Anomaly<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_DISC_Learning_From_Noisy_Labels_via_Dynamic_Instance-Specific_Selection_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_DISC_Learning_From_Noisy_Labels_via_Dynamic_Instance-Specific_Selection_and_CVPR_2023_paper.pdf">DISC: Learning from Noisy Labels via Dynamic Instance-Specific Selection<br />and Correction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Diversity-Measurable_Anomaly_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Diversity-Measurable_Anomaly_Detection_CVPR_2023_paper.pdf">Diversity-Measurable Anomaly Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mall_Change-Aware_Sampling_and_Contrastive_Learning_for_Satellite_Images_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mall_Change-Aware_Sampling_and_Contrastive_Learning_for_Satellite_Images_CVPR_2023_paper.pdf">Change-Aware Sampling and Contrastive Learning for Satellite Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">42</td>
<td style="text-align: center;">2022-12-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Goel_Interactive_Segmentation_of_Radiance_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Goel_Interactive_Segmentation_of_Radiance_Fields_CVPR_2023_paper.pdf">Interactive Segmentation of Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-01-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lee_Shape-Aware_Text-Driven_Layered_Video_Editing_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lee_Shape-Aware_Text-Driven_Layered_Video_Editing_CVPR_2023_paper.pdf">Shape-Aware Text-Driven Layered Video Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_Ego-Body_Pose_Estimation_via_Ego-Head_Pose_Estimation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Ego-Body_Pose_Estimation_via_Ego-Head_Pose_Estimation_CVPR_2023_paper.pdf">Ego-Body Pose Estimation via Ego-Head Pose Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Pautrat_DeepLSD_Line_Segment_Detection_and_Refinement_With_Deep_Image_Gradients_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pautrat_DeepLSD_Line_Segment_Detection_and_Refinement_With_Deep_Image_Gradients_CVPR_2023_paper.pdf">DeepLSD: Line Segment Detection and Refinement with Deep Image<br />Gradients</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-04-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ding_DiffusionRig_Learning_Personalized_Priors_for_Facial_Appearance_Editing_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ding_DiffusionRig_Learning_Personalized_Priors_for_Facial_Appearance_Editing_CVPR_2023_paper.pdf">DiffusionRig: Learning Personalized Priors for Facial Appearance Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jin_Video-Text_As_Game_Players_Hierarchical_Banzhaf_Interaction_for_Cross-Modal_Representation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Video-Text_As_Game_Players_Hierarchical_Banzhaf_Interaction_for_Cross-Modal_Representation_CVPR_2023_paper.pdf">Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal<br />Representation Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Park_All-in-One_Image_Restoration_for_Unknown_Degradations_Using_Adaptive_Discriminative_Filters_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_All-in-One_Image_Restoration_for_Unknown_Degradations_Using_Adaptive_Discriminative_Filters_CVPR_2023_paper.pdf">All-in-One Image Restoration for Unknown Degradations Using Adaptive Discriminative<br />Filters for Specific Degradations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xiong_Neural_Map_Prior_for_Autonomous_Driving_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Neural_Map_Prior_for_Autonomous_Driving_CVPR_2023_paper.pdf">Neural Map Prior for Autonomous Driving</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_CVPR_2023_paper.pdf">TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras<br />in 3D Environments</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mansour_Zero-Shot_Noise2Noise_Efficient_Image_Denoising_Without_Any_Data_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mansour_Zero-Shot_Noise2Noise_Efficient_Image_Denoising_Without_Any_Data_CVPR_2023_paper.pdf">Zero-Shot Noise2Noise: Efficient Image Denoising without any Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lv_Unbiased_Multiple_Instance_Learning_for_Weakly_Supervised_Video_Anomaly_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lv_Unbiased_Multiple_Instance_Learning_for_Weakly_Supervised_Video_Anomaly_Detection_CVPR_2023_paper.pdf">Unbiased Multiple Instance Learning for Weakly Supervised Video Anomaly<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2023-03-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Nerflets_Local_Radiance_Fields_for_Efficient_Structure-Aware_3D_Scene_Representation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Nerflets_Local_Radiance_Fields_for_Efficient_Structure-Aware_3D_Scene_Representation_CVPR_2023_paper.pdf">Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene<br />Representation from 2D Supervision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">41</td>
<td style="text-align: center;">2022-12-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Bidirectional_Cross-Modal_Knowledge_Exploration_for_Video_Recognition_With_Pre-Trained_Vision-Language_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Bidirectional_Cross-Modal_Knowledge_Exploration_for_Video_Recognition_With_Pre-Trained_Vision-Language_CVPR_2023_paper.pdf">Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained<br />Vision-Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Vision_Transformer_With_Super_Token_Sampling_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Vision_Transformer_With_Super_Token_Sampling_CVPR_2023_paper.pdf">Vision Transformer with Super Token Sampling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2022-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ge_Hyperbolic_Contrastive_Learning_for_Visual_Representations_Beyond_Objects_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ge_Hyperbolic_Contrastive_Learning_for_Visual_Representations_Beyond_Objects_CVPR_2023_paper.pdf">Hyperbolic Contrastive Learning for Visual Representations beyond Objects</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2023-03-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ju_Human-Art_A_Versatile_Human-Centric_Dataset_Bridging_Natural_and_Artificial_Scenes_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ju_Human-Art_A_Versatile_Human-Centric_Dataset_Bridging_Natural_and_Artificial_Scenes_CVPR_2023_paper.pdf">Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial<br />Scenes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2023-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Open-Vocabulary_Point-Cloud_Object_Detection_Without_3D_Annotation_CVPR_2023_paper.pdf">Open-Vocabulary Point-Cloud Object Detection without 3D Annotation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Exploiting_Completeness_and_Uncertainty_of_Pseudo_Labels_for_Weakly_Supervised_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Exploiting_Completeness_and_Uncertainty_of_Pseudo_Labels_for_Weakly_Supervised_CVPR_2023_paper.pdf">Exploiting Completeness and Uncertainty of Pseudo Labels for Weakly<br />Supervised Video Anomaly Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2022-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Grigorev_HOOD_Hierarchical_Graphs_for_Generalized_Modelling_of_Clothing_Dynamics_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Grigorev_HOOD_Hierarchical_Graphs_for_Generalized_Modelling_of_Clothing_Dynamics_CVPR_2023_paper.pdf">HOOD: Hierarchical Graphs for Generalized Modelling of Clothing Dynamics</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_DBARF_Deep_Bundle-Adjusting_Generalizable_Neural_Radiance_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_DBARF_Deep_Bundle-Adjusting_Generalizable_Neural_Radiance_Fields_CVPR_2023_paper.pdf">DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2022-06-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Raab_MoDi_Unconditional_Motion_Synthesis_From_Diverse_Data_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Raab_MoDi_Unconditional_Motion_Synthesis_From_Diverse_Data_CVPR_2023_paper.pdf">MoDi: Unconditional Motion Synthesis from Diverse Data</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2022-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Fine-Grained_Face_Swapping_via_Regional_GAN_Inversion_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Fine-Grained_Face_Swapping_via_Regional_GAN_Inversion_CVPR_2023_paper.pdf">Fine-Grained Face Swapping Via Regional GAN Inversion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">2023-03-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Seth_DeAR_Debiasing_Vision-Language_Models_With_Additive_Residuals_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Seth_DeAR_Debiasing_Vision-Language_Models_With_Additive_Residuals_CVPR_2023_paper.pdf">DeAR: Debiasing Vision-Language Models with Additive Residuals</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2022-12-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Pena_Re-Basin_via_Implicit_Sinkhorn_Differentiation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pena_Re-Basin_via_Implicit_Sinkhorn_Differentiation_CVPR_2023_paper.pdf">Re-basin via implicit Sinkhorn differentiation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hong_3D_Concept_Learning_and_Reasoning_From_Multi-View_Images_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hong_3D_Concept_Learning_and_Reasoning_From_Multi-View_Images_CVPR_2023_paper.pdf">3D Concept Learning and Reasoning from Multi-View Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gao_Backdoor_Defense_via_Adaptively_Splitting_Poisoned_Dataset_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Backdoor_Defense_via_Adaptively_Splitting_Poisoned_Dataset_CVPR_2023_paper.pdf">Backdoor Defense via Adaptively Splitting Poisoned Dataset</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2023-04-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Levy_SeaThru-NeRF_Neural_Radiance_Fields_in_Scattering_Media_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Levy_SeaThru-NeRF_Neural_Radiance_Fields_in_Scattering_Media_CVPR_2023_paper.pdf">SeaThru-NeRF: Neural Radiance Fields in Scattering Media</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2023-02-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Turning_a_CLIP_Model_Into_a_Scene_Text_Detector_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Turning_a_CLIP_Model_Into_a_Scene_Text_Detector_CVPR_2023_paper.pdf">Turning a CLIP Model into a Scene Text Detector</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2022-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kuang_PaletteNeRF_Palette-Based_Appearance_Editing_of_Neural_Radiance_Fields_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kuang_PaletteNeRF_Palette-Based_Appearance_Editing_of_Neural_Radiance_Fields_CVPR_2023_paper.pdf">PaletteNeRF: Palette-based Appearance Editing of Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">39</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Rong_Boundary-Enhanced_Co-Training_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Rong_Boundary-Enhanced_Co-Training_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf">Boundary-enhanced Co-training for Weakly Supervised Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-03-31</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Zero-Shot_Referring_Image_Segmentation_With_Global-Local_Context_Features_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Zero-Shot_Referring_Image_Segmentation_With_Global-Local_Context_Features_CVPR_2023_paper.pdf">Zero-shot Referring Image Segmentation with Global-Local Context Features</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-02-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Park_Temporal_Interpolation_Is_All_You_Need_for_Dynamic_Neural_Radiance_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Temporal_Interpolation_Is_All_You_Need_for_Dynamic_Neural_Radiance_CVPR_2023_paper.pdf">Temporal Interpolation is all You Need for Dynamic Neural<br />Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tosi_NeRF-Supervised_Deep_Stereo_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tosi_NeRF-Supervised_Deep_Stereo_CVPR_2023_paper.pdf">NeRF-Supervised Deep Stereo</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jin_Multi-Level_Logit_Distillation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Multi-Level_Logit_Distillation_CVPR_2023_paper.pdf">Multi-Level Logit Distillation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-05-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chai_LayoutDM_Transformer-Based_Diffusion_Model_for_Layout_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chai_LayoutDM_Transformer-Based_Diffusion_Model_for_Layout_Generation_CVPR_2023_paper.pdf">LayoutDM: Transformer-based Diffusion Model for Layout Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Mixed_Autoencoder_for_Self-Supervised_Visual_Representation_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Mixed_Autoencoder_for_Self-Supervised_Visual_Representation_Learning_CVPR_2023_paper.pdf">Mixed Autoencoder for Self-Supervised Visual Representation Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Masked_Image_Training_for_Generalizable_Deep_Image_Denoising_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Masked_Image_Training_for_Generalizable_Deep_Image_Denoising_CVPR_2023_paper.pdf">Masked Image Training for Generalizable Deep Image Denoising</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2022-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_UV_Volumes_for_Real-Time_Rendering_of_Editable_Free-View_Human_Performance_CVPR_2023_paper.pdf">UV Volumes for Real-time Rendering of Editable Free-view Human<br />Performance</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-05-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Guan_StyleSync_High-Fidelity_Generalized_and_Personalized_Lip_Sync_in_Style-Based_Generator_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Guan_StyleSync_High-Fidelity_Generalized_and_Personalized_Lip_Sync_in_Style-Based_Generator_CVPR_2023_paper.pdf">StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-Based<br />Generator</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-04-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Neural_Residual_Radiance_Fields_for_Streamably_Free-Viewpoint_Videos_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Neural_Residual_Radiance_Fields_for_Streamably_Free-Viewpoint_Videos_CVPR_2023_paper.pdf">Neural Residual Radiance Fields for Streamably Free-Viewpoint Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2022-05-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_PointDistiller_Structured_Knowledge_Distillation_Towards_Efficient_and_Compact_3D_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PointDistiller_Structured_Knowledge_Distillation_Towards_Efficient_and_Compact_3D_Detection_CVPR_2023_paper.pdf">PointDistiller: Structured Knowledge Distillation Towards Efficient and Compact 3D<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yi_MIME_Human-Aware_3D_Scene_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yi_MIME_Human-Aware_3D_Scene_Generation_CVPR_2023_paper.pdf">MIME: Human-Aware 3D Scene Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Continual_Detection_Transformer_for_Incremental_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Continual_Detection_Transformer_for_Incremental_Object_Detection_CVPR_2023_paper.pdf">Continual Detection Transformer for Incremental Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Hard_Patches_Mining_for_Masked_Image_Modeling_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Hard_Patches_Mining_for_Masked_Image_Modeling_CVPR_2023_paper.pdf">Hard Patches Mining for Masked Image Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Slimmable_Dataset_Condensation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Slimmable_Dataset_Condensation_CVPR_2023_paper.pdf">Slimmable Dataset Condensation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wei_CFA_Class-Wise_Calibrated_Fair_Adversarial_Training_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_CFA_Class-Wise_Calibrated_Fair_Adversarial_Training_CVPR_2023_paper.pdf">CFA: Class-Wise Calibrated Fair Adversarial Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">38</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shi_Make_Landscape_Flatter_in_Differentially_Private_Federated_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_Make_Landscape_Flatter_in_Differentially_Private_Federated_Learning_CVPR_2023_paper.pdf">Make Landscape Flatter in Differentially Private Federated Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-01-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_NeRF_in_the_Palm_of_Your_Hand_Corrective_Augmentation_for_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_NeRF_in_the_Palm_of_Your_Hand_Corrective_Augmentation_for_CVPR_2023_paper.pdf">NeRF in the Palm of Your Hand: Corrective Augmentation<br />for Robotics via Novel-View Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-02-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Pang_Standing_Between_Past_and_Future_Spatio-Temporal_Modeling_for_Multi-Camera_3D_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pang_Standing_Between_Past_and_Future_Spatio-Temporal_Modeling_for_Multi-Camera_3D_CVPR_2023_paper.pdf">Standing Between Past and Future: Spatio-Temporal Modeling for Multi-Camera<br />3D Multi-Object Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf">Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models<br />to Learn Any Unseen Style</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2022-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Villa_PIVOT_Prompting_for_Video_Continual_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Villa_PIVOT_Prompting_for_Video_Continual_Learning_CVPR_2023_paper.pdf">PIVOT: Prompting for Video Continual Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2022-09-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Consistent-Teacher_Towards_Reducing_Inconsistent_Pseudo-Targets_in_Semi-Supervised_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Consistent-Teacher_Towards_Reducing_Inconsistent_Pseudo-Targets_in_Semi-Supervised_Object_Detection_CVPR_2023_paper.pdf">Consistent-Teacher: Towards Reducing Inconsistent Pseudo-Targets in Semi-Supervised Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Han_Clothing-Change_Feature_Augmentation_for_Person_Re-Identification_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Han_Clothing-Change_Feature_Augmentation_for_Person_Re-Identification_CVPR_2023_paper.pdf">Clothing-Change Feature Augmentation for Person Re-Identiﬁcation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2022-02-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fel_Dont_Lie_to_Me_Robust_and_Efficient_Explainability_With_Verified_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fel_Dont_Lie_to_Me_Robust_and_Efficient_Explainability_With_Verified_CVPR_2023_paper.pdf">Don't Lie to Me! Robust and Efficient Explainability with<br />Verified Perturbation Analysis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-04-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_NeUDF_Leaning_Neural_Unsigned_Distance_Fields_With_Volume_Rendering_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_NeUDF_Leaning_Neural_Unsigned_Distance_Fields_With_Volume_Rendering_CVPR_2023_paper.pdf">NeUDF: Leaning Neural Unsigned Distance Fields with Volume Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ke_VILA_Learning_Image_Aesthetics_From_User_Comments_With_Vision-Language_Pretraining_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ke_VILA_Learning_Image_Aesthetics_From_User_Comments_With_Vision-Language_Pretraining_CVPR_2023_paper.pdf">VILA: Learning Image Aesthetics from User Comments with Vision-Language<br />Pretraining</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2022-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tschannen_CLIPPO_Image-and-Language_Understanding_From_Pixels_Only_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tschannen_CLIPPO_Image-and-Language_Understanding_From_Pixels_Only_CVPR_2023_paper.pdf">CLIPPO: Image-and-Language Understanding from Pixels Only</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Rethinking_Domain_Generalization_for_Face_Anti-Spoofing_Separability_and_Alignment_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Rethinking_Domain_Generalization_for_Face_Anti-Spoofing_Separability_and_Alignment_CVPR_2023_paper.pdf">Rethinking Domain Generalization for Face Anti-spoofing: Separability and Alignment</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Du_Adaptive_Sparse_Convolutional_Networks_With_Global_Context_Enhancement_for_Faster_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Du_Adaptive_Sparse_Convolutional_Networks_With_Global_Context_Enhancement_for_Faster_CVPR_2023_paper.pdf">Adaptive Sparse Convolutional Networks with Global Context Enhancement for<br />Faster Object Detection on Drone Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-04-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Song_Learning_With_Fantasy_Semantic-Aware_Virtual_Contrastive_Constraint_for_Few-Shot_Class-Incremental_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Song_Learning_With_Fantasy_Semantic-Aware_Virtual_Contrastive_Constraint_for_Few-Shot_Class-Incremental_CVPR_2023_paper.pdf">Learning with Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot<br />Class-Incremental Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-01-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhong_Understanding_Imbalanced_Semantic_Segmentation_Through_Neural_Collapse_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhong_Understanding_Imbalanced_Semantic_Segmentation_Through_Neural_Collapse_CVPR_2023_paper.pdf">Understanding Imbalanced Semantic Segmentation Through Neural Collapse</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-01-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Revisiting_Temporal_Modeling_for_CLIP-Based_Image-to-Video_Knowledge_Transferring_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Revisiting_Temporal_Modeling_for_CLIP-Based_Image-to-Video_Knowledge_Transferring_CVPR_2023_paper.pdf">Revisiting Temporal Modeling for CLIP-Based Image-to-Video Knowledge Transferring</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2022-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Naeem_I2MVFormer_Large_Language_Model_Generated_Multi-View_Document_Supervision_for_Zero-Shot_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Naeem_I2MVFormer_Large_Language_Model_Generated_Multi-View_Document_Supervision_for_Zero-Shot_CVPR_2023_paper.pdf">I2MVFormer: Large Language Model Generated Multi-View Document Supervision for<br />Zero-Shot Image Classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">37</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Park_Self-Positioning_Point-Based_Transformer_for_Point_Cloud_Understanding_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_Self-Positioning_Point-Based_Transformer_for_Point_Cloud_Understanding_CVPR_2023_paper.pdf">Self-Positioning Point-Based Transformer for Point Cloud Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-04-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hou_Evading_DeepFake_Detectors_via_Adversarial_Statistical_Consistency_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hou_Evading_DeepFake_Detectors_via_Adversarial_Statistical_Consistency_CVPR_2023_paper.pdf">Evading DeepFake Detectors via Adversarial Statistical Consistency</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-04-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liang_CrowdCLIP_Unsupervised_Crowd_Counting_via_Vision-Language_Model_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liang_CrowdCLIP_Unsupervised_Crowd_Counting_via_Vision-Language_Model_CVPR_2023_paper.pdf">CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ao_BUFFER_Balancing_Accuracy_Efficiency_and_Generalizability_in_Point_Cloud_Registration_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ao_BUFFER_Balancing_Accuracy_Efficiency_and_Generalizability_in_Point_Cloud_Registration_CVPR_2023_paper.pdf">BUFFER: Balancing Accuracy, Efficiency, and Generalizability in Point Cloud<br />Registration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-03-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shen_X-Avatar_Expressive_Human_Avatars_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_X-Avatar_Expressive_Human_Avatars_CVPR_2023_paper.pdf">X-Avatar: Expressive Human Avatars</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2022-07-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ye_PVO_Panoptic_Visual_Odometry_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ye_PVO_Panoptic_Visual_Odometry_CVPR_2023_paper.pdf">PVO: Panoptic Visual Odometry</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-03-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Continuous_Sign_Language_Recognition_With_Correlation_Network_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Continuous_Sign_Language_Recognition_With_Correlation_Network_CVPR_2023_paper.pdf">Continuous Sign Language Recognition with Correlation Network</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-04-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_PCR_Proxy-Based_Contrastive_Replay_for_Online_Class-Incremental_Continual_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_PCR_Proxy-Based_Contrastive_Replay_for_Online_Class-Incremental_Continual_Learning_CVPR_2023_paper.pdf">PCR: Proxy-Based Contrastive Replay for Online Class-Incremental Continual Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2022-06-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_A_Simple_Baseline_for_Video_Restoration_With_Grouped_Spatial-Temporal_Shift_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_A_Simple_Baseline_for_Video_Restoration_With_Grouped_Spatial-Temporal_Shift_CVPR_2023_paper.pdf">A Simple Baseline for Video Restoration with Grouped Spatial-Temporal<br />Shift</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_SparseViT_Revisiting_Activation_Sparsity_for_Efficient_High-Resolution_Vision_Transformer_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_SparseViT_Revisiting_Activation_Sparsity_for_Efficient_High-Resolution_Vision_Transformer_CVPR_2023_paper.pdf">SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shim_Diffusion-Based_Signed_Distance_Fields_for_3D_Shape_Generation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shim_Diffusion-Based_Signed_Distance_Fields_for_3D_Shape_Generation_CVPR_2023_paper.pdf">Diffusion-Based Signed Distance Fields for 3D Shape Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Efficient_RGB-T_Tracking_via_Cross-Modality_Distillation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Efficient_RGB-T_Tracking_via_Cross-Modality_Distillation_CVPR_2023_paper.pdf">Efficient RGB-T Tracking via Cross-Modality Distillation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ilhan_ScaleFL_Resource-Adaptive_Federated_Learning_With_Heterogeneous_Clients_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ilhan_ScaleFL_Resource-Adaptive_Federated_Learning_With_Heterogeneous_Clients_CVPR_2023_paper.pdf">ScaleFL: Resource-Adaptive Federated Learning with Heterogeneous Clients</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2022-11-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_T-SEA_Transfer-Based_Self-Ensemble_Attack_on_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_T-SEA_Transfer-Based_Self-Ensemble_Attack_on_Object_Detection_CVPR_2023_paper.pdf">T-SEA: Transfer-Based Self-Ensemble Attack on Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2023-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Twin_Contrastive_Learning_With_Noisy_Labels_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Twin_Contrastive_Learning_With_Noisy_Labels_CVPR_2023_paper.pdf">Twin Contrastive Learning with Noisy Labels</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2022-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cao_CiaoSR_Continuous_Implicit_Attention-in-Attention_Network_for_Arbitrary-Scale_Image_Super-Resolution_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_CiaoSR_Continuous_Implicit_Attention-in-Attention_Network_for_Arbitrary-Scale_Image_Super-Resolution_CVPR_2023_paper.pdf">CiaoSR: Continuous Implicit Attention-in-Attention Network for Arbitrary-Scale Image Super-Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">36</td>
<td style="text-align: center;">2022-11-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zeng_SceneComposer_Any-Level_Semantic_Image_Synthesis_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zeng_SceneComposer_Any-Level_Semantic_Image_Synthesis_CVPR_2023_paper.pdf">SceneComposer: Any-Level Semantic Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-01-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wimbauer_Behind_the_Scenes_Density_Fields_for_Single_View_Reconstruction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wimbauer_Behind_the_Scenes_Density_Fields_for_Single_View_Reconstruction_CVPR_2023_paper.pdf">Behind the Scenes: Density Fields for Single View Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2022-11-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Heo_A_Generalized_Framework_for_Video_Instance_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Heo_A_Generalized_Framework_for_Video_Instance_Segmentation_CVPR_2023_paper.pdf">A Generalized Framework for Video Instance Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-03-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Luo_Class-Incremental_Exemplar_Compression_for_Class-Incremental_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Luo_Class-Incremental_Exemplar_Compression_for_Class-Incremental_Learning_CVPR_2023_paper.pdf">Class-Incremental Exemplar Compression for Class-Incremental Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-02-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Seo_MixNeRF_Modeling_a_Ray_With_Mixture_Density_for_Novel_View_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Seo_MixNeRF_Modeling_a_Ray_With_Mixture_Density_for_Novel_View_CVPR_2023_paper.pdf">MixNeRF: Modeling a Ray with Mixture Density for Novel<br />View Synthesis from Sparse Inputs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_BEV-LaneDet_An_Efficient_3D_Lane_Detection_Based_on_Virtual_Camera_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_BEV-LaneDet_An_Efficient_3D_Lane_Detection_Based_on_Virtual_Camera_CVPR_2023_paper.pdf">BEV-LaneDet: An Efficient 3D Lane Detection Based on Virtual<br />Camera via Key-Points</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Hierarchical_Semantic_Contrast_for_Scene-Aware_Video_Anomaly_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Hierarchical_Semantic_Contrast_for_Scene-Aware_Video_Anomaly_Detection_CVPR_2023_paper.pdf">Hierarchical Semantic Contrast for Scene-aware Video Anomaly Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2022-06-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Seidenschwarz_Simple_Cues_Lead_to_a_Strong_Multi-Object_Tracker_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Seidenschwarz_Simple_Cues_Lead_to_a_Strong_Multi-Object_Tracker_CVPR_2023_paper.pdf">Simple Cues Lead to a Strong Multi-Object Tracker</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2022-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Deep_Frequency_Filtering_for_Domain_Generalization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Deep_Frequency_Filtering_for_Domain_Generalization_CVPR_2023_paper.pdf">Deep Frequency Filtering for Domain Generalization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_MCF_Mutual_Correction_Framework_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_MCF_Mutual_Correction_Framework_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf">MCF: Mutual Correction Framework for Semi-Supervised Medical Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_OVTrack_Open-Vocabulary_Multiple_Object_Tracking_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_OVTrack_Open-Vocabulary_Multiple_Object_Tracking_CVPR_2023_paper.pdf">OVTrack: Open-Vocabulary Multiple Object Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Improving_the_Transferability_of_Adversarial_Samples_by_Path-Augmented_Method_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Improving_the_Transferability_of_Adversarial_Samples_by_Path-Augmented_Method_CVPR_2023_paper.pdf">Improving the Transferability of Adversarial Samples by Path-Augmented Method</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-03-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_X-Pruner_eXplainable_Pruning_for_Vision_Transformers_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_X-Pruner_eXplainable_Pruning_for_Vision_Transformers_CVPR_2023_paper.pdf">X-Pruner: eXplainable Pruning for Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-01-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ashutosh_HierVL_Learning_Hierarchical_Video-Language_Embeddings_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ashutosh_HierVL_Learning_Hierarchical_Video-Language_Embeddings_CVPR_2023_paper.pdf">HierVL: Learning Hierarchical Video-Language Embeddings</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Giebenhain_Learning_Neural_Parametric_Head_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Giebenhain_Learning_Neural_Parametric_Head_Models_CVPR_2023_paper.pdf">Learning Neural Parametric Head Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_Color_Backdoor_A_Robust_Poisoning_Attack_in_Color_Space_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Color_Backdoor_A_Robust_Poisoning_Attack_in_Color_Space_CVPR_2023_paper.pdf">Color Backdoor: A Robust Poisoning Attack in Color Space</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-05-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Cao_Contrastive_Mean_Teacher_for_Domain_Adaptive_Object_Detectors_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Contrastive_Mean_Teacher_for_Domain_Adaptive_Object_Detectors_CVPR_2023_paper.pdf">Contrastive Mean Teacher for Domain Adaptive Object Detectors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-04-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Pan_Slide-Transformer_Hierarchical_Vision_Transformer_With_Local_Self-Attention_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pan_Slide-Transformer_Hierarchical_Vision_Transformer_With_Local_Self-Attention_CVPR_2023_paper.pdf">Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2022-11-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Fan_PMR_Prototypical_Modal_Rebalance_for_Multimodal_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_PMR_Prototypical_Modal_Rebalance_for_Multimodal_Learning_CVPR_2023_paper.pdf">PMR: Prototypical Modal Rebalance for Multimodal Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2022-10-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Bulat_LASP_Text-to-Text_Optimization_for_Language-Aware_Soft_Prompting_of_Vision__CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Bulat_LASP_Text-to-Text_Optimization_for_Language-Aware_Soft_Prompting_of_Vision__CVPR_2023_paper.pdf">LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision<br />&amp; Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_ProphNet_Efficient_Agent-Centric_Motion_Forecasting_With_Anchor-Informed_Proposals_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_ProphNet_Efficient_Agent-Centric_Motion_Forecasting_With_Anchor-Informed_Proposals_CVPR_2023_paper.pdf">ProphNet: Efficient Agent-Centric Motion Forecasting with Anchor-Informed Proposals</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">35</td>
<td style="text-align: center;">2023-04-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shao_Detecting_and_Grounding_Multi-Modal_Media_Manipulation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shao_Detecting_and_Grounding_Multi-Modal_Media_Manipulation_CVPR_2023_paper.pdf">Detecting and Grounding Multi-Modal Media Manipulation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-03-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ci_UniHCP_A_Unified_Model_for_Human-Centric_Perceptions_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ci_UniHCP_A_Unified_Model_for_Human-Centric_Perceptions_CVPR_2023_paper.pdf">UniHCP: A Unified Model for Human-Centric Perceptions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2022-08-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_TrojViT_Trojan_Insertion_in_Vision_Transformers_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_TrojViT_Trojan_Insertion_in_Vision_Transformers_CVPR_2023_paper.pdf">TrojViT: Trojan Insertion in Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhuang_GKEAL_Gaussian_Kernel_Embedded_Analytic_Learning_for_Few-Shot_Class_Incremental_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhuang_GKEAL_Gaussian_Kernel_Embedded_Analytic_Learning_for_Few-Shot_Class_Incremental_CVPR_2023_paper.pdf">GKEAL: Gaussian Kernel Embedded Analytic Learning for Few-Shot Class<br />Incremental Task</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mehl_Spring_A_High-Resolution_High-Detail_Dataset_and_Benchmark_for_Scene_Flow_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Mehl_Spring_A_High-Resolution_High-Detail_Dataset_and_Benchmark_for_Scene_Flow_CVPR_2023_paper.pdf">Spring: A High-Resolution High-Detail Dataset and Benchmark for Scene<br />Flow, Optical Flow and Stereo</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ning_HOICLIP_Efficient_Knowledge_Transfer_for_HOI_Detection_With_Vision-Language_Models_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ning_HOICLIP_Efficient_Knowledge_Transfer_for_HOI_Detection_With_Vision-Language_Models_CVPR_2023_paper.pdf">HOICLIP: Efficient Knowledge Transfer for HOI Detection with Vision-Language<br />Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_Joint_Visual_Grounding_and_Tracking_With_Natural_Language_Specification_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhou_Joint_Visual_Grounding_and_Tracking_With_Natural_Language_Specification_CVPR_2023_paper.pdf">Joint Visual Grounding and Tracking with Natural Language Specification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Context_De-Confounded_Emotion_Recognition_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Context_De-Confounded_Emotion_Recognition_CVPR_2023_paper.pdf">Context De-Confounded Emotion Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-02-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wu_Attention-Based_Point_Cloud_Edge_Sampling_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Attention-Based_Point_Cloud_Edge_Sampling_CVPR_2023_paper.pdf">Attention-Based Point Cloud Edge Sampling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Few-Shot_Class-Incremental_Learning_via_Class-Aware_Bilateral_Distillation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Few-Shot_Class-Incremental_Learning_via_Class-Aware_Bilateral_Distillation_CVPR_2023_paper.pdf">Few-Shot Class-Incremental Learning via Class-Aware Bilateral Distillation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-04-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kim_On_the_Stability-Plasticity_Dilemma_of_Class-Incremental_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_On_the_Stability-Plasticity_Dilemma_of_Class-Incremental_Learning_CVPR_2023_paper.pdf">On the Stability-Plasticity Dilemma of Class-Incremental Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-01-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tarasiou_ViTs_for_SITS_Vision_Transformers_for_Satellite_Image_Time_Series_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tarasiou_ViTs_for_SITS_Vision_Transformers_for_Satellite_Image_Time_Series_CVPR_2023_paper.pdf">ViTs for SITS: Vision Transformers for Satellite Image Time<br />Series</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2022-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Meta_Architecture_for_Point_Cloud_Analysis_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Meta_Architecture_for_Point_Cloud_Analysis_CVPR_2023_paper.pdf">Meta Architecture for Point Cloud Analysis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2022-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sinha_SparsePose_Sparse-View_Camera_Pose_Regression_and_Refinement_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sinha_SparsePose_Sparse-View_Camera_Pose_Regression_and_Refinement_CVPR_2023_paper.pdf">SparsePose: Sparse-View Camera Pose Regression and Refinement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jin_Context-Aware_Alignment_and_Mutual_Masking_for_3D-Language_Pre-Training_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jin_Context-Aware_Alignment_and_Mutual_Masking_for_3D-Language_Pre-Training_CVPR_2023_paper.pdf">Context-aware Alignment and Mutual Masking for 3D-Language Pre-training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2022-05-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_MixMAE_Mixed_and_Masked_Autoencoder_for_Efficient_Pretraining_of_Hierarchical_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_MixMAE_Mixed_and_Masked_Autoencoder_for_Efficient_Pretraining_of_Hierarchical_CVPR_2023_paper.pdf">MixMAE: Mixed and Masked Autoencoder for Efficient Pretraining of<br />Hierarchical Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2022-10-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Kamath_A_New_Path_Scaling_Vision-and-Language_Navigation_With_Synthetic_Instructions_and_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Kamath_A_New_Path_Scaling_Vision-and-Language_Navigation_With_Synthetic_Instructions_and_CVPR_2023_paper.pdf">A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions<br />and Imitation Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-04-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Continual_Semantic_Segmentation_With_Automatic_Memory_Sample_Selection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_Continual_Semantic_Segmentation_With_Automatic_Memory_Sample_Selection_CVPR_2023_paper.pdf">Continual Semantic Segmentation with Automatic Memory Sample Selection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-04-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Improved_Test-Time_Adaptation_for_Domain_Generalization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Improved_Test-Time_Adaptation_for_Domain_Generalization_CVPR_2023_paper.pdf">Improved Test-Time Adaptation for Domain Generalization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-01-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_End-to-End_3D_Dense_Captioning_With_Vote2Cap-DETR_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_End-to-End_3D_Dense_Captioning_With_Vote2Cap-DETR_CVPR_2023_paper.pdf">End-to-End 3D Dense Captioning with Vote2Cap-DETR</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-01-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Ramakrishnan_NaQ_Leveraging_Narrations_As_Queries_To_Supervise_Episodic_Memory_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ramakrishnan_NaQ_Leveraging_Narrations_As_Queries_To_Supervise_Episodic_Memory_CVPR_2023_paper.pdf">NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chan_Histopathology_Whole_Slide_Image_Analysis_With_Heterogeneous_Graph_Representation_Learning_CVPR_2023_paper.pdf">Histopathology Whole Slide Image Analysis with Heterogeneous Graph Representation<br />Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Rethinking_the_Correlation_in_Few-Shot_Segmentation_A_Buoys_View_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Rethinking_the_Correlation_in_Few-Shot_Segmentation_A_Buoys_View_CVPR_2023_paper.pdf">Rethinking the Correlation in Few-Shot Segmentation: A Buoys View</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-04-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lu_TransFlow_Transformer_As_Flow_Learner_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_TransFlow_Transformer_As_Flow_Learner_CVPR_2023_paper.pdf">TransFlow: Transformer as Flow Learner</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">34</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Geng_Human_Pose_As_Compositional_Tokens_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Geng_Human_Pose_As_Compositional_Tokens_CVPR_2023_paper.pdf">Human Pose as Compositional Tokens</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2022-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_NoisyQuant_Noisy_Bias-Enhanced_Post-Training_Activation_Quantization_for_Vision_Transformers_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_NoisyQuant_Noisy_Bias-Enhanced_Post-Training_Activation_Quantization_for_Vision_Transformers_CVPR_2023_paper.pdf">NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-05-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_PET-NeuS_Positional_Encoding_Tri-Planes_for_Neural_Surfaces_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_PET-NeuS_Positional_Encoding_Tri-Planes_for_Neural_Surfaces_CVPR_2023_paper.pdf">PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-03-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yu_CelebV-Text_A_Large-Scale_Facial_Text-Video_Dataset_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_CelebV-Text_A_Large-Scale_Facial_Text-Video_Dataset_CVPR_2023_paper.pdf">CelebV-Text: A Large-Scale Facial Text-Video Dataset</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-02-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tan_Backdoor_Attacks_Against_Deep_Image_Compression_via_Adaptive_Frequency_Trigger_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tan_Backdoor_Attacks_Against_Deep_Image_Compression_via_Adaptive_Frequency_Trigger_CVPR_2023_paper.pdf">Backdoor Attacks Against Deep Image Compression via Adaptive Frequency<br />Trigger</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2022-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Su_Towards_All-in-One_Pre-Training_via_Maximizing_Multi-Modal_Mutual_Information_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Su_Towards_All-in-One_Pre-Training_via_Maximizing_Multi-Modal_Mutual_Information_CVPR_2023_paper.pdf">Towards All-in-One Pre-Training via Maximizing Multi-Modal Mutual Information</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Spatial-Frequency_Mutual_Learning_for_Face_Super-Resolution_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Spatial-Frequency_Mutual_Learning_for_Face_Super-Resolution_CVPR_2023_paper.pdf">Spatial-Frequency Mutual Learning for Face Super-Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2022-09-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_PyPose_A_Library_for_Robot_Learning_With_Physics-Based_Optimization_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_PyPose_A_Library_for_Robot_Learning_With_Physics-Based_Optimization_CVPR_2023_paper.pdf">PyPose: A Library for Robot Learning with Physics-based Optimization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-04-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Nguyen_Micron-BERT_BERT-Based_Facial_Micro-Expression_Recognition_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Nguyen_Micron-BERT_BERT-Based_Facial_Micro-Expression_Recognition_CVPR_2023_paper.pdf">Micron-BERT: BERT-Based Facial Micro-Expression Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_GEN_Pushing_the_Limits_of_Softmax-Based_Out-of-Distribution_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GEN_Pushing_the_Limits_of_Softmax-Based_Out-of-Distribution_Detection_CVPR_2023_paper.pdf">GEN: Pushing the Limits of Softmax-Based Out-of-Distribution Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_I2-SDF_Intrinsic_Indoor_Scene_Reconstruction_and_Editing_via_Raytracing_in_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhu_I2-SDF_Intrinsic_Indoor_Scene_Reconstruction_and_Editing_via_Raytracing_in_CVPR_2023_paper.pdf">I2-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing<br />in Neural SDFs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-02-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Li_ProxyFormer_Proxy_Alignment_Assisted_Point_Cloud_Completion_With_Missing_Part_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Li_ProxyFormer_Proxy_Alignment_Assisted_Point_Cloud_Completion_With_Missing_Part_CVPR_2023_paper.pdf">ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing<br />Part Sensitive Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2022-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Shen_LidarGait_Benchmarking_3D_Gait_Recognition_With_Point_Clouds_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shen_LidarGait_Benchmarking_3D_Gait_Recognition_With_Point_Clouds_CVPR_2023_paper.pdf">LidarGait: Benchmarking 3D Gait Recognition with Point Clouds</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2022-12-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Islam_Efficient_Movie_Scene_Detection_Using_State-Space_Transformers_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Islam_Efficient_Movie_Scene_Detection_Using_State-Space_Transformers_CVPR_2023_paper.pdf">Efficient Movie Scene Detection using State-Space Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2022-07-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Gao_Back_to_the_Source_Diffusion-Driven_Adaptation_To_Test-Time_Corruption_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Gao_Back_to_the_Source_Diffusion-Driven_Adaptation_To_Test-Time_Corruption_CVPR_2023_paper.pdf">Back to the Source: Diffusion-Driven Adaptation to Test-Time Corruption</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-03-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yin_Hi4D_4D_Instance_Segmentation_of_Close_Human_Interaction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yin_Hi4D_4D_Instance_Segmentation_of_Close_Human_Interaction_CVPR_2023_paper.pdf">Hi4D: 4D Instance Segmentation of Close Human Interaction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2022-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tu_Visual_Query_Tuning_Towards_Effective_Usage_of_Intermediate_Representations_for_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Tu_Visual_Query_Tuning_Towards_Effective_Usage_of_Intermediate_Representations_for_CVPR_2023_paper.pdf">Visual Query Tuning: Towards Effective Usage of Intermediate Representations<br />for Parameter and Memory Efficient Transfer Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2022-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Messikommer_Data-Driven_Feature_Tracking_for_Event_Cameras_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Messikommer_Data-Driven_Feature_Tracking_for_Event_Cameras_CVPR_2023_paper.pdf">Data-Driven Feature Tracking for Event Cameras</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-03-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Masked_Image_Modeling_With_Local_Multi-Scale_Reconstruction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Masked_Image_Modeling_With_Local_Multi-Scale_Reconstruction_CVPR_2023_paper.pdf">Masked Image Modeling with Local Multi-Scale Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Basak_Pseudo-Label_Guided_Contrastive_Learning_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Basak_Pseudo-Label_Guided_Contrastive_Learning_for_Semi-Supervised_Medical_Image_Segmentation_CVPR_2023_paper.pdf">Pseudo-Label Guided Contrastive Learning for Semi-Supervised Medical Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_CAMS_CAnonicalized_Manipulation_Spaces_for_Category-Level_Functional_Hand-Object_Manipulation_Synthesis_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zheng_CAMS_CAnonicalized_Manipulation_Spaces_for_Category-Level_Functional_Hand-Object_Manipulation_Synthesis_CVPR_2023_paper.pdf">CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation<br />Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-02-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Khurana_Point_Cloud_Forecasting_as_a_Proxy_for_4D_Occupancy_Forecasting_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Khurana_Point_Cloud_Forecasting_as_a_Proxy_for_4D_Occupancy_Forecasting_CVPR_2023_paper.pdf">Point Cloud Forecasting as a Proxy for 4D Occupancy<br />Forecasting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-04-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Dynamic_Coarse-To-Fine_Learning_for_Oriented_Tiny_Object_Detection_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xu_Dynamic_Coarse-To-Fine_Learning_for_Oriented_Tiny_Object_Detection_CVPR_2023_paper.pdf">Dynamic Coarse-to-Fine Learning for Oriented Tiny Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Leyva-Vallina_Data-Efficient_Large_Scale_Place_Recognition_With_Graded_Similarity_Supervision_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Leyva-Vallina_Data-Efficient_Large_Scale_Place_Recognition_With_Graded_Similarity_Supervision_CVPR_2023_paper.pdf">Data-Efficient Large Scale Place Recognition with Graded Similarity Supervision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-03-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Feature_Alignment_and_Uniformity_for_Test_Time_Adaptation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Feature_Alignment_and_Uniformity_for_Test_Time_Adaptation_CVPR_2023_paper.pdf">Feature Alignment and Uniformity for Test Time Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-04-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Feng_Shape-Erased_Feature_Learning_for_Visible-Infrared_Person_Re-Identification_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Shape-Erased_Feature_Learning_for_Visible-Infrared_Person_Re-Identification_CVPR_2023_paper.pdf">Shape-Erased Feature Learning for Visible-Infrared Person Re-Identification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-03-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Feng_Detecting_Backdoors_in_Pre-Trained_Encoders_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Feng_Detecting_Backdoors_in_Pre-Trained_Encoders_CVPR_2023_paper.pdf">Detecting Backdoors in Pre-trained Encoders</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">33</td>
<td style="text-align: center;">2023-04-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Park_ViPLO_Vision_Transformer_Based_Pose-Conditioned_Self-Loop_Graph_for_Human-Object_Interaction_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Park_ViPLO_Vision_Transformer_Based_Pose-Conditioned_Self-Loop_Graph_for_Human-Object_Interaction_CVPR_2023_paper.pdf">ViPLO: Vision Transformer Based Pose-Conditioned Self-Loop Graph for Human-Object<br />Interaction Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2023-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_Fair_Federated_Medical_Image_Segmentation_via_Client_Contribution_Estimation_CVPR_2023_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Jiang_Fair_Federated_Medical_Image_Segmentation_via_Client_Contribution_Estimation_CVPR_2023_paper.pdf">Fair Federated Medical Image Segmentation via Client Contribution Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../COLM/COLM_2024/" class="btn btn-neutral float-left" title="COLM 2024"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../CVPR_2024/" class="btn btn-neutral float-right" title="CVPR 2024">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../COLM/COLM_2024/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../CVPR_2024/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
