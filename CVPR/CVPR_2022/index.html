<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>CVPR 2022 - AI Papers</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../mytheme.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "CVPR 2022";
        var mkdocs_page_input_path = "CVPR/CVPR_2022.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> AI Papers
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">AI Papers</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">ACL</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ACL/ACL_2022/">ACL 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ACL/ACL_2023/">ACL 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">CVPR</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">CVPR 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../CVPR_2023/">CVPR 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">EMNLP</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../EMNLP/EMNLP_2022/">EMNLP 2022</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICCV</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICCV/ICCV_2023/">ICCV 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICLR</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2022/">ICLR 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICLR/ICLR_2023/">ICLR 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">ICML</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2022/">ICML 2022</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../ICML/ICML_2023/">ICML 2023</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">NeurIPS</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../NeurIPS/NeurIPS_2022/">NeurIPS 2022</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">AI Papers</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">CVPR</li>
      <li class="breadcrumb-item active">CVPR 2022</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <p>Last updated: 2023-10-11 17:35:51. Maintained by <a href="https://wayson.tech/">Weisen Jiang</a>.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">citation</th>
<th style="text-align: center;">date</th>
<th style="text-align: center;">review</th>
<th style="text-align: left;">title (pdf)</th>
<th style="text-align: left;">authors</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">3096</td>
<td style="text-align: center;">2021-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf">High-Resolution Image Synthesis with Latent Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">2798</td>
<td style="text-align: center;">2021-11-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf">Masked Autoencoders Are Scalable Vision Learners</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">1729</td>
<td style="text-align: center;">2022-01-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.pdf">A ConvNet for the 2020s</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">700</td>
<td style="text-align: center;">2021-06-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Video_Swin_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Video_Swin_Transformer_CVPR_2022_paper.pdf">Video Swin Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">646</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.pdf">Masked-attention Mask Transformer for Universal Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">599</td>
<td style="text-align: center;">2021-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.pdf">Swin Transformer V2: Scaling Up Capacity and Resolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">580</td>
<td style="text-align: center;">2021-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zamir_Restormer_Efficient_Transformer_for_High-Resolution_Image_Restoration_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zamir_Restormer_Efficient_Transformer_for_High-Resolution_Image_Restoration_CVPR_2022_paper.pdf">Restormer: Efficient Transformer for High-Resolution Image Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">569</td>
<td style="text-align: center;">2021-06-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhai_Scaling_Vision_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhai_Scaling_Vision_Transformers_CVPR_2022_paper.pdf">Scaling Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">561</td>
<td style="text-align: center;">2021-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xie_SimMIM_A_Simple_Framework_for_Masked_Image_Modeling_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_SimMIM_A_Simple_Framework_for_Masked_Image_Modeling_CVPR_2022_paper.pdf">SimMIM: a Simple Framework for Masked Image Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">553</td>
<td style="text-align: center;">2021-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Fridovich-Keil_Plenoxels_Radiance_Fields_Without_Neural_Networks_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fridovich-Keil_Plenoxels_Radiance_Fields_Without_Neural_Networks_CVPR_2022_paper.pdf">Plenoxels: Radiance Fields without Neural Networks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">506</td>
<td style="text-align: center;">2021-06-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Uformer_A_General_U-Shaped_Transformer_for_Image_Restoration_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Uformer_A_General_U-Shaped_Transformer_for_Image_Restoration_CVPR_2022_paper.pdf">Uformer: A General U-Shaped Transformer for Image Restoration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">477</td>
<td style="text-align: center;">2021-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chan_Efficient_Geometry-Aware_3D_Generative_Adversarial_Networks_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chan_Efficient_Geometry-Aware_3D_Generative_Adversarial_Networks_CVPR_2022_paper.pdf">Efficient Geometry-aware 3D Generative Adversarial Networks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">445</td>
<td style="text-align: center;">2021-07-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Dong_CSWin_Transformer_A_General_Vision_Transformer_Backbone_With_Cross-Shaped_Windows_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_CSWin_Transformer_A_General_Vision_Transformer_Backbone_With_Cross-Shaped_Windows_CVPR_2022_paper.pdf">CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped<br />Windows</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">411</td>
<td style="text-align: center;">2022-01-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lugmayr_RePaint_Inpainting_Using_Denoising_Diffusion_Probabilistic_Models_CVPR_2022_paper.pdf">RePaint: Inpainting using Denoising Diffusion Probabilistic Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">376</td>
<td style="text-align: center;">2022-03-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.pdf">Conditional Prompt Learning for Vision-Language Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">374</td>
<td style="text-align: center;">2021-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Barron_Mip-NeRF_360_Unbounded_Anti-Aliased_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Barron_Mip-NeRF_360_Unbounded_Anti-Aliased_Neural_Radiance_Fields_CVPR_2022_paper.pdf">Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">366</td>
<td style="text-align: center;">2021-01-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Meinhardt_TrackFormer_Multi-Object_Tracking_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Meinhardt_TrackFormer_Multi-Object_Tracking_With_Transformers_CVPR_2022_paper.pdf">TrackFormer: Multi-Object Tracking with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">361</td>
<td style="text-align: center;">2021-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sun_Direct_Voxel_Grid_Optimization_Super-Fast_Convergence_for_Radiance_Fields_Reconstruction_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Direct_Voxel_Grid_Optimization_Super-Fast_Convergence_for_Radiance_Fields_Reconstruction_CVPR_2022_paper.pdf">Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields<br />Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">348</td>
<td style="text-align: center;">2021-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wei_Masked_Feature_Prediction_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wei_Masked_Feature_Prediction_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.pdf">Masked Feature Prediction for Self-Supervised Visual Pre-Training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">330</td>
<td style="text-align: center;">2021-07-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Deng_Depth-Supervised_NeRF_Fewer_Views_and_Faster_Training_for_Free_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_Depth-Supervised_NeRF_Fewer_Views_and_Faster_Training_for_Free_CVPR_2022_paper.pdf">Depth-supervised NeRF: Fewer Views and Faster Training for Free</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">328</td>
<td style="text-align: center;">2021-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.pdf">Grounded Language-Image Pre-training</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">315</td>
<td style="text-align: center;">2021-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yu_MetaFormer_Is_Actually_What_You_Need_for_Vision_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_MetaFormer_Is_Actually_What_You_Need_for_Vision_CVPR_2022_paper.pdf">MetaFormer is Actually What You Need for Vision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">297</td>
<td style="text-align: center;">2022-03-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.pdf">Scaling Up Your Kernels to 31×31: Revisiting Large Kernel<br />Design in CNNs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">292</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.pdf">MViTv2: Improved Multiscale Vision Transformers for Classification and Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">291</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Gu_Vector_Quantized_Diffusion_Model_for_Text-to-Image_Synthesis_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Gu_Vector_Quantized_Diffusion_Model_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf">Vector Quantized Diffusion Model for Text-to-Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">274</td>
<td style="text-align: center;">2021-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.pdf">FLAVA: A Foundational Language And Vision Alignment Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">273</td>
<td style="text-align: center;">2022-02-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tancik_Block-NeRF_Scalable_Large_Scene_Neural_View_Synthesis_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tancik_Block-NeRF_Scalable_Large_Scene_Neural_View_Synthesis_CVPR_2022_paper.pdf">Block-NeRF: Scalable Large Scene Neural View Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">272</td>
<td style="text-align: center;">2021-07-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_CMT_Convolutional_Neural_Networks_Meet_Vision_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_CMT_Convolutional_Neural_Networks_Meet_Vision_Transformers_CVPR_2022_paper.pdf">CMT: Convolutional Neural Networks Meet Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">269</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Avrahami_Blended_Diffusion_for_Text-Driven_Editing_of_Natural_Images_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Avrahami_Blended_Diffusion_for_Text-Driven_Editing_of_Natural_Images_CVPR_2022_paper.pdf">Blended Diffusion for Text-driven Editing of Natural Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">264</td>
<td style="text-align: center;">2021-09-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wortsman_Robust_Fine-Tuning_of_Zero-Shot_Models_CVPR_2022_paper.pdf">Robust fine-tuning of zero-shot models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">247</td>
<td style="text-align: center;">2021-11-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhai_LiT_Zero-Shot_Transfer_With_Locked-Image_Text_Tuning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhai_LiT_Zero-Shot_Transfer_With_Locked-Image_Text_Tuning_CVPR_2022_paper.pdf">LiT: Zero-Shot Transfer with Locked-image text Tuning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">232</td>
<td style="text-align: center;">2021-06-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Roth_Towards_Total_Recall_in_Industrial_Anomaly_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Roth_Towards_Total_Recall_in_Industrial_Anomaly_Detection_CVPR_2022_paper.pdf">Towards Total Recall in Industrial Anomaly Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">228</td>
<td style="text-align: center;">2021-12-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Verbin_Ref-NeRF_Structured_View-Dependent_Appearance_for_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Verbin_Ref-NeRF_Structured_View-Dependent_Appearance_for_Neural_Radiance_Fields_CVPR_2022_paper.pdf">Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">221</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Jain_Zero-Shot_Text-Guided_Object_Generation_With_Dream_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Jain_Zero-Shot_Text-Guided_Object_Generation_With_Dream_Fields_CVPR_2022_paper.pdf">Zero-Shot Text-Guided Object Generation with Dream Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">218</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.pdf">DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">211</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.pdf">Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point<br />Modeling</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">209</td>
<td style="text-align: center;">2021-10-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kim_DiffusionCLIP_Text-Guided_Diffusion_Models_for_Robust_Image_Manipulation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_DiffusionCLIP_Text-Guided_Diffusion_Models_for_Robust_Image_Manipulation_CVPR_2022_paper.pdf">DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">208</td>
<td style="text-align: center;">2021-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Or-El_StyleSDF_High-Resolution_3D-Consistent_Image_and_Geometry_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Or-El_StyleSDF_High-Resolution_3D-Consistent_Image_and_Geometry_Generation_CVPR_2022_paper.pdf">StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">205</td>
<td style="text-align: center;">2021-08-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Mobile-Former_Bridging_MobileNet_and_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Mobile-Former_Bridging_MobileNet_and_Transformer_CVPR_2022_paper.pdf">Mobile-Former: Bridging MobileNet and Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">198</td>
<td style="text-align: center;">2021-12-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_NICE-SLAM_Neural_Implicit_Scalable_Encoding_for_SLAM_CVPR_2022_paper.pdf">NICE-SLAM: Neural Implicit Scalable Encoding for SLAM</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">195</td>
<td style="text-align: center;">2021-11-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Dou_An_Empirical_Study_of_Training_End-to-End_Vision-and-Language_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dou_An_Empirical_Study_of_Training_End-to-End_Vision-and-Language_Transformers_CVPR_2022_paper.pdf">An Empirical Study of Training End-to-End Vision-and-Language Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">192</td>
<td style="text-align: center;">2021-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Niemeyer_RegNeRF_Regularizing_Neural_Radiance_Fields_for_View_Synthesis_From_Sparse_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Niemeyer_RegNeRF_Regularizing_Neural_Radiance_Fields_for_View_Synthesis_From_Sparse_CVPR_2022_paper.pdf">RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from<br />Sparse Inputs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">191</td>
<td style="text-align: center;">2021-04-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Duan_Revisiting_Skeleton-Based_Action_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Duan_Revisiting_Skeleton-Based_Action_Recognition_CVPR_2022_paper.pdf">Revisiting Skeleton-based Action Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">189</td>
<td style="text-align: center;">2022-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Bai_TransFusion_Robust_LiDAR-Camera_Fusion_for_3D_Object_Detection_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Bai_TransFusion_Robust_LiDAR-Camera_Fusion_for_3D_Object_Detection_With_Transformers_CVPR_2022_paper.pdf">TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with<br />Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">185</td>
<td style="text-align: center;">2022-03-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.pdf">DN-DETR: Accelerate DETR Training by Introducing Query DeNoising</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">183</td>
<td style="text-align: center;">2022-02-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chang_MaskGIT_Masked_Generative_Image_Transformer_CVPR_2022_paper.pdf">MaskGIT: Masked Generative Image Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">182</td>
<td style="text-align: center;">2021-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.pdf">RegionCLIP: Region-based Language-Image Pretraining</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">177</td>
<td style="text-align: center;">2021-04-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chan_BasicVSR_Improving_Video_Super-Resolution_With_Enhanced_Propagation_and_Alignment_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chan_BasicVSR_Improving_Video_Super-Resolution_With_Enhanced_Propagation_and_Alignment_CVPR_2022_paper.pdf">BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation and Alignment</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">177</td>
<td style="text-align: center;">2021-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_CLIP-NeRF_Text-and-Image_Driven_Manipulation_of_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_CLIP-NeRF_Text-and-Image_Driven_Manipulation_of_Neural_Radiance_Fields_CVPR_2022_paper.pdf">CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">175</td>
<td style="text-align: center;">2021-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Learning_To_Prompt_for_Continual_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Learning_To_Prompt_for_Continual_Learning_CVPR_2022_paper.pdf">Learning to Prompt for Continual Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">174</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.pdf">Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image<br />Analysis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">171</td>
<td style="text-align: center;">2022-02-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_GroupViT_Semantic_Segmentation_Emerges_From_Text_Supervision_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_GroupViT_Semantic_Segmentation_Emerges_From_Text_Supervision_CVPR_2022_paper.pdf">GroupViT: Semantic Segmentation Emerges from Text Supervision</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">169</td>
<td style="text-align: center;">2022-01-11</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Weng_HumanNeRF_Free-Viewpoint_Rendering_of_Moving_People_From_Monocular_Video_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Weng_HumanNeRF_Free-Viewpoint_Rendering_of_Moving_People_From_Monocular_Video_CVPR_2022_paper.pdf">HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular Video</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">166</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Hoyer_DAFormer_Improving_Network_Architectures_and_Training_Strategies_for_Domain-Adaptive_Semantic_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hoyer_DAFormer_Improving_Network_Architectures_and_Training_Strategies_for_Domain-Adaptive_Semantic_CVPR_2022_paper.pdf">DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive<br />Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">161</td>
<td style="text-align: center;">2022-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhao_Decoupled_Knowledge_Distillation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_Decoupled_Knowledge_Distillation_CVPR_2022_paper.pdf">Decoupled Knowledge Distillation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">160</td>
<td style="text-align: center;">2022-01-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tu_MAXIM_Multi-Axis_MLP_for_Image_Processing_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tu_MAXIM_Multi-Axis_MLP_for_Image_Processing_CVPR_2022_paper.pdf">MAXIM: Multi-Axis MLP for Image Processing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">153</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Alaluf_HyperStyle_StyleGAN_Inversion_With_HyperNetworks_for_Real_Image_Editing_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Alaluf_HyperStyle_StyleGAN_Inversion_With_HyperNetworks_for_Real_Image_Editing_CVPR_2022_paper.pdf">HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">151</td>
<td style="text-align: center;">2022-01-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_Point-NeRF_Point-Based_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Point-NeRF_Point-Based_Neural_Radiance_Fields_CVPR_2022_paper.pdf">Point-NeRF: Point-based Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">151</td>
<td style="text-align: center;">2021-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Michel_Text2Mesh_Text-Driven_Neural_Stylization_for_Meshes_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Michel_Text2Mesh_Text-Driven_Neural_Stylization_for_Meshes_CVPR_2022_paper.pdf">Text2Mesh: Text-Driven Neural Stylization for Meshes</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">150</td>
<td style="text-align: center;">2021-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mildenhall_NeRF_in_the_Dark_High_Dynamic_Range_View_Synthesis_From_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Mildenhall_NeRF_in_the_Dark_High_Dynamic_Range_View_Synthesis_From_CVPR_2022_paper.pdf">NeRF in the Dark: High Dynamic Range View Synthesis<br />from Noisy Raw Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">141</td>
<td style="text-align: center;">2022-01-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xia_Vision_Transformer_With_Deformable_Attention_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xia_Vision_Transformer_With_Deformable_Attention_CVPR_2022_paper.pdf">Vision Transformer with Deformable Attention</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">141</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Preechakul_Diffusion_Autoencoders_Toward_a_Meaningful_and_Decodable_Representation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Preechakul_Diffusion_Autoencoders_Toward_a_Meaningful_and_Decodable_Representation_CVPR_2022_paper.pdf">Diffusion Autoencoders: Toward a Meaningful and Decodable Representation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">140</td>
<td style="text-align: center;">2021-09-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_High-Fidelity_GAN_Inversion_for_Image_Attribute_Editing_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_High-Fidelity_GAN_Inversion_for_Image_Attribute_Editing_CVPR_2022_paper.pdf">High-Fidelity GAN Inversion for Image Attribute Editing</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">139</td>
<td style="text-align: center;">2021-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf">PointCLIP: Point Cloud Understanding by CLIP</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">137</td>
<td style="text-align: center;">2021-12-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Roessle_Dense_Depth_Priors_for_Neural_Radiance_Fields_From_Sparse_Input_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Roessle_Dense_Depth_Priors_for_Neural_Radiance_Fields_From_Sparse_Input_CVPR_2022_paper.pdf">Dense Depth Priors for Neural Radiance Fields from Sparse<br />Input Views</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">136</td>
<td style="text-align: center;">2021-06-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Beyer_Knowledge_Distillation_A_Good_Teacher_Is_Patient_and_Consistent_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Beyer_Knowledge_Distillation_A_Good_Teacher_Is_Patient_and_Consistent_CVPR_2022_paper.pdf">Knowledge distillation: A good teacher is patient and consistent</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">136</td>
<td style="text-align: center;">2021-10-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sanghi_CLIP-Forge_Towards_Zero-Shot_Text-To-Shape_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sanghi_CLIP-Forge_Towards_Zero-Shot_Text-To-Shape_Generation_CVPR_2022_paper.pdf">CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">135</td>
<td style="text-align: center;">2021-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Deng_GRAM_Generative_Radiance_Manifolds_for_3D-Aware_Image_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_GRAM_Generative_Radiance_Manifolds_for_3D-Aware_Image_Generation_CVPR_2022_paper.pdf">GRAM: Generative Radiance Manifolds for 3D-Aware Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">135</td>
<td style="text-align: center;">2021-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sung_VL-Adapter_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Tasks_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sung_VL-Adapter_Parameter-Efficient_Transfer_Learning_for_Vision-and-Language_Tasks_CVPR_2022_paper.pdf">VL-ADAPTER: Parameter-Efficient Transfer Learning for Vision-and-Language Tasks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">134</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Scaling_Vision_Transformers_to_Gigapixel_Images_via_Hierarchical_Self-Supervised_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Scaling_Vision_Transformers_to_Gigapixel_Images_via_Hierarchical_Self-Supervised_Learning_CVPR_2022_paper.pdf">Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised<br />Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">133</td>
<td style="text-align: center;">2022-04-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ma_Toward_Fast_Flexible_and_Robust_Low-Light_Image_Enhancement_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Toward_Fast_Flexible_and_Robust_Low-Light_Image_Enhancement_CVPR_2022_paper.pdf">Toward Fast, Flexible, and Robust Low-Light Image Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">130</td>
<td style="text-align: center;">2022-04-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Thrush_Winoground_Probing_Vision_and_Language_Models_for_Visio-Linguistic_Compositionality_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Thrush_Winoground_Probing_Vision_and_Language_Models_for_Visio-Linguistic_Compositionality_CVPR_2022_paper.pdf">Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">2021-11-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Munkberg_Extracting_Triangular_3D_Models_Materials_and_Lighting_From_Images_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Munkberg_Extracting_Triangular_3D_Models_Materials_and_Lighting_From_Images_CVPR_2022_paper.pdf">Extracting Triangular 3D Models, Materials, and Lighting From Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">125</td>
<td style="text-align: center;">2022-03-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Continual_Test-Time_Domain_Adaptation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Continual_Test-Time_Domain_Adaptation_CVPR_2022_paper.pdf">Continual Test-Time Domain Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">123</td>
<td style="text-align: center;">2021-04-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Azinovic_Neural_RGB-D_Surface_Reconstruction_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Azinovic_Neural_RGB-D_Surface_Reconstruction_CVPR_2022_paper.pdf">Neural RGB-D Surface Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">123</td>
<td style="text-align: center;">2022-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cui_MixFormer_End-to-End_Tracking_With_Iterative_Mixed_Attention_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cui_MixFormer_End-to-End_Tracking_With_Iterative_Mixed_Attention_CVPR_2022_paper.pdf">MixFormer: End-to-End Tracking with Iterative Mixed Attention</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">122</td>
<td style="text-align: center;">2021-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chung_Come-Closer-Diffuse-Faster_Accelerating_Conditional_Diffusion_Models_for_Inverse_Problems_Through_Stochastic_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chung_Come-Closer-Diffuse-Faster_Accelerating_Conditional_Diffusion_Models_for_Inverse_Problems_Through_Stochastic_CVPR_2022_paper.pdf">Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through<br />Stochastic Contraction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">122</td>
<td style="text-align: center;">2022-02-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Vision-Language_Pre-Training_With_Triple_Contrastive_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Vision-Language_Pre-Training_With_Triple_Contrastive_Learning_CVPR_2022_paper.pdf">Vision-Language Pre-Training with Triple Contrastive Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">120</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_CRIS_CLIP-Driven_Referring_Image_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_CRIS_CLIP-Driven_Referring_Image_Segmentation_CVPR_2022_paper.pdf">CRIS: CLIP-Driven Referring Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">120</td>
<td style="text-align: center;">2022-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Du_Learning_To_Prompt_for_Open-Vocabulary_Object_Detection_With_Vision-Language_Model_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Du_Learning_To_Prompt_for_Open-Vocabulary_Object_Detection_With_Vision-Language_Model_CVPR_2022_paper.pdf">Learning to Prompt for Open-Vocabulary Object Detection with Vision-Language<br />Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">119</td>
<td style="text-align: center;">2022-03-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Semi-Supervised_Semantic_Segmentation_Using_Unreliable_Pseudo-Labels_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Semi-Supervised_Semantic_Segmentation_Using_Unreliable_Pseudo-Labels_CVPR_2022_paper.pdf">Semi-Supervised Semantic Segmentation Using Unreliable Pseudo-Labels</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">119</td>
<td style="text-align: center;">2021-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kwon_CLIPstyler_Image_Style_Transfer_With_a_Single_Text_Condition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kwon_CLIPstyler_Image_Style_Transfer_With_a_Single_Text_Condition_CVPR_2022_paper.pdf">CLIPstyler: Image Style Transfer with a Single Text Condition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">118</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Parmar_On_Aliased_Resizing_and_Surprising_Subtleties_in_GAN_Evaluation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Parmar_On_Aliased_Resizing_and_Surprising_Subtleties_in_GAN_Evaluation_CVPR_2022_paper.pdf">On Aliased Resizing and Surprising Subtleties in GAN Evaluation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">117</td>
<td style="text-align: center;">2022-01-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yan_Multiview_Transformers_for_Video_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yan_Multiview_Transformers_for_Video_Recognition_CVPR_2022_paper.pdf">Multiview Transformers for Video Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">115</td>
<td style="text-align: center;">2021-12-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Skorokhodov_StyleGAN-V_A_Continuous_Video_Generator_With_the_Price_Image_Quality_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Skorokhodov_StyleGAN-V_A_Continuous_Video_Generator_With_the_Price_Image_Quality_CVPR_2022_paper.pdf">StyleGAN-V: A Continuous Video Generator with the Price, Image<br />Quality and Perks of StyleGAN2</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">115</td>
<td style="text-align: center;">2021-11-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Khandelwal_Simple_but_Effective_CLIP_Embeddings_for_Embodied_AI_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Khandelwal_Simple_but_Effective_CLIP_Embeddings_for_Embodied_AI_CVPR_2022_paper.pdf">Simple but Effective: CLIP Embeddings for Embodied AI</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">113</td>
<td style="text-align: center;">2022-02-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Qin_Geometric_Transformer_for_Fast_and_Robust_Point_Cloud_Registration_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Qin_Geometric_Transformer_for_Fast_and_Robust_Point_Cloud_Registration_CVPR_2022_paper.pdf">Geometric Transformer for Fast and Robust Point Cloud Registration</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">113</td>
<td style="text-align: center;">2022-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_DeepFusion_Lidar-Camera_Deep_Fusion_for_Multi-Modal_3D_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_DeepFusion_Lidar-Camera_Deep_Fusion_for_Multi-Modal_3D_Object_Detection_CVPR_2022_paper.pdf">DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">112</td>
<td style="text-align: center;">2021-11-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Douillard_DyTox_Transformers_for_Continual_Learning_With_DYnamic_TOken_eXpansion_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Douillard_DyTox_Transformers_for_Continual_Learning_With_DYnamic_TOken_eXpansion_CVPR_2022_paper.pdf">DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">112</td>
<td style="text-align: center;">2022-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lai_Stratified_Transformer_for_3D_Point_Cloud_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lai_Stratified_Transformer_for_3D_Point_Cloud_Segmentation_CVPR_2022_paper.pdf">Stratified Transformer for 3D Point Cloud Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">111</td>
<td style="text-align: center;">2022-04-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Unified_Contrastive_Learning_in_Image-Text-Label_Space_CVPR_2022_paper.pdf">Unified Contrastive Learning in Image-Text-Label Space</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">111</td>
<td style="text-align: center;">2021-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yin_A-ViT_Adaptive_Tokens_for_Efficient_Vision_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yin_A-ViT_Adaptive_Tokens_for_Efficient_Vision_Transformer_CVPR_2022_paper.pdf">A-ViT: Adaptive Tokens for Efficient Vision Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">110</td>
<td style="text-align: center;">2022-04-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kim_AdaFace_Quality_Adaptive_Margin_for_Face_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kim_AdaFace_Quality_Adaptive_Margin_for_Face_Recognition_CVPR_2022_paper.pdf">AdaFace: Quality Adaptive Margin for Face Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">110</td>
<td style="text-align: center;">2021-12-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Hong_HeadNeRF_A_Real-Time_NeRF-Based_Parametric_Head_Model_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_HeadNeRF_A_Real-Time_NeRF-Based_Parametric_Head_Model_CVPR_2022_paper.pdf">HeadNeRF: A Realtime NeRF-based Parametric Head Model</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">109</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_BEVT_BERT_Pretraining_of_Video_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_BEVT_BERT_Pretraining_of_Video_Transformers_CVPR_2022_paper.pdf">BEVT: BERT Pretraining of Video Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">108</td>
<td style="text-align: center;">2022-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Not_All_Points_Are_Equal_Learning_Highly_Efficient_Point-Based_Detectors_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Not_All_Points_Are_Equal_Learning_Highly_Efficient_Point-Based_Detectors_CVPR_2022_paper.pdf">Not All Points Are Equal: Learning Highly Efficient Point-based<br />Detectors for 3D LiDAR Point Clouds</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">108</td>
<td style="text-align: center;">2022-05-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Cross-View_Transformers_for_Real-Time_Map-View_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Cross-View_Transformers_for_Real-Time_Map-View_Semantic_Segmentation_CVPR_2022_paper.pdf">Cross-view Transformers for real-time Map-view Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">107</td>
<td style="text-align: center;">2022-03-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Afham_CrossPoint_Self-Supervised_Cross-Modal_Contrastive_Learning_for_3D_Point_Cloud_Understanding_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Afham_CrossPoint_Self-Supervised_Cross-Modal_Contrastive_Learning_for_3D_Point_Cloud_Understanding_CVPR_2022_paper.pdf">CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud<br />Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">106</td>
<td style="text-align: center;">2021-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Towards_Language-Free_Training_for_Text-to-Image_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Towards_Language-Free_Training_for_Text-to-Image_Generation_CVPR_2022_paper.pdf">Towards Language-Free Training for Text-to-Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">106</td>
<td style="text-align: center;">2021-12-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Fan_Embracing_Single_Stride_3D_Object_Detector_With_Sparse_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fan_Embracing_Single_Stride_3D_Object_Detector_With_Sparse_Transformer_CVPR_2022_paper.pdf">Embracing Single Stride 3D Object Detector with Sparse Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">105</td>
<td style="text-align: center;">2022-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cazenavette_Dataset_Distillation_by_Matching_Training_Trajectories_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cazenavette_Dataset_Distillation_by_Matching_Training_Trajectories_CVPR_2022_paper.pdf">Dataset Distillation by Matching Training Trajectories</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">105</td>
<td style="text-align: center;">2021-12-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xiu_ICON_Implicit_Clothed_Humans_Obtained_From_Normals_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xiu_ICON_Implicit_Clothed_Humans_Obtained_From_Normals_CVPR_2022_paper.pdf">ICON: Implicit Clothed humans Obtained from Normals</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">103</td>
<td style="text-align: center;">2021-12-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Whang_Deblurring_via_Stochastic_Refinement_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Whang_Deblurring_via_Stochastic_Refinement_CVPR_2022_paper.pdf">Deblurring via Stochastic Refinement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">103</td>
<td style="text-align: center;">2022-05-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kundu_Panoptic_Neural_Fields_A_Semantic_Object-Aware_Neural_Scene_Representation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kundu_Panoptic_Neural_Fields_A_Semantic_Object-Aware_Neural_Scene_Representation_CVPR_2022_paper.pdf">Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">102</td>
<td style="text-align: center;">2021-06-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_ST_Make_Self-Training_Work_Better_for_Semi-Supervised_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_ST_Make_Self-Training_Work_Better_for_Semi-Supervised_Semantic_Segmentation_CVPR_2022_paper.pdf">ST++: Make Self-trainingWork Better for Semi-supervised Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">101</td>
<td style="text-align: center;">2022-01-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Deng_Anomaly_Detection_via_Reverse_Distillation_From_One-Class_Embedding_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_Anomaly_Detection_via_Reverse_Distillation_From_One-Class_Embedding_CVPR_2022_paper.pdf">Anomaly Detection via Reverse Distillation from One-Class Embedding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">2022-01-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zellers_MERLOT_Reserve_Neural_Script_Knowledge_Through_Vision_and_Language_and_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zellers_MERLOT_Reserve_Neural_Script_Knowledge_Through_Vision_and_Language_and_CVPR_2022_paper.pdf">MERLOT RESERVE: Neural Script Knowledge through Vision and Language<br />and Sound</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">100</td>
<td style="text-align: center;">2021-12-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Luddecke_Image_Segmentation_Using_Text_and_Image_Prompts_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Luddecke_Image_Segmentation_Using_Text_and_Image_Prompts_CVPR_2022_paper.pdf">Image Segmentation Using Text and Image Prompts</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">99</td>
<td style="text-align: center;">2022-01-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Girdhar_Omnivore_A_Single_Model_for_Many_Visual_Modalities_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Girdhar_Omnivore_A_Single_Model_for_Many_Visual_Modalities_CVPR_2022_paper.pdf">Omnivore: A Single Model for Many Visual Modalities</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">99</td>
<td style="text-align: center;">2021-05-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mao_Towards_Robust_Vision_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Mao_Towards_Robust_Vision_Transformer_CVPR_2022_paper.pdf">Towards Robust Vision Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">99</td>
<td style="text-align: center;">2021-12-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zheng_I_M_Avatar_Implicit_Morphable_Head_Avatars_From_Videos_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_I_M_Avatar_Implicit_Morphable_Head_Avatars_From_Videos_CVPR_2022_paper.pdf">I M Avatar: Implicit Morphable Head Avatars from Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">99</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rematas_Urban_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rematas_Urban_Radiance_Fields_CVPR_2022_paper.pdf">Urban Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">97</td>
<td style="text-align: center;">2022-03-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Greff_Kubric_A_Scalable_Dataset_Generator_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Greff_Kubric_A_Scalable_Dataset_Generator_CVPR_2022_paper.pdf">Kubric: A scalable dataset generator</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">2021-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_GMFlow_Learning_Optical_Flow_via_Global_Matching_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_GMFlow_Learning_Optical_Flow_via_Global_Matching_CVPR_2022_paper.pdf">GMFlow: Learning Optical Flow via Global Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">95</td>
<td style="text-align: center;">2022-03-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_MAT_Mask-Aware_Transformer_for_Large_Hole_Image_Inpainting_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MAT_Mask-Aware_Transformer_for_Large_Hole_Image_Inpainting_CVPR_2022_paper.pdf">MAT: Mask-Aware Transformer for Large Hole Image Inpainting</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">94</td>
<td style="text-align: center;">2021-12-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Align_and_Prompt_Video-and-Language_Pre-Training_With_Entity_Prompts_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Align_and_Prompt_Video-and-Language_Pre-Training_With_Entity_Prompts_CVPR_2022_paper.pdf">Align and Prompt: Video-and-Language Pre-training with Entity Prompts</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">93</td>
<td style="text-align: center;">2022-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_ViM_Out-of-Distribution_With_Virtual-Logit_Matching_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_ViM_Out-of-Distribution_With_Virtual-Logit_Matching_CVPR_2022_paper.pdf">ViM: Out-Of-Distribution with Virtual-logit Matching</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">93</td>
<td style="text-align: center;">2022-03-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Rethinking_Semantic_Segmentation_A_Prototype_View_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Rethinking_Semantic_Segmentation_A_Prototype_View_CVPR_2022_paper.pdf">Rethinking Semantic Segmentation: A Prototype View</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">93</td>
<td style="text-align: center;">2021-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Turki_Mega-NERF_Scalable_Construction_of_Large-Scale_NeRFs_for_Virtual_Fly-Throughs_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Turki_Mega-NERF_Scalable_Construction_of_Large-Scale_NeRFs_for_Virtual_Fly-Throughs_CVPR_2022_paper.pdf">Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-<br />Throughs</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">92</td>
<td style="text-align: center;">2021-11-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.pdf">MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">90</td>
<td style="text-align: center;">2021-05-24</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Oriented_RepPoints_for_Aerial_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Oriented_RepPoints_for_Aerial_Object_Detection_CVPR_2022_paper.pdf">Oriented RepPoints for Aerial Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">90</td>
<td style="text-align: center;">2021-12-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lee_MPViT_Multi-Path_Vision_Transformer_for_Dense_Prediction_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_MPViT_Multi-Path_Vision_Transformer_for_Dense_Prediction_CVPR_2022_paper.pdf">MPViT: Multi-Path Vision Transformer for Dense Prediction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">89</td>
<td style="text-align: center;">2021-07-28</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Neural_Rays_for_Occlusion-Aware_Image-Based_Rendering_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Neural_Rays_for_Occlusion-Aware_Image-Based_Rendering_CVPR_2022_paper.pdf">Neural Rays for Occlusion-aware Image-based Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">88</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Valanarasu_TransWeather_Transformer-Based_Restoration_of_Images_Degraded_by_Adverse_Weather_Conditions_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Valanarasu_TransWeather_Transformer-Based_Restoration_of_Images_Degraded_by_Adverse_Weather_Conditions_CVPR_2022_paper.pdf">TransWeather: Transformer-based Restoration of Images Degraded by Adverse Weather<br />Conditions</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">88</td>
<td style="text-align: center;">2022-04-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Contrastive_Test-Time_Adaptation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Contrastive_Test-Time_Adaptation_CVPR_2022_paper.pdf">Contrastive Test-Time Adaptation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">88</td>
<td style="text-align: center;">2022-05-10</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yuan_NeRF-Editing_Geometry_Editing_of_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yuan_NeRF-Editing_Geometry_Editing_of_Neural_Radiance_Fields_CVPR_2022_paper.pdf">NeRF-Editing: Geometry Editing of Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">86</td>
<td style="text-align: center;">2021-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Neural_3D_Video_Synthesis_From_Multi-View_Video_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Neural_3D_Video_Synthesis_From_Multi-View_Video_CVPR_2022_paper.pdf">Neural 3D Video Synthesis from Multi-view Video</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">86</td>
<td style="text-align: center;">2021-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_StyleSwin_Transformer-Based_GAN_for_High-Resolution_Image_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_StyleSwin_Transformer-Based_GAN_for_High-Resolution_Image_Generation_CVPR_2022_paper.pdf">StyleSwin: Transformer-based GAN for High-resolution Image Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">84</td>
<td style="text-align: center;">2021-06-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tang_Patch_Slimming_for_Efficient_Vision_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Patch_Slimming_for_Efficient_Vision_Transformers_CVPR_2022_paper.pdf">Patch Slimming for Efficient Vision Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2021-05-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cole_When_Does_Contrastive_Visual_Representation_Learning_Work_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cole_When_Does_Contrastive_Visual_Representation_Learning_Work_CVPR_2022_paper.pdf">When Does Contrastive Visual Representation Learning Work?</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf">Generating Diverse and Natural 3D Human Motions from Text</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2021-11-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Cai_Mask-Guided_Spectral-Wise_Transformer_for_Efficient_Hyperspectral_Image_Reconstruction_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cai_Mask-Guided_Spectral-Wise_Transformer_for_Efficient_Hyperspectral_Image_Reconstruction_CVPR_2022_paper.pdf">Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2021-12-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Xu_3D-Aware_Image_Synthesis_via_Learning_Structural_and_Textural_Representations_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_3D-Aware_Image_Synthesis_via_Learning_Structural_and_Textural_Representations_CVPR_2022_paper.pdf">3D-aware Image Synthesis via Learning Structural and Textural Representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2021-11-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_Focal_and_Global_Knowledge_Distillation_for_Detectors_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Focal_and_Global_Knowledge_Distillation_for_Detectors_CVPR_2022_paper.pdf">Focal and Global Knowledge Distillation for Detectors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">83</td>
<td style="text-align: center;">2022-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mittal_AutoSDF_Shape_Priors_for_3D_Completion_Reconstruction_and_Generation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Mittal_AutoSDF_Shape_Priors_for_3D_Completion_Reconstruction_and_Generation_CVPR_2022_paper.pdf">AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ren_Shunted_Self-Attention_via_Multi-Scale_Token_Aggregation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ren_Shunted_Self-Attention_via_Multi-Scale_Token_Aggregation_CVPR_2022_paper.pdf">Shunted Self-Attention via Multi-Scale Token Aggregation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Pan_On_the_Integration_of_Self-Attention_and_Convolution_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Pan_On_the_Integration_of_Self-Attention_and_Convolution_CVPR_2022_paper.pdf">On the Integration of Self-Attention and Convolution</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2022-03-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Fine-Tuning_Global_Model_via_Data-Free_Knowledge_Distillation_for_Non-IID_Federated_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Fine-Tuning_Global_Model_via_Data-Free_Knowledge_Distillation_for_Non-IID_Federated_CVPR_2022_paper.pdf">Fine-tuning Global Model via Data-Free Knowledge Distillation for Non-IID<br />Federated Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2021-12-04</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_LAVT_Language-Aware_Vision_Transformer_for_Referring_Image_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_LAVT_Language-Aware_Vision_Transformer_for_Referring_Image_Segmentation_CVPR_2022_paper.pdf">LAVT: Language-Aware Vision Transformer for Referring Image Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Grassal_Neural_Head_Avatars_From_Monocular_RGB_Videos_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Grassal_Neural_Head_Avatars_From_Monocular_RGB_Videos_CVPR_2022_paper.pdf">Neural Head Avatars from Monocular RGB Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">82</td>
<td style="text-align: center;">2021-11-29</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tewel_ZeroCap_Zero-Shot_Image-to-Text_Generation_for_Visual-Semantic_Arithmetic_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tewel_ZeroCap_Zero-Shot_Image-to-Text_Generation_for_Visual-Semantic_Arithmetic_CVPR_2022_paper.pdf">ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">81</td>
<td style="text-align: center;">2022-05-06</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lu_Prompt_Distribution_Learning_CVPR_2022_paper.pdf">Prompt Distribution Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">81</td>
<td style="text-align: center;">2022-04-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Focal_Sparse_Convolutional_Networks_for_3D_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Focal_Sparse_Convolutional_Networks_for_3D_Object_Detection_CVPR_2022_paper.pdf">Focal Sparse Convolutional Networks for 3D Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2020-08-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Tao_DF-GAN_A_Simple_and_Effective_Baseline_for_Text-to-Image_Synthesis_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tao_DF-GAN_A_Simple_and_Effective_Baseline_for_Text-to-Image_Synthesis_CVPR_2022_paper.pdf">DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">80</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sun_FENeRF_Face_Editing_in_Neural_Radiance_Fields_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_FENeRF_Face_Editing_in_Neural_Radiance_Fields_CVPR_2022_paper.pdf">FENeRF: Face Editing in Neural Radiance Fields</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">79</td>
<td style="text-align: center;">2022-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Mayer_Transforming_Model_Prediction_for_Tracking_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Mayer_Transforming_Model_Prediction_for_Tracking_CVPR_2022_paper.pdf">Transforming Model Prediction for Tracking</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">79</td>
<td style="text-align: center;">2022-03-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Target-Aware_Dual_Adversarial_Learning_and_a_Multi-Scenario_Multi-Modality_Benchmark_To_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Target-Aware_Dual_Adversarial_Learning_and_a_Multi-Scenario_Multi-Modality_Benchmark_To_CVPR_2022_paper.pdf">Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark<br />to Fuse Infrared and Visible for Object Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2021-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lin_SwinBERT_End-to-End_Transformers_With_Sparse_Attention_for_Video_Captioning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_SwinBERT_End-to-End_Transformers_With_Sparse_Attention_for_Video_Captioning_CVPR_2022_paper.pdf">SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2022-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yu_DAIR-V2X_A_Large-Scale_Dataset_for_Vehicle-Infrastructure_Cooperative_3D_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_DAIR-V2X_A_Large-Scale_Dataset_for_Vehicle-Infrastructure_Cooperative_3D_Object_Detection_CVPR_2022_paper.pdf">DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2022-02-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Self-Supervised_Transformers_for_Unsupervised_Object_Discovery_Using_Normalized_Cut_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Self-Supervised_Transformers_for_Unsupervised_Object_Discovery_Using_Normalized_Cut_CVPR_2022_paper.pdf">Self-Supervised Transformers for Unsupervised Object Discovery using Normalized Cut</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">2021-12-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Dinh_HyperInverter_Improving_StyleGAN_Inversion_via_Hypernetwork_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dinh_HyperInverter_Improving_StyleGAN_Inversion_via_Hypernetwork_CVPR_2022_paper.pdf">HyperInverter: Improving StyleGAN Inversion via Hypernetwork</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2021-11-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Gu_Multi-Scale_High-Resolution_Vision_Transformer_for_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Gu_Multi-Scale_High-Resolution_Vision_Transformer_for_Semantic_Segmentation_CVPR_2022_paper.pdf">Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">77</td>
<td style="text-align: center;">2021-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sajjadi_Scene_Representation_Transformer_Geometry-Free_Novel_View_Synthesis_Through_Set-Latent_Scene_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sajjadi_Scene_Representation_Transformer_Geometry-Free_Novel_View_Synthesis_Through_Set-Latent_Scene_CVPR_2022_paper.pdf">Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent<br />Scene Representations</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">2022-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_DTFD-MIL_Double-Tier_Feature_Distillation_Multiple_Instance_Learning_for_Histopathology_Whole_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_DTFD-MIL_Double-Tier_Feature_Distillation_Multiple_Instance_Learning_for_Histopathology_Whole_CVPR_2022_paper.pdf">DTFD-MIL: Double-Tier Feature Distillation Multiple Instance Learning for Histopathology<br />Whole Slide Image Classification</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">75</td>
<td style="text-align: center;">2021-02-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_VisualGPT_Data-Efficient_Adaptation_of_Pretrained_Language_Models_for_Image_Captioning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_VisualGPT_Data-Efficient_Adaptation_of_Pretrained_Language_Models_for_Image_Captioning_CVPR_2022_paper.pdf">VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image<br />Captioning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">74</td>
<td style="text-align: center;">2021-12-23</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_BANMo_Building_Animatable_3D_Neural_Models_From_Many_Casual_Videos_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_BANMo_Building_Animatable_3D_Neural_Models_From_Many_Casual_Videos_CVPR_2022_paper.pdf">BANMo: Building Animatable 3D Neural Models from Many Casual<br />Videos</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">74</td>
<td style="text-align: center;">2021-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ding_Decoupling_Zero-Shot_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ding_Decoupling_Zero-Shot_Semantic_Segmentation_CVPR_2022_paper.pdf">Decoupling Zero-Shot Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">74</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wu_URetinex-Net_Retinex-Based_Deep_Unfolding_Network_for_Low-Light_Image_Enhancement_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_URetinex-Net_Retinex-Based_Deep_Unfolding_Network_for_Low-Light_Image_Enhancement_CVPR_2022_paper.pdf">URetinex-Net: Retinex-based Deep Unfolding Network for Low-light Image Enhancement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2022-01-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wu_MeMViT_Memory-Augmented_Multiscale_Vision_Transformer_for_Efficient_Long-Term_Video_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_MeMViT_Memory-Augmented_Multiscale_Vision_Transformer_for_Efficient_Long-Term_Video_Recognition_CVPR_2022_paper.pdf">MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video<br />Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">73</td>
<td style="text-align: center;">2022-03-21</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/He_ELIC_Efficient_Learned_Image_Compression_With_Unevenly_Grouped_Space-Channel_Contextual_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_ELIC_Efficient_Learned_Image_Compression_With_Unevenly_Grouped_Space-Channel_Contextual_CVPR_2022_paper.pdf">ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel<br />Contextual Adaptive Coding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2021-11-26</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Johari_GeoNeRF_Generalizing_NeRF_With_Geometry_Priors_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Johari_GeoNeRF_Generalizing_NeRF_With_Geometry_Priors_CVPR_2022_paper.pdf">GeoNeRF: Generalizing NeRF with Geometry Priors</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2021-05-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Deng_StyTr2_Image_Style_Transfer_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Deng_StyTr2_Image_Style_Transfer_With_Transformers_CVPR_2022_paper.pdf">StyTr2: Image Style Transfer with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">72</td>
<td style="text-align: center;">2022-05-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Melas-Kyriazi_Deep_Spectral_Methods_A_Surprisingly_Strong_Baseline_for_Unsupervised_Semantic_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Melas-Kyriazi_Deep_Spectral_Methods_A_Surprisingly_Strong_Baseline_for_Unsupervised_Semantic_CVPR_2022_paper.pdf">Deep Spectral Methods: A Surprisingly Strong Baseline for Unsupervised<br />Semantic Segmentation and Localization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2021-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Shi_Video_Frame_Interpolation_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shi_Video_Frame_Interpolation_Transformer_CVPR_2022_paper.pdf">Video Frame Interpolation Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2022-03-03</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lee_Autoregressive_Image_Generation_Using_Residual_Quantization_CVPR_2022_paper.pdf">Autoregressive Image Generation using Residual Quantization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2022-04-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Hu_Pushing_the_Limits_of_Simple_Pipelines_for_Few-Shot_Learning_External_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_Pushing_the_Limits_of_Simple_Pipelines_for_Few-Shot_Learning_External_CVPR_2022_paper.pdf">Pushing the Limits of Simple Pipelines for Few-Shot Learning:<br />External Data and Fine-Tuning Make a Difference</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_Image_Dehazing_Transformer_With_Transmission-Aware_3D_Position_Embedding_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Image_Dehazing_Transformer_With_Transmission-Aware_3D_Position_Embedding_CVPR_2022_paper.pdf">Image Dehazing Transformer with Transmission-Aware 3D Position Embedding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">71</td>
<td style="text-align: center;">2021-11-27</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Targeted_Supervised_Contrastive_Learning_for_Long-Tailed_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Targeted_Supervised_Contrastive_Learning_for_Long-Tailed_Recognition_CVPR_2022_paper.pdf">Targeted Supervised Contrastive Learning for Long-Tailed Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">70</td>
<td style="text-align: center;">2022-01-20</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Seo_End-to-End_Generative_Pretraining_for_Multimodal_Video_Captioning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Seo_End-to-End_Generative_Pretraining_for_Multimodal_Video_Captioning_CVPR_2022_paper.pdf">End-to-end Generative Pretraining for Multimodal Video Captioning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2021-10-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Collins_ABO_Dataset_and_Benchmarks_for_Real-World_3D_Object_Understanding_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Collins_ABO_Dataset_and_Benchmarks_for_Real-World_3D_Object_Understanding_CVPR_2022_paper.pdf">ABO: Dataset and Benchmarks for Real-World 3D Object Understanding</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2021-11-25</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Perturbed_and_Strict_Mean_Teachers_for_Semi-Supervised_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Perturbed_and_Strict_Mean_Teachers_for_Semi-Supervised_Semantic_Segmentation_CVPR_2022_paper.pdf">Perturbed and Strict Mean Teachers for Semi-supervised Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">69</td>
<td style="text-align: center;">2021-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lindell_BACON_Band-Limited_Coordinate_Networks_for_Multiscale_Scene_Representation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lindell_BACON_Band-Limited_Coordinate_Networks_for_Multiscale_Scene_Representation_CVPR_2022_paper.pdf">Bacon: Band-limited Coordinate Networks for Multiscale Scene Representation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2022-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Practical_Stereo_Matching_via_Cascaded_Recurrent_Network_With_Adaptive_Correlation_CVPR_2022_paper.pdf">Practical Stereo Matching via Cascaded Recurrent Network with Adaptive<br />Correlation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">68</td>
<td style="text-align: center;">2022-01-07</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Vaze_Generalized_Category_Discovery_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Vaze_Generalized_Category_Discovery_CVPR_2022_paper.pdf">Generalized Category Discovery</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2022-04-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Choi_Perception_Prioritized_Training_of_Diffusion_Models_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Choi_Perception_Prioritized_Training_of_Diffusion_Models_CVPR_2022_paper.pdf">Perception Prioritized Training of Diffusion Models</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2021-12-09</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Park_Fast_Point_Transformer_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Park_Fast_Point_Transformer_CVPR_2022_paper.pdf">Fast Point Transformer</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2021-03-16</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Yang_QueryDet_Cascaded_Sparse_Query_for_Accelerating_High-Resolution_Small_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_QueryDet_Cascaded_Sparse_Query_for_Accelerating_High-Resolution_Small_Object_Detection_CVPR_2022_paper.pdf">QueryDet: Cascaded Sparse Query for Accelerating High-Resolution Small Object<br />Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2021-11-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rebain_LOLNerf_Learn_From_One_Look_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rebain_LOLNerf_Learn_From_One_Look_CVPR_2022_paper.pdf">LOLNeRF: Learn from One Look</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2021-11-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Meng_AdaViT_Adaptive_Vision_Transformers_for_Efficient_Image_Recognition_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Meng_AdaViT_Adaptive_Vision_Transformers_for_Efficient_Image_Recognition_CVPR_2022_paper.pdf">AdaViT: Adaptive Vision Transformers for Efficient Image Recognition</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">67</td>
<td style="text-align: center;">2022-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Lang_Learning_What_Not_To_Segment_A_New_Perspective_on_Few-Shot_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Lang_Learning_What_Not_To_Segment_A_New_Perspective_on_Few-Shot_CVPR_2022_paper.pdf">Learning What Not to Segment: A New Perspective on<br />Few-Shot Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2021-11-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ristea_Self-Supervised_Predictive_Convolutional_Attentive_Block_for_Anomaly_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ristea_Self-Supervised_Predictive_Convolutional_Attentive_Block_for_Anomaly_Detection_CVPR_2022_paper.pdf">Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-06-01</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_All-in-One_Image_Restoration_for_Unknown_Corruption_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_All-in-One_Image_Restoration_for_Unknown_Corruption_CVPR_2022_paper.pdf">All-In-One Image Restoration for Unknown Corruption</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">66</td>
<td style="text-align: center;">2022-02-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Fourier_PlenOctrees_for_Dynamic_Radiance_Field_Rendering_in_Real-Time_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Fourier_PlenOctrees_for_Dynamic_Radiance_Field_Rendering_in_Real-Time_CVPR_2022_paper.pdf">Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2021-12-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Sun_Putting_People_in_Their_Place_Monocular_Regression_of_3D_People_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Sun_Putting_People_in_Their_Place_Monocular_Regression_of_3D_People_CVPR_2022_paper.pdf">Putting People in their Place: Monocular Regression of 3D<br />People in Depth</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2022-04-12</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_TopFormer_Token_Pyramid_Transformer_for_Mobile_Semantic_Segmentation_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_TopFormer_Token_Pyramid_Transformer_for_Mobile_Semantic_Segmentation_CVPR_2022_paper.pdf">TopFormer: Token Pyramid Transformer for Mobile Semantic Segmentation</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2021-12-17</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Suhail_Light_Field_Neural_Rendering_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Suhail_Light_Field_Neural_Rendering_CVPR_2022_paper.pdf">Light Field Neural Rendering</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">2022-03-15</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Exact_Feature_Distribution_Matching_for_Arbitrary_Style_Transfer_and_Domain_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Exact_Feature_Distribution_Matching_for_Arbitrary_Style_Transfer_and_Domain_CVPR_2022_paper.pdf">Exact Feature Distribution Matching for Arbitrary Style Transfer and<br />Domain Generalization</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2022-03-14</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Forward_Compatible_Few-Shot_Class-Incremental_Learning_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhou_Forward_Compatible_Few-Shot_Class-Incremental_Learning_CVPR_2022_paper.pdf">Forward Compatible Few-Shot Class-Incremental Learning</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2022-04-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Shiohara_Detecting_Deepfakes_With_Self-Blended_Images_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shiohara_Detecting_Deepfakes_With_Self-Blended_Images_CVPR_2022_paper.pdf">Detecting Deepfakes with Self-Blended Images</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2021-09-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.pdf">Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2022-05-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_Neural_3D_Scene_Reconstruction_With_the_Manhattan-World_Assumption_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Neural_3D_Scene_Reconstruction_With_the_Manhattan-World_Assumption_CVPR_2022_paper.pdf">Neural 3D Scene Reconstruction with the Manhattan-world Assumption</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2022-03-18</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wu_Sparse_Fuse_Dense_Towards_High_Quality_3D_Detection_With_Depth_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wu_Sparse_Fuse_Dense_Towards_High_Quality_3D_Detection_With_Depth_CVPR_2022_paper.pdf">Sparse Fuse Dense: Towards High Quality 3D Detection with<br />Depth Completion</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2021-12-02</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.pdf">Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot<br />and Few-shot Tasks</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">2021-12-08</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Fini_Self-Supervised_Models_Are_Continual_Learners_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Fini_Self-Supervised_Models_Are_Continual_Learners_CVPR_2022_paper.pdf">Self-Supervised Models are Continual Learners</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2022-03-19</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/He_Voxel_Set_Transformer_A_Set-to-Set_Approach_to_3D_Object_Detection_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/He_Voxel_Set_Transformer_A_Set-to-Set_Approach_to_3D_Object_Detection_CVPR_2022_paper.pdf">Voxel Set Transformer: A Set-to-Set Approach to 3D Object<br />Detection from Point Clouds</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">None</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Wang_CAFE_Learning_To_Condense_Dataset_by_Aligning_Features_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_CAFE_Learning_To_Condense_Dataset_by_Aligning_Features_CVPR_2022_paper.pdf">CAFE Learning to Condense Dataset by Aligning Features</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2022-01-13</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_CLIP-Event_Connecting_Text_and_Images_With_Event_Structures_CVPR_2022_paper.pdf">CLIP-Event: Connecting Text and Images with Event Structures</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2022-03-22</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Learning_From_All_Vehicles_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Learning_From_All_Vehicles_CVPR_2022_paper.pdf">Learning from All Vehicles</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">62</td>
<td style="text-align: center;">2021-08-30</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_Hire-MLP_Vision_MLP_via_Hierarchical_Rearrangement_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Hire-MLP_Vision_MLP_via_Hierarchical_Rearrangement_CVPR_2022_paper.pdf">Hire-MLP: Vision MLP via Hierarchical Rearrangement</a></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2022-03-05</td>
<td style="text-align: center;"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Ru_Learning_Affinity_From_Attention_End-to-End_Weakly-Supervised_Semantic_Segmentation_With_Transformers_CVPR_2022_paper.html">link</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Ru_Learning_Affinity_From_Attention_End-to-End_Weakly-Supervised_Semantic_Segmentation_With_Transformers_CVPR_2022_paper.pdf">Learning Affinity from Attention: End-to-End Weakly-Supervised Semantic Segmentation with<br />Transformers</a></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../ACL/ACL_2023/" class="btn btn-neutral float-left" title="ACL 2023"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../CVPR_2023/" class="btn btn-neutral float-right" title="CVPR 2023">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../ACL/ACL_2023/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../CVPR_2023/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
